@main () -> () {
    prog.fragment @deinit <CPU> <data> {
        addr:0xffffffffffffffff prog.label[symbol:deinit.__entry]
    }
    prog.fragment @__MLLM_JIT_PACKAGE_CODE_SEGMENT <CPU> <code> {
        addr:0x00000000000000 prog.mode_config[flag:1]
        addr:0x00000000000001 prog.bind[type:0, program_uuid:583, input_pos:0]
        addr:0x00000000000002 prog.bind[type:0, program_uuid:1270, input_pos:1]
        addr:0x00000000000003 prog.bind[type:0, program_uuid:1271, input_pos:2]
        addr:0x00000000000004 prog.jump model.__entry[offset:3]
        addr:0x00000000000005 prog.bind[type:1, program_uuid:2029, input_pos:0]
        addr:0x00000000000006 prog.exit() -> ()
        addr:0x00000000000007 prog.label[symbol:model.__entry]
        addr:0x00000000000008 prog.jump model.layers.0.__entry[offset:34]
        addr:0x00000000000009 prog.jump model.layers.1.__entry[offset:97]
        addr:0x0000000000000a prog.jump model.layers.2.__entry[offset:160]
        addr:0x0000000000000b prog.jump model.layers.3.__entry[offset:223]
        addr:0x0000000000000c prog.jump model.layers.4.__entry[offset:286]
        addr:0x0000000000000d prog.jump model.layers.5.__entry[offset:349]
        addr:0x0000000000000e prog.jump model.layers.6.__entry[offset:412]
        addr:0x0000000000000f prog.jump model.layers.7.__entry[offset:475]
        addr:0x00000000000010 prog.jump model.layers.8.__entry[offset:538]
        addr:0x00000000000011 prog.jump model.layers.9.__entry[offset:601]
        addr:0x00000000000012 prog.jump model.layers.10.__entry[offset:664]
        addr:0x00000000000013 prog.jump model.layers.11.__entry[offset:727]
        addr:0x00000000000014 prog.jump model.layers.12.__entry[offset:790]
        addr:0x00000000000015 prog.jump model.layers.13.__entry[offset:853]
        addr:0x00000000000016 prog.jump model.layers.14.__entry[offset:916]
        addr:0x00000000000017 prog.jump model.layers.15.__entry[offset:979]
        addr:0x00000000000018 prog.jump model.layers.16.__entry[offset:1042]
        addr:0x00000000000019 prog.jump model.layers.17.__entry[offset:1105]
        addr:0x0000000000001a prog.jump model.layers.18.__entry[offset:1168]
        addr:0x0000000000001b prog.jump model.layers.19.__entry[offset:1231]
        addr:0x0000000000001c prog.jump model.layers.20.__entry[offset:1294]
        addr:0x0000000000001d prog.jump model.layers.21.__entry[offset:1357]
        addr:0x0000000000001e prog.jump model.layers.22.__entry[offset:1420]
        addr:0x0000000000001f prog.jump model.layers.23.__entry[offset:1483]
        addr:0x00000000000020 prog.jump model.layers.24.__entry[offset:1546]
        addr:0x00000000000021 prog.jump model.layers.25.__entry[offset:1609]
        addr:0x00000000000022 prog.jump model.layers.26.__entry[offset:1672]
        addr:0x00000000000023 prog.jump model.layers.27.__entry[offset:1735]
        addr:0x00000000000024 prog.kernel_launch(%2027:tensor<[1, 192, 1536], Float32, CPU>) -> (%2028:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.norm, op_type:RMSNorm, op_options:{"add_unit_offset":false,"epsilon":9.999999974752427e-07}]
        addr:0x00000000000025 prog.free(%2027:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x00000000000026 prog.kernel_launch(%2028:tensor<[1, 192, 1536], Float32, CPU>) -> (%2028:tensor<[1, 192, 1536], Float32, CPU>)[op_type:Slice, op_options:{"indices":[{"end":2147483633,"start":2147483633,"step":1},{"end":192,"start":191,"step":1},{"end":2147483633,"start":2147483633,"step":1}]}]
        addr:0x00000000000027 prog.kernel_launch(%2028:tensor<[1, 192, 1536], Float32, CPU>) -> (%2029:tensor<[1, 1, 151936], Float32, CPU>)[symbol_name:model.lm_head, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":151936}]
        addr:0x00000000000028 prog.free(%2028:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x00000000000029 prog.ret
        addr:0x0000000000002a prog.label[symbol:model.layers.0.__entry]
        addr:0x0000000000002b prog.kernel_launch(%583:tensor<[1, 192, 1536], Float32, CPU>) -> (%1272:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.0.input_layernorm, op_type:RMSNorm, op_options:{"add_unit_offset":false,"epsilon":9.999999974752427e-07}]
        addr:0x0000000000002c prog.jump model.layers.0.self_attn.__entry[offset:10]
        addr:0x0000000000002d prog.kernel_launch(%1290:tensor<[1, 192, 1536], Float32, CPU>, %583:tensor<[1, 192, 1536], Float32, CPU>) -> (%1291:tensor<[1, 192, 1536], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x0000000000002e prog.free(%583:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x0000000000002f prog.free(%1290:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x00000000000030 prog.kernel_launch(%1291:tensor<[1, 192, 1536], Float32, CPU>) -> (%1292:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.0.post_attention_layernorm, op_type:RMSNorm, op_options:{"add_unit_offset":false,"epsilon":9.999999974752427e-07}]
        addr:0x00000000000031 prog.jump model.layers.0.mlp.__entry[offset:45]
        addr:0x00000000000032 prog.kernel_launch(%1297:tensor<[1, 192, 1536], Float32, CPU>, %1291:tensor<[1, 192, 1536], Float32, CPU>) -> (%1298:tensor<[1, 192, 1536], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x00000000000033 prog.free(%1291:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x00000000000034 prog.free(%1297:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x00000000000035 prog.ret
        addr:0x00000000000036 prog.label[symbol:model.layers.0.self_attn.__entry]
        addr:0x00000000000037 prog.kernel_launch(%1272:tensor<[1, 192, 1536], Float32, CPU>) -> (%1273:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.0.self_attn.q_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x00000000000038 prog.kernel_launch(%1272:tensor<[1, 192, 1536], Float32, CPU>) -> (%1274:tensor<[1, 192, 256], Float32, CPU>)[symbol_name:model.layers.0.self_attn.k_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x00000000000039 prog.kernel_launch(%1272:tensor<[1, 192, 1536], Float32, CPU>) -> (%1275:tensor<[1, 192, 256], Float32, CPU>)[symbol_name:model.layers.0.self_attn.v_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x0000000000003a prog.free(%1272:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x0000000000003b prog.kernel_launch(%1273:tensor<[1, 192, 1536], Float32, CPU>) -> (%1273:tensor<[1, 192, 12, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,12,128]}]
        addr:0x0000000000003c prog.kernel_launch(%1274:tensor<[1, 192, 256], Float32, CPU>) -> (%1274:tensor<[1, 192, 2, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,2,128]}]
        addr:0x0000000000003d prog.kernel_launch(%1275:tensor<[1, 192, 256], Float32, CPU>) -> (%1275:tensor<[1, 192, 2, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,2,128]}]
        addr:0x0000000000003e prog.kernel_launch(%1273:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%1276:tensor<[1, 12, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x0000000000003f prog.free(%1273:tensor<[1, 192, 12, 128], Float32, CPU>) -> ()
        addr:0x00000000000040 prog.kernel_launch(%1274:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%1277:tensor<[1, 2, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x00000000000041 prog.free(%1274:tensor<[1, 192, 2, 128], Float32, CPU>) -> ()
        addr:0x00000000000042 prog.kernel_launch(%1275:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%1278:tensor<[1, 2, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x00000000000043 prog.free(%1275:tensor<[1, 192, 2, 128], Float32, CPU>) -> ()
        addr:0x00000000000044 prog.kernel_launch(%1276:tensor<[1, 12, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1279:tensor<[1, 12, 192, 128], Float32, CPU>)[symbol_name:model.layers.0.self_attn.q_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x00000000000045 prog.free(%1276:tensor<[1, 12, 192, 128], Float32, CPU>) -> ()
        addr:0x00000000000046 prog.kernel_launch(%1277:tensor<[1, 2, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1280:tensor<[1, 2, 192, 128], Float32, CPU>)[symbol_name:model.layers.0.self_attn.k_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x00000000000047 prog.free(%1277:tensor<[1, 2, 192, 128], Float32, CPU>) -> ()
        addr:0x00000000000048 prog.kernel_launch(%1280:tensor<[1, 2, 192, 128], Float32, CPU>, %1278:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1281:tensor<[1, 2, 192, 128], Float32, CPU>, %1282:tensor<[1, 2, 192, 128], Float32, CPU>)[symbol_name:model.layers.0.self_attn.kv_cache, op_type:KVCache, op_options:{"head_dim":128,"kv_head":2,"layer_idx":0,"q_head":12,"use_fa2":false}]
        addr:0x00000000000049 prog.free(%1278:tensor<[1, 2, 192, 128], Float32, CPU>) -> ()
        addr:0x0000000000004a prog.free(%1280:tensor<[1, 2, 192, 128], Float32, CPU>) -> ()
        addr:0x0000000000004b prog.kernel_launch(%1279:tensor<[1, 12, 192, 128], Float32, CPU>, %1281:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1283:tensor<[1, 12, 192, 192], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":true}]
        addr:0x0000000000004c prog.free(%1281:tensor<[1, 2, 192, 128], Float32, CPU>) -> ()
        addr:0x0000000000004d prog.free(%1279:tensor<[1, 12, 192, 128], Float32, CPU>) -> ()
        addr:0x0000000000004e prog.kernel_launch(%1283:tensor<[1, 12, 192, 192], Float32, CPU>, %1284:tensor<[1], Float32, CPU>) -> (%1285:tensor<[1, 12, 192, 192], Float32, CPU>)[op_type:Mul, op_options:null]
        addr:0x0000000000004f prog.free(%1284:tensor<[1], Float32, CPU>) -> ()
        addr:0x00000000000050 prog.free(%1283:tensor<[1, 12, 192, 192], Float32, CPU>) -> ()
        addr:0x00000000000051 prog.kernel_launch(%1285:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%1286:tensor<[1, 12, 192, 192], Float32, CPU>)[symbol_name:model.layers.0.self_attn.mask, op_type:CausalMask, op_options:{"sliding_window":false,"window_size":0}]
        addr:0x00000000000052 prog.free(%1285:tensor<[1, 12, 192, 192], Float32, CPU>) -> ()
        addr:0x00000000000053 prog.kernel_launch(%1286:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%1287:tensor<[1, 12, 192, 192], Float32, CPU>)[symbol_name:model.layers.0.self_attn.softmax, op_type:Softmax, op_options:{"axis":-1}]
        addr:0x00000000000054 prog.free(%1286:tensor<[1, 12, 192, 192], Float32, CPU>) -> ()
        addr:0x00000000000055 prog.kernel_launch(%1287:tensor<[1, 12, 192, 192], Float32, CPU>, %1282:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1288:tensor<[1, 12, 192, 128], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":false}]
        addr:0x00000000000056 prog.free(%1282:tensor<[1, 2, 192, 128], Float32, CPU>) -> ()
        addr:0x00000000000057 prog.free(%1287:tensor<[1, 12, 192, 192], Float32, CPU>) -> ()
        addr:0x00000000000058 prog.kernel_launch(%1288:tensor<[1, 12, 192, 128], Float32, CPU>) -> (%1289:tensor<[1, 192, 12, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x00000000000059 prog.free(%1288:tensor<[1, 12, 192, 128], Float32, CPU>) -> ()
        addr:0x0000000000005a prog.kernel_launch(%1289:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%1289:tensor<[1, 192, 1536], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,1536]}]
        addr:0x0000000000005b prog.kernel_launch(%1289:tensor<[1, 192, 1536], Float32, CPU>) -> (%1290:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.0.self_attn.o_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x0000000000005c prog.free(%1289:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x0000000000005d prog.ret
        addr:0x0000000000005e prog.label[symbol:model.layers.0.mlp.__entry]
        addr:0x0000000000005f prog.kernel_launch(%1292:tensor<[1, 192, 1536], Float32, CPU>) -> (%1293:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.0.mlp.gate_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x00000000000060 prog.kernel_launch(%1293:tensor<[1, 192, 8960], Float32, CPU>) -> (%1294:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.0.mlp.act, op_type:SiLU, op_options:null]
        addr:0x00000000000061 prog.free(%1293:tensor<[1, 192, 8960], Float32, CPU>) -> ()
        addr:0x00000000000062 prog.kernel_launch(%1292:tensor<[1, 192, 1536], Float32, CPU>) -> (%1295:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.0.mlp.up_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x00000000000063 prog.free(%1292:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x00000000000064 prog.kernel_launch(%1294:tensor<[1, 192, 8960], Float32, CPU>, %1295:tensor<[1, 192, 8960], Float32, CPU>) -> (%1296:tensor<[1, 192, 8960], Float32, CPU>)[op_type:Mul, op_options:null]
        addr:0x00000000000065 prog.free(%1295:tensor<[1, 192, 8960], Float32, CPU>) -> ()
        addr:0x00000000000066 prog.free(%1294:tensor<[1, 192, 8960], Float32, CPU>) -> ()
        addr:0x00000000000067 prog.kernel_launch(%1296:tensor<[1, 192, 8960], Float32, CPU>) -> (%1297:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.0.mlp.down_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":8960,"out_channels":1536}]
        addr:0x00000000000068 prog.free(%1296:tensor<[1, 192, 8960], Float32, CPU>) -> ()
        addr:0x00000000000069 prog.ret
        addr:0x0000000000006a prog.label[symbol:model.layers.1.__entry]
        addr:0x0000000000006b prog.kernel_launch(%1298:tensor<[1, 192, 1536], Float32, CPU>) -> (%1299:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.1.input_layernorm, op_type:RMSNorm, op_options:{"add_unit_offset":false,"epsilon":9.999999974752427e-07}]
        addr:0x0000000000006c prog.jump model.layers.1.self_attn.__entry[offset:10]
        addr:0x0000000000006d prog.kernel_launch(%1317:tensor<[1, 192, 1536], Float32, CPU>, %1298:tensor<[1, 192, 1536], Float32, CPU>) -> (%1318:tensor<[1, 192, 1536], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x0000000000006e prog.free(%1298:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x0000000000006f prog.free(%1317:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x00000000000070 prog.kernel_launch(%1318:tensor<[1, 192, 1536], Float32, CPU>) -> (%1319:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.1.post_attention_layernorm, op_type:RMSNorm, op_options:{"add_unit_offset":false,"epsilon":9.999999974752427e-07}]
        addr:0x00000000000071 prog.jump model.layers.1.mlp.__entry[offset:45]
        addr:0x00000000000072 prog.kernel_launch(%1324:tensor<[1, 192, 1536], Float32, CPU>, %1318:tensor<[1, 192, 1536], Float32, CPU>) -> (%1325:tensor<[1, 192, 1536], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x00000000000073 prog.free(%1318:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x00000000000074 prog.free(%1324:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x00000000000075 prog.ret
        addr:0x00000000000076 prog.label[symbol:model.layers.1.self_attn.__entry]
        addr:0x00000000000077 prog.kernel_launch(%1299:tensor<[1, 192, 1536], Float32, CPU>) -> (%1300:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.1.self_attn.q_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x00000000000078 prog.kernel_launch(%1299:tensor<[1, 192, 1536], Float32, CPU>) -> (%1301:tensor<[1, 192, 256], Float32, CPU>)[symbol_name:model.layers.1.self_attn.k_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x00000000000079 prog.kernel_launch(%1299:tensor<[1, 192, 1536], Float32, CPU>) -> (%1302:tensor<[1, 192, 256], Float32, CPU>)[symbol_name:model.layers.1.self_attn.v_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x0000000000007a prog.free(%1299:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x0000000000007b prog.kernel_launch(%1300:tensor<[1, 192, 1536], Float32, CPU>) -> (%1300:tensor<[1, 192, 12, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,12,128]}]
        addr:0x0000000000007c prog.kernel_launch(%1301:tensor<[1, 192, 256], Float32, CPU>) -> (%1301:tensor<[1, 192, 2, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,2,128]}]
        addr:0x0000000000007d prog.kernel_launch(%1302:tensor<[1, 192, 256], Float32, CPU>) -> (%1302:tensor<[1, 192, 2, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,2,128]}]
        addr:0x0000000000007e prog.kernel_launch(%1300:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%1303:tensor<[1, 12, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x0000000000007f prog.free(%1300:tensor<[1, 192, 12, 128], Float32, CPU>) -> ()
        addr:0x00000000000080 prog.kernel_launch(%1301:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%1304:tensor<[1, 2, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x00000000000081 prog.free(%1301:tensor<[1, 192, 2, 128], Float32, CPU>) -> ()
        addr:0x00000000000082 prog.kernel_launch(%1302:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%1305:tensor<[1, 2, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x00000000000083 prog.free(%1302:tensor<[1, 192, 2, 128], Float32, CPU>) -> ()
        addr:0x00000000000084 prog.kernel_launch(%1303:tensor<[1, 12, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1306:tensor<[1, 12, 192, 128], Float32, CPU>)[symbol_name:model.layers.1.self_attn.q_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x00000000000085 prog.free(%1303:tensor<[1, 12, 192, 128], Float32, CPU>) -> ()
        addr:0x00000000000086 prog.kernel_launch(%1304:tensor<[1, 2, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1307:tensor<[1, 2, 192, 128], Float32, CPU>)[symbol_name:model.layers.1.self_attn.k_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x00000000000087 prog.free(%1304:tensor<[1, 2, 192, 128], Float32, CPU>) -> ()
        addr:0x00000000000088 prog.kernel_launch(%1307:tensor<[1, 2, 192, 128], Float32, CPU>, %1305:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1308:tensor<[1, 2, 192, 128], Float32, CPU>, %1309:tensor<[1, 2, 192, 128], Float32, CPU>)[symbol_name:model.layers.1.self_attn.kv_cache, op_type:KVCache, op_options:{"head_dim":128,"kv_head":2,"layer_idx":0,"q_head":12,"use_fa2":false}]
        addr:0x00000000000089 prog.free(%1305:tensor<[1, 2, 192, 128], Float32, CPU>) -> ()
        addr:0x0000000000008a prog.free(%1307:tensor<[1, 2, 192, 128], Float32, CPU>) -> ()
        addr:0x0000000000008b prog.kernel_launch(%1306:tensor<[1, 12, 192, 128], Float32, CPU>, %1308:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1310:tensor<[1, 12, 192, 192], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":true}]
        addr:0x0000000000008c prog.free(%1308:tensor<[1, 2, 192, 128], Float32, CPU>) -> ()
        addr:0x0000000000008d prog.free(%1306:tensor<[1, 12, 192, 128], Float32, CPU>) -> ()
        addr:0x0000000000008e prog.kernel_launch(%1310:tensor<[1, 12, 192, 192], Float32, CPU>, %1311:tensor<[1], Float32, CPU>) -> (%1312:tensor<[1, 12, 192, 192], Float32, CPU>)[op_type:Mul, op_options:null]
        addr:0x0000000000008f prog.free(%1311:tensor<[1], Float32, CPU>) -> ()
        addr:0x00000000000090 prog.free(%1310:tensor<[1, 12, 192, 192], Float32, CPU>) -> ()
        addr:0x00000000000091 prog.kernel_launch(%1312:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%1313:tensor<[1, 12, 192, 192], Float32, CPU>)[symbol_name:model.layers.1.self_attn.mask, op_type:CausalMask, op_options:{"sliding_window":false,"window_size":0}]
        addr:0x00000000000092 prog.free(%1312:tensor<[1, 12, 192, 192], Float32, CPU>) -> ()
        addr:0x00000000000093 prog.kernel_launch(%1313:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%1314:tensor<[1, 12, 192, 192], Float32, CPU>)[symbol_name:model.layers.1.self_attn.softmax, op_type:Softmax, op_options:{"axis":-1}]
        addr:0x00000000000094 prog.free(%1313:tensor<[1, 12, 192, 192], Float32, CPU>) -> ()
        addr:0x00000000000095 prog.kernel_launch(%1314:tensor<[1, 12, 192, 192], Float32, CPU>, %1309:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1315:tensor<[1, 12, 192, 128], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":false}]
        addr:0x00000000000096 prog.free(%1309:tensor<[1, 2, 192, 128], Float32, CPU>) -> ()
        addr:0x00000000000097 prog.free(%1314:tensor<[1, 12, 192, 192], Float32, CPU>) -> ()
        addr:0x00000000000098 prog.kernel_launch(%1315:tensor<[1, 12, 192, 128], Float32, CPU>) -> (%1316:tensor<[1, 192, 12, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x00000000000099 prog.free(%1315:tensor<[1, 12, 192, 128], Float32, CPU>) -> ()
        addr:0x0000000000009a prog.kernel_launch(%1316:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%1316:tensor<[1, 192, 1536], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,1536]}]
        addr:0x0000000000009b prog.kernel_launch(%1316:tensor<[1, 192, 1536], Float32, CPU>) -> (%1317:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.1.self_attn.o_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x0000000000009c prog.free(%1316:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x0000000000009d prog.ret
        addr:0x0000000000009e prog.label[symbol:model.layers.1.mlp.__entry]
        addr:0x0000000000009f prog.kernel_launch(%1319:tensor<[1, 192, 1536], Float32, CPU>) -> (%1320:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.1.mlp.gate_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x000000000000a0 prog.kernel_launch(%1320:tensor<[1, 192, 8960], Float32, CPU>) -> (%1321:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.1.mlp.act, op_type:SiLU, op_options:null]
        addr:0x000000000000a1 prog.free(%1320:tensor<[1, 192, 8960], Float32, CPU>) -> ()
        addr:0x000000000000a2 prog.kernel_launch(%1319:tensor<[1, 192, 1536], Float32, CPU>) -> (%1322:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.1.mlp.up_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x000000000000a3 prog.free(%1319:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x000000000000a4 prog.kernel_launch(%1321:tensor<[1, 192, 8960], Float32, CPU>, %1322:tensor<[1, 192, 8960], Float32, CPU>) -> (%1323:tensor<[1, 192, 8960], Float32, CPU>)[op_type:Mul, op_options:null]
        addr:0x000000000000a5 prog.free(%1322:tensor<[1, 192, 8960], Float32, CPU>) -> ()
        addr:0x000000000000a6 prog.free(%1321:tensor<[1, 192, 8960], Float32, CPU>) -> ()
        addr:0x000000000000a7 prog.kernel_launch(%1323:tensor<[1, 192, 8960], Float32, CPU>) -> (%1324:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.1.mlp.down_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":8960,"out_channels":1536}]
        addr:0x000000000000a8 prog.free(%1323:tensor<[1, 192, 8960], Float32, CPU>) -> ()
        addr:0x000000000000a9 prog.ret
        addr:0x000000000000aa prog.label[symbol:model.layers.2.__entry]
        addr:0x000000000000ab prog.kernel_launch(%1325:tensor<[1, 192, 1536], Float32, CPU>) -> (%1326:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.2.input_layernorm, op_type:RMSNorm, op_options:{"add_unit_offset":false,"epsilon":9.999999974752427e-07}]
        addr:0x000000000000ac prog.jump model.layers.2.self_attn.__entry[offset:10]
        addr:0x000000000000ad prog.kernel_launch(%1344:tensor<[1, 192, 1536], Float32, CPU>, %1325:tensor<[1, 192, 1536], Float32, CPU>) -> (%1345:tensor<[1, 192, 1536], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x000000000000ae prog.free(%1325:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x000000000000af prog.free(%1344:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x000000000000b0 prog.kernel_launch(%1345:tensor<[1, 192, 1536], Float32, CPU>) -> (%1346:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.2.post_attention_layernorm, op_type:RMSNorm, op_options:{"add_unit_offset":false,"epsilon":9.999999974752427e-07}]
        addr:0x000000000000b1 prog.jump model.layers.2.mlp.__entry[offset:45]
        addr:0x000000000000b2 prog.kernel_launch(%1351:tensor<[1, 192, 1536], Float32, CPU>, %1345:tensor<[1, 192, 1536], Float32, CPU>) -> (%1352:tensor<[1, 192, 1536], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x000000000000b3 prog.free(%1345:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x000000000000b4 prog.free(%1351:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x000000000000b5 prog.ret
        addr:0x000000000000b6 prog.label[symbol:model.layers.2.self_attn.__entry]
        addr:0x000000000000b7 prog.kernel_launch(%1326:tensor<[1, 192, 1536], Float32, CPU>) -> (%1327:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.2.self_attn.q_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x000000000000b8 prog.kernel_launch(%1326:tensor<[1, 192, 1536], Float32, CPU>) -> (%1328:tensor<[1, 192, 256], Float32, CPU>)[symbol_name:model.layers.2.self_attn.k_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x000000000000b9 prog.kernel_launch(%1326:tensor<[1, 192, 1536], Float32, CPU>) -> (%1329:tensor<[1, 192, 256], Float32, CPU>)[symbol_name:model.layers.2.self_attn.v_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x000000000000ba prog.free(%1326:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x000000000000bb prog.kernel_launch(%1327:tensor<[1, 192, 1536], Float32, CPU>) -> (%1327:tensor<[1, 192, 12, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,12,128]}]
        addr:0x000000000000bc prog.kernel_launch(%1328:tensor<[1, 192, 256], Float32, CPU>) -> (%1328:tensor<[1, 192, 2, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,2,128]}]
        addr:0x000000000000bd prog.kernel_launch(%1329:tensor<[1, 192, 256], Float32, CPU>) -> (%1329:tensor<[1, 192, 2, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,2,128]}]
        addr:0x000000000000be prog.kernel_launch(%1327:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%1330:tensor<[1, 12, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x000000000000bf prog.free(%1327:tensor<[1, 192, 12, 128], Float32, CPU>) -> ()
        addr:0x000000000000c0 prog.kernel_launch(%1328:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%1331:tensor<[1, 2, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x000000000000c1 prog.free(%1328:tensor<[1, 192, 2, 128], Float32, CPU>) -> ()
        addr:0x000000000000c2 prog.kernel_launch(%1329:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%1332:tensor<[1, 2, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x000000000000c3 prog.free(%1329:tensor<[1, 192, 2, 128], Float32, CPU>) -> ()
        addr:0x000000000000c4 prog.kernel_launch(%1330:tensor<[1, 12, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1333:tensor<[1, 12, 192, 128], Float32, CPU>)[symbol_name:model.layers.2.self_attn.q_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x000000000000c5 prog.free(%1330:tensor<[1, 12, 192, 128], Float32, CPU>) -> ()
        addr:0x000000000000c6 prog.kernel_launch(%1331:tensor<[1, 2, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1334:tensor<[1, 2, 192, 128], Float32, CPU>)[symbol_name:model.layers.2.self_attn.k_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x000000000000c7 prog.free(%1331:tensor<[1, 2, 192, 128], Float32, CPU>) -> ()
        addr:0x000000000000c8 prog.kernel_launch(%1334:tensor<[1, 2, 192, 128], Float32, CPU>, %1332:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1335:tensor<[1, 2, 192, 128], Float32, CPU>, %1336:tensor<[1, 2, 192, 128], Float32, CPU>)[symbol_name:model.layers.2.self_attn.kv_cache, op_type:KVCache, op_options:{"head_dim":128,"kv_head":2,"layer_idx":0,"q_head":12,"use_fa2":false}]
        addr:0x000000000000c9 prog.free(%1332:tensor<[1, 2, 192, 128], Float32, CPU>) -> ()
        addr:0x000000000000ca prog.free(%1334:tensor<[1, 2, 192, 128], Float32, CPU>) -> ()
        addr:0x000000000000cb prog.kernel_launch(%1333:tensor<[1, 12, 192, 128], Float32, CPU>, %1335:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1337:tensor<[1, 12, 192, 192], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":true}]
        addr:0x000000000000cc prog.free(%1335:tensor<[1, 2, 192, 128], Float32, CPU>) -> ()
        addr:0x000000000000cd prog.free(%1333:tensor<[1, 12, 192, 128], Float32, CPU>) -> ()
        addr:0x000000000000ce prog.kernel_launch(%1337:tensor<[1, 12, 192, 192], Float32, CPU>, %1338:tensor<[1], Float32, CPU>) -> (%1339:tensor<[1, 12, 192, 192], Float32, CPU>)[op_type:Mul, op_options:null]
        addr:0x000000000000cf prog.free(%1338:tensor<[1], Float32, CPU>) -> ()
        addr:0x000000000000d0 prog.free(%1337:tensor<[1, 12, 192, 192], Float32, CPU>) -> ()
        addr:0x000000000000d1 prog.kernel_launch(%1339:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%1340:tensor<[1, 12, 192, 192], Float32, CPU>)[symbol_name:model.layers.2.self_attn.mask, op_type:CausalMask, op_options:{"sliding_window":false,"window_size":0}]
        addr:0x000000000000d2 prog.free(%1339:tensor<[1, 12, 192, 192], Float32, CPU>) -> ()
        addr:0x000000000000d3 prog.kernel_launch(%1340:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%1341:tensor<[1, 12, 192, 192], Float32, CPU>)[symbol_name:model.layers.2.self_attn.softmax, op_type:Softmax, op_options:{"axis":-1}]
        addr:0x000000000000d4 prog.free(%1340:tensor<[1, 12, 192, 192], Float32, CPU>) -> ()
        addr:0x000000000000d5 prog.kernel_launch(%1341:tensor<[1, 12, 192, 192], Float32, CPU>, %1336:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1342:tensor<[1, 12, 192, 128], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":false}]
        addr:0x000000000000d6 prog.free(%1336:tensor<[1, 2, 192, 128], Float32, CPU>) -> ()
        addr:0x000000000000d7 prog.free(%1341:tensor<[1, 12, 192, 192], Float32, CPU>) -> ()
        addr:0x000000000000d8 prog.kernel_launch(%1342:tensor<[1, 12, 192, 128], Float32, CPU>) -> (%1343:tensor<[1, 192, 12, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x000000000000d9 prog.free(%1342:tensor<[1, 12, 192, 128], Float32, CPU>) -> ()
        addr:0x000000000000da prog.kernel_launch(%1343:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%1343:tensor<[1, 192, 1536], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,1536]}]
        addr:0x000000000000db prog.kernel_launch(%1343:tensor<[1, 192, 1536], Float32, CPU>) -> (%1344:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.2.self_attn.o_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x000000000000dc prog.free(%1343:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x000000000000dd prog.ret
        addr:0x000000000000de prog.label[symbol:model.layers.2.mlp.__entry]
        addr:0x000000000000df prog.kernel_launch(%1346:tensor<[1, 192, 1536], Float32, CPU>) -> (%1347:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.2.mlp.gate_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x000000000000e0 prog.kernel_launch(%1347:tensor<[1, 192, 8960], Float32, CPU>) -> (%1348:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.2.mlp.act, op_type:SiLU, op_options:null]
        addr:0x000000000000e1 prog.free(%1347:tensor<[1, 192, 8960], Float32, CPU>) -> ()
        addr:0x000000000000e2 prog.kernel_launch(%1346:tensor<[1, 192, 1536], Float32, CPU>) -> (%1349:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.2.mlp.up_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x000000000000e3 prog.free(%1346:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x000000000000e4 prog.kernel_launch(%1348:tensor<[1, 192, 8960], Float32, CPU>, %1349:tensor<[1, 192, 8960], Float32, CPU>) -> (%1350:tensor<[1, 192, 8960], Float32, CPU>)[op_type:Mul, op_options:null]
        addr:0x000000000000e5 prog.free(%1349:tensor<[1, 192, 8960], Float32, CPU>) -> ()
        addr:0x000000000000e6 prog.free(%1348:tensor<[1, 192, 8960], Float32, CPU>) -> ()
        addr:0x000000000000e7 prog.kernel_launch(%1350:tensor<[1, 192, 8960], Float32, CPU>) -> (%1351:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.2.mlp.down_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":8960,"out_channels":1536}]
        addr:0x000000000000e8 prog.free(%1350:tensor<[1, 192, 8960], Float32, CPU>) -> ()
        addr:0x000000000000e9 prog.ret
        addr:0x000000000000ea prog.label[symbol:model.layers.3.__entry]
        addr:0x000000000000eb prog.kernel_launch(%1352:tensor<[1, 192, 1536], Float32, CPU>) -> (%1353:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.3.input_layernorm, op_type:RMSNorm, op_options:{"add_unit_offset":false,"epsilon":9.999999974752427e-07}]
        addr:0x000000000000ec prog.jump model.layers.3.self_attn.__entry[offset:10]
        addr:0x000000000000ed prog.kernel_launch(%1371:tensor<[1, 192, 1536], Float32, CPU>, %1352:tensor<[1, 192, 1536], Float32, CPU>) -> (%1372:tensor<[1, 192, 1536], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x000000000000ee prog.free(%1352:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x000000000000ef prog.free(%1371:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x000000000000f0 prog.kernel_launch(%1372:tensor<[1, 192, 1536], Float32, CPU>) -> (%1373:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.3.post_attention_layernorm, op_type:RMSNorm, op_options:{"add_unit_offset":false,"epsilon":9.999999974752427e-07}]
        addr:0x000000000000f1 prog.jump model.layers.3.mlp.__entry[offset:45]
        addr:0x000000000000f2 prog.kernel_launch(%1378:tensor<[1, 192, 1536], Float32, CPU>, %1372:tensor<[1, 192, 1536], Float32, CPU>) -> (%1379:tensor<[1, 192, 1536], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x000000000000f3 prog.free(%1372:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x000000000000f4 prog.free(%1378:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x000000000000f5 prog.ret
        addr:0x000000000000f6 prog.label[symbol:model.layers.3.self_attn.__entry]
        addr:0x000000000000f7 prog.kernel_launch(%1353:tensor<[1, 192, 1536], Float32, CPU>) -> (%1354:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.3.self_attn.q_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x000000000000f8 prog.kernel_launch(%1353:tensor<[1, 192, 1536], Float32, CPU>) -> (%1355:tensor<[1, 192, 256], Float32, CPU>)[symbol_name:model.layers.3.self_attn.k_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x000000000000f9 prog.kernel_launch(%1353:tensor<[1, 192, 1536], Float32, CPU>) -> (%1356:tensor<[1, 192, 256], Float32, CPU>)[symbol_name:model.layers.3.self_attn.v_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x000000000000fa prog.free(%1353:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x000000000000fb prog.kernel_launch(%1354:tensor<[1, 192, 1536], Float32, CPU>) -> (%1354:tensor<[1, 192, 12, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,12,128]}]
        addr:0x000000000000fc prog.kernel_launch(%1355:tensor<[1, 192, 256], Float32, CPU>) -> (%1355:tensor<[1, 192, 2, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,2,128]}]
        addr:0x000000000000fd prog.kernel_launch(%1356:tensor<[1, 192, 256], Float32, CPU>) -> (%1356:tensor<[1, 192, 2, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,2,128]}]
        addr:0x000000000000fe prog.kernel_launch(%1354:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%1357:tensor<[1, 12, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x000000000000ff prog.free(%1354:tensor<[1, 192, 12, 128], Float32, CPU>) -> ()
        addr:0x00000000000100 prog.kernel_launch(%1355:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%1358:tensor<[1, 2, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x00000000000101 prog.free(%1355:tensor<[1, 192, 2, 128], Float32, CPU>) -> ()
        addr:0x00000000000102 prog.kernel_launch(%1356:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%1359:tensor<[1, 2, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x00000000000103 prog.free(%1356:tensor<[1, 192, 2, 128], Float32, CPU>) -> ()
        addr:0x00000000000104 prog.kernel_launch(%1357:tensor<[1, 12, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1360:tensor<[1, 12, 192, 128], Float32, CPU>)[symbol_name:model.layers.3.self_attn.q_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x00000000000105 prog.free(%1357:tensor<[1, 12, 192, 128], Float32, CPU>) -> ()
        addr:0x00000000000106 prog.kernel_launch(%1358:tensor<[1, 2, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1361:tensor<[1, 2, 192, 128], Float32, CPU>)[symbol_name:model.layers.3.self_attn.k_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x00000000000107 prog.free(%1358:tensor<[1, 2, 192, 128], Float32, CPU>) -> ()
        addr:0x00000000000108 prog.kernel_launch(%1361:tensor<[1, 2, 192, 128], Float32, CPU>, %1359:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1362:tensor<[1, 2, 192, 128], Float32, CPU>, %1363:tensor<[1, 2, 192, 128], Float32, CPU>)[symbol_name:model.layers.3.self_attn.kv_cache, op_type:KVCache, op_options:{"head_dim":128,"kv_head":2,"layer_idx":0,"q_head":12,"use_fa2":false}]
        addr:0x00000000000109 prog.free(%1359:tensor<[1, 2, 192, 128], Float32, CPU>) -> ()
        addr:0x0000000000010a prog.free(%1361:tensor<[1, 2, 192, 128], Float32, CPU>) -> ()
        addr:0x0000000000010b prog.kernel_launch(%1360:tensor<[1, 12, 192, 128], Float32, CPU>, %1362:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1364:tensor<[1, 12, 192, 192], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":true}]
        addr:0x0000000000010c prog.free(%1362:tensor<[1, 2, 192, 128], Float32, CPU>) -> ()
        addr:0x0000000000010d prog.free(%1360:tensor<[1, 12, 192, 128], Float32, CPU>) -> ()
        addr:0x0000000000010e prog.kernel_launch(%1364:tensor<[1, 12, 192, 192], Float32, CPU>, %1365:tensor<[1], Float32, CPU>) -> (%1366:tensor<[1, 12, 192, 192], Float32, CPU>)[op_type:Mul, op_options:null]
        addr:0x0000000000010f prog.free(%1365:tensor<[1], Float32, CPU>) -> ()
        addr:0x00000000000110 prog.free(%1364:tensor<[1, 12, 192, 192], Float32, CPU>) -> ()
        addr:0x00000000000111 prog.kernel_launch(%1366:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%1367:tensor<[1, 12, 192, 192], Float32, CPU>)[symbol_name:model.layers.3.self_attn.mask, op_type:CausalMask, op_options:{"sliding_window":false,"window_size":0}]
        addr:0x00000000000112 prog.free(%1366:tensor<[1, 12, 192, 192], Float32, CPU>) -> ()
        addr:0x00000000000113 prog.kernel_launch(%1367:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%1368:tensor<[1, 12, 192, 192], Float32, CPU>)[symbol_name:model.layers.3.self_attn.softmax, op_type:Softmax, op_options:{"axis":-1}]
        addr:0x00000000000114 prog.free(%1367:tensor<[1, 12, 192, 192], Float32, CPU>) -> ()
        addr:0x00000000000115 prog.kernel_launch(%1368:tensor<[1, 12, 192, 192], Float32, CPU>, %1363:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1369:tensor<[1, 12, 192, 128], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":false}]
        addr:0x00000000000116 prog.free(%1363:tensor<[1, 2, 192, 128], Float32, CPU>) -> ()
        addr:0x00000000000117 prog.free(%1368:tensor<[1, 12, 192, 192], Float32, CPU>) -> ()
        addr:0x00000000000118 prog.kernel_launch(%1369:tensor<[1, 12, 192, 128], Float32, CPU>) -> (%1370:tensor<[1, 192, 12, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x00000000000119 prog.free(%1369:tensor<[1, 12, 192, 128], Float32, CPU>) -> ()
        addr:0x0000000000011a prog.kernel_launch(%1370:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%1370:tensor<[1, 192, 1536], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,1536]}]
        addr:0x0000000000011b prog.kernel_launch(%1370:tensor<[1, 192, 1536], Float32, CPU>) -> (%1371:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.3.self_attn.o_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x0000000000011c prog.free(%1370:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x0000000000011d prog.ret
        addr:0x0000000000011e prog.label[symbol:model.layers.3.mlp.__entry]
        addr:0x0000000000011f prog.kernel_launch(%1373:tensor<[1, 192, 1536], Float32, CPU>) -> (%1374:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.3.mlp.gate_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x00000000000120 prog.kernel_launch(%1374:tensor<[1, 192, 8960], Float32, CPU>) -> (%1375:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.3.mlp.act, op_type:SiLU, op_options:null]
        addr:0x00000000000121 prog.free(%1374:tensor<[1, 192, 8960], Float32, CPU>) -> ()
        addr:0x00000000000122 prog.kernel_launch(%1373:tensor<[1, 192, 1536], Float32, CPU>) -> (%1376:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.3.mlp.up_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x00000000000123 prog.free(%1373:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x00000000000124 prog.kernel_launch(%1375:tensor<[1, 192, 8960], Float32, CPU>, %1376:tensor<[1, 192, 8960], Float32, CPU>) -> (%1377:tensor<[1, 192, 8960], Float32, CPU>)[op_type:Mul, op_options:null]
        addr:0x00000000000125 prog.free(%1376:tensor<[1, 192, 8960], Float32, CPU>) -> ()
        addr:0x00000000000126 prog.free(%1375:tensor<[1, 192, 8960], Float32, CPU>) -> ()
        addr:0x00000000000127 prog.kernel_launch(%1377:tensor<[1, 192, 8960], Float32, CPU>) -> (%1378:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.3.mlp.down_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":8960,"out_channels":1536}]
        addr:0x00000000000128 prog.free(%1377:tensor<[1, 192, 8960], Float32, CPU>) -> ()
        addr:0x00000000000129 prog.ret
        addr:0x0000000000012a prog.label[symbol:model.layers.4.__entry]
        addr:0x0000000000012b prog.kernel_launch(%1379:tensor<[1, 192, 1536], Float32, CPU>) -> (%1380:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.4.input_layernorm, op_type:RMSNorm, op_options:{"add_unit_offset":false,"epsilon":9.999999974752427e-07}]
        addr:0x0000000000012c prog.jump model.layers.4.self_attn.__entry[offset:10]
        addr:0x0000000000012d prog.kernel_launch(%1398:tensor<[1, 192, 1536], Float32, CPU>, %1379:tensor<[1, 192, 1536], Float32, CPU>) -> (%1399:tensor<[1, 192, 1536], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x0000000000012e prog.free(%1379:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x0000000000012f prog.free(%1398:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x00000000000130 prog.kernel_launch(%1399:tensor<[1, 192, 1536], Float32, CPU>) -> (%1400:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.4.post_attention_layernorm, op_type:RMSNorm, op_options:{"add_unit_offset":false,"epsilon":9.999999974752427e-07}]
        addr:0x00000000000131 prog.jump model.layers.4.mlp.__entry[offset:45]
        addr:0x00000000000132 prog.kernel_launch(%1405:tensor<[1, 192, 1536], Float32, CPU>, %1399:tensor<[1, 192, 1536], Float32, CPU>) -> (%1406:tensor<[1, 192, 1536], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x00000000000133 prog.free(%1399:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x00000000000134 prog.free(%1405:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x00000000000135 prog.ret
        addr:0x00000000000136 prog.label[symbol:model.layers.4.self_attn.__entry]
        addr:0x00000000000137 prog.kernel_launch(%1380:tensor<[1, 192, 1536], Float32, CPU>) -> (%1381:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.4.self_attn.q_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x00000000000138 prog.kernel_launch(%1380:tensor<[1, 192, 1536], Float32, CPU>) -> (%1382:tensor<[1, 192, 256], Float32, CPU>)[symbol_name:model.layers.4.self_attn.k_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x00000000000139 prog.kernel_launch(%1380:tensor<[1, 192, 1536], Float32, CPU>) -> (%1383:tensor<[1, 192, 256], Float32, CPU>)[symbol_name:model.layers.4.self_attn.v_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x0000000000013a prog.free(%1380:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x0000000000013b prog.kernel_launch(%1381:tensor<[1, 192, 1536], Float32, CPU>) -> (%1381:tensor<[1, 192, 12, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,12,128]}]
        addr:0x0000000000013c prog.kernel_launch(%1382:tensor<[1, 192, 256], Float32, CPU>) -> (%1382:tensor<[1, 192, 2, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,2,128]}]
        addr:0x0000000000013d prog.kernel_launch(%1383:tensor<[1, 192, 256], Float32, CPU>) -> (%1383:tensor<[1, 192, 2, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,2,128]}]
        addr:0x0000000000013e prog.kernel_launch(%1381:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%1384:tensor<[1, 12, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x0000000000013f prog.free(%1381:tensor<[1, 192, 12, 128], Float32, CPU>) -> ()
        addr:0x00000000000140 prog.kernel_launch(%1382:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%1385:tensor<[1, 2, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x00000000000141 prog.free(%1382:tensor<[1, 192, 2, 128], Float32, CPU>) -> ()
        addr:0x00000000000142 prog.kernel_launch(%1383:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%1386:tensor<[1, 2, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x00000000000143 prog.free(%1383:tensor<[1, 192, 2, 128], Float32, CPU>) -> ()
        addr:0x00000000000144 prog.kernel_launch(%1384:tensor<[1, 12, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1387:tensor<[1, 12, 192, 128], Float32, CPU>)[symbol_name:model.layers.4.self_attn.q_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x00000000000145 prog.free(%1384:tensor<[1, 12, 192, 128], Float32, CPU>) -> ()
        addr:0x00000000000146 prog.kernel_launch(%1385:tensor<[1, 2, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1388:tensor<[1, 2, 192, 128], Float32, CPU>)[symbol_name:model.layers.4.self_attn.k_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x00000000000147 prog.free(%1385:tensor<[1, 2, 192, 128], Float32, CPU>) -> ()
        addr:0x00000000000148 prog.kernel_launch(%1388:tensor<[1, 2, 192, 128], Float32, CPU>, %1386:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1389:tensor<[1, 2, 192, 128], Float32, CPU>, %1390:tensor<[1, 2, 192, 128], Float32, CPU>)[symbol_name:model.layers.4.self_attn.kv_cache, op_type:KVCache, op_options:{"head_dim":128,"kv_head":2,"layer_idx":0,"q_head":12,"use_fa2":false}]
        addr:0x00000000000149 prog.free(%1386:tensor<[1, 2, 192, 128], Float32, CPU>) -> ()
        addr:0x0000000000014a prog.free(%1388:tensor<[1, 2, 192, 128], Float32, CPU>) -> ()
        addr:0x0000000000014b prog.kernel_launch(%1387:tensor<[1, 12, 192, 128], Float32, CPU>, %1389:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1391:tensor<[1, 12, 192, 192], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":true}]
        addr:0x0000000000014c prog.free(%1389:tensor<[1, 2, 192, 128], Float32, CPU>) -> ()
        addr:0x0000000000014d prog.free(%1387:tensor<[1, 12, 192, 128], Float32, CPU>) -> ()
        addr:0x0000000000014e prog.kernel_launch(%1391:tensor<[1, 12, 192, 192], Float32, CPU>, %1392:tensor<[1], Float32, CPU>) -> (%1393:tensor<[1, 12, 192, 192], Float32, CPU>)[op_type:Mul, op_options:null]
        addr:0x0000000000014f prog.free(%1392:tensor<[1], Float32, CPU>) -> ()
        addr:0x00000000000150 prog.free(%1391:tensor<[1, 12, 192, 192], Float32, CPU>) -> ()
        addr:0x00000000000151 prog.kernel_launch(%1393:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%1394:tensor<[1, 12, 192, 192], Float32, CPU>)[symbol_name:model.layers.4.self_attn.mask, op_type:CausalMask, op_options:{"sliding_window":false,"window_size":0}]
        addr:0x00000000000152 prog.free(%1393:tensor<[1, 12, 192, 192], Float32, CPU>) -> ()
        addr:0x00000000000153 prog.kernel_launch(%1394:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%1395:tensor<[1, 12, 192, 192], Float32, CPU>)[symbol_name:model.layers.4.self_attn.softmax, op_type:Softmax, op_options:{"axis":-1}]
        addr:0x00000000000154 prog.free(%1394:tensor<[1, 12, 192, 192], Float32, CPU>) -> ()
        addr:0x00000000000155 prog.kernel_launch(%1395:tensor<[1, 12, 192, 192], Float32, CPU>, %1390:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1396:tensor<[1, 12, 192, 128], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":false}]
        addr:0x00000000000156 prog.free(%1390:tensor<[1, 2, 192, 128], Float32, CPU>) -> ()
        addr:0x00000000000157 prog.free(%1395:tensor<[1, 12, 192, 192], Float32, CPU>) -> ()
        addr:0x00000000000158 prog.kernel_launch(%1396:tensor<[1, 12, 192, 128], Float32, CPU>) -> (%1397:tensor<[1, 192, 12, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x00000000000159 prog.free(%1396:tensor<[1, 12, 192, 128], Float32, CPU>) -> ()
        addr:0x0000000000015a prog.kernel_launch(%1397:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%1397:tensor<[1, 192, 1536], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,1536]}]
        addr:0x0000000000015b prog.kernel_launch(%1397:tensor<[1, 192, 1536], Float32, CPU>) -> (%1398:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.4.self_attn.o_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x0000000000015c prog.free(%1397:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x0000000000015d prog.ret
        addr:0x0000000000015e prog.label[symbol:model.layers.4.mlp.__entry]
        addr:0x0000000000015f prog.kernel_launch(%1400:tensor<[1, 192, 1536], Float32, CPU>) -> (%1401:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.4.mlp.gate_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x00000000000160 prog.kernel_launch(%1401:tensor<[1, 192, 8960], Float32, CPU>) -> (%1402:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.4.mlp.act, op_type:SiLU, op_options:null]
        addr:0x00000000000161 prog.free(%1401:tensor<[1, 192, 8960], Float32, CPU>) -> ()
        addr:0x00000000000162 prog.kernel_launch(%1400:tensor<[1, 192, 1536], Float32, CPU>) -> (%1403:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.4.mlp.up_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x00000000000163 prog.free(%1400:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x00000000000164 prog.kernel_launch(%1402:tensor<[1, 192, 8960], Float32, CPU>, %1403:tensor<[1, 192, 8960], Float32, CPU>) -> (%1404:tensor<[1, 192, 8960], Float32, CPU>)[op_type:Mul, op_options:null]
        addr:0x00000000000165 prog.free(%1403:tensor<[1, 192, 8960], Float32, CPU>) -> ()
        addr:0x00000000000166 prog.free(%1402:tensor<[1, 192, 8960], Float32, CPU>) -> ()
        addr:0x00000000000167 prog.kernel_launch(%1404:tensor<[1, 192, 8960], Float32, CPU>) -> (%1405:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.4.mlp.down_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":8960,"out_channels":1536}]
        addr:0x00000000000168 prog.free(%1404:tensor<[1, 192, 8960], Float32, CPU>) -> ()
        addr:0x00000000000169 prog.ret
        addr:0x0000000000016a prog.label[symbol:model.layers.5.__entry]
        addr:0x0000000000016b prog.kernel_launch(%1406:tensor<[1, 192, 1536], Float32, CPU>) -> (%1407:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.5.input_layernorm, op_type:RMSNorm, op_options:{"add_unit_offset":false,"epsilon":9.999999974752427e-07}]
        addr:0x0000000000016c prog.jump model.layers.5.self_attn.__entry[offset:10]
        addr:0x0000000000016d prog.kernel_launch(%1425:tensor<[1, 192, 1536], Float32, CPU>, %1406:tensor<[1, 192, 1536], Float32, CPU>) -> (%1426:tensor<[1, 192, 1536], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x0000000000016e prog.free(%1406:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x0000000000016f prog.free(%1425:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x00000000000170 prog.kernel_launch(%1426:tensor<[1, 192, 1536], Float32, CPU>) -> (%1427:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.5.post_attention_layernorm, op_type:RMSNorm, op_options:{"add_unit_offset":false,"epsilon":9.999999974752427e-07}]
        addr:0x00000000000171 prog.jump model.layers.5.mlp.__entry[offset:45]
        addr:0x00000000000172 prog.kernel_launch(%1432:tensor<[1, 192, 1536], Float32, CPU>, %1426:tensor<[1, 192, 1536], Float32, CPU>) -> (%1433:tensor<[1, 192, 1536], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x00000000000173 prog.free(%1426:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x00000000000174 prog.free(%1432:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x00000000000175 prog.ret
        addr:0x00000000000176 prog.label[symbol:model.layers.5.self_attn.__entry]
        addr:0x00000000000177 prog.kernel_launch(%1407:tensor<[1, 192, 1536], Float32, CPU>) -> (%1408:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.5.self_attn.q_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x00000000000178 prog.kernel_launch(%1407:tensor<[1, 192, 1536], Float32, CPU>) -> (%1409:tensor<[1, 192, 256], Float32, CPU>)[symbol_name:model.layers.5.self_attn.k_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x00000000000179 prog.kernel_launch(%1407:tensor<[1, 192, 1536], Float32, CPU>) -> (%1410:tensor<[1, 192, 256], Float32, CPU>)[symbol_name:model.layers.5.self_attn.v_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x0000000000017a prog.free(%1407:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x0000000000017b prog.kernel_launch(%1408:tensor<[1, 192, 1536], Float32, CPU>) -> (%1408:tensor<[1, 192, 12, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,12,128]}]
        addr:0x0000000000017c prog.kernel_launch(%1409:tensor<[1, 192, 256], Float32, CPU>) -> (%1409:tensor<[1, 192, 2, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,2,128]}]
        addr:0x0000000000017d prog.kernel_launch(%1410:tensor<[1, 192, 256], Float32, CPU>) -> (%1410:tensor<[1, 192, 2, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,2,128]}]
        addr:0x0000000000017e prog.kernel_launch(%1408:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%1411:tensor<[1, 12, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x0000000000017f prog.free(%1408:tensor<[1, 192, 12, 128], Float32, CPU>) -> ()
        addr:0x00000000000180 prog.kernel_launch(%1409:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%1412:tensor<[1, 2, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x00000000000181 prog.free(%1409:tensor<[1, 192, 2, 128], Float32, CPU>) -> ()
        addr:0x00000000000182 prog.kernel_launch(%1410:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%1413:tensor<[1, 2, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x00000000000183 prog.free(%1410:tensor<[1, 192, 2, 128], Float32, CPU>) -> ()
        addr:0x00000000000184 prog.kernel_launch(%1411:tensor<[1, 12, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1414:tensor<[1, 12, 192, 128], Float32, CPU>)[symbol_name:model.layers.5.self_attn.q_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x00000000000185 prog.free(%1411:tensor<[1, 12, 192, 128], Float32, CPU>) -> ()
        addr:0x00000000000186 prog.kernel_launch(%1412:tensor<[1, 2, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1415:tensor<[1, 2, 192, 128], Float32, CPU>)[symbol_name:model.layers.5.self_attn.k_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x00000000000187 prog.free(%1412:tensor<[1, 2, 192, 128], Float32, CPU>) -> ()
        addr:0x00000000000188 prog.kernel_launch(%1415:tensor<[1, 2, 192, 128], Float32, CPU>, %1413:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1416:tensor<[1, 2, 192, 128], Float32, CPU>, %1417:tensor<[1, 2, 192, 128], Float32, CPU>)[symbol_name:model.layers.5.self_attn.kv_cache, op_type:KVCache, op_options:{"head_dim":128,"kv_head":2,"layer_idx":0,"q_head":12,"use_fa2":false}]
        addr:0x00000000000189 prog.free(%1413:tensor<[1, 2, 192, 128], Float32, CPU>) -> ()
        addr:0x0000000000018a prog.free(%1415:tensor<[1, 2, 192, 128], Float32, CPU>) -> ()
        addr:0x0000000000018b prog.kernel_launch(%1414:tensor<[1, 12, 192, 128], Float32, CPU>, %1416:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1418:tensor<[1, 12, 192, 192], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":true}]
        addr:0x0000000000018c prog.free(%1416:tensor<[1, 2, 192, 128], Float32, CPU>) -> ()
        addr:0x0000000000018d prog.free(%1414:tensor<[1, 12, 192, 128], Float32, CPU>) -> ()
        addr:0x0000000000018e prog.kernel_launch(%1418:tensor<[1, 12, 192, 192], Float32, CPU>, %1419:tensor<[1], Float32, CPU>) -> (%1420:tensor<[1, 12, 192, 192], Float32, CPU>)[op_type:Mul, op_options:null]
        addr:0x0000000000018f prog.free(%1419:tensor<[1], Float32, CPU>) -> ()
        addr:0x00000000000190 prog.free(%1418:tensor<[1, 12, 192, 192], Float32, CPU>) -> ()
        addr:0x00000000000191 prog.kernel_launch(%1420:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%1421:tensor<[1, 12, 192, 192], Float32, CPU>)[symbol_name:model.layers.5.self_attn.mask, op_type:CausalMask, op_options:{"sliding_window":false,"window_size":0}]
        addr:0x00000000000192 prog.free(%1420:tensor<[1, 12, 192, 192], Float32, CPU>) -> ()
        addr:0x00000000000193 prog.kernel_launch(%1421:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%1422:tensor<[1, 12, 192, 192], Float32, CPU>)[symbol_name:model.layers.5.self_attn.softmax, op_type:Softmax, op_options:{"axis":-1}]
        addr:0x00000000000194 prog.free(%1421:tensor<[1, 12, 192, 192], Float32, CPU>) -> ()
        addr:0x00000000000195 prog.kernel_launch(%1422:tensor<[1, 12, 192, 192], Float32, CPU>, %1417:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1423:tensor<[1, 12, 192, 128], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":false}]
        addr:0x00000000000196 prog.free(%1417:tensor<[1, 2, 192, 128], Float32, CPU>) -> ()
        addr:0x00000000000197 prog.free(%1422:tensor<[1, 12, 192, 192], Float32, CPU>) -> ()
        addr:0x00000000000198 prog.kernel_launch(%1423:tensor<[1, 12, 192, 128], Float32, CPU>) -> (%1424:tensor<[1, 192, 12, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x00000000000199 prog.free(%1423:tensor<[1, 12, 192, 128], Float32, CPU>) -> ()
        addr:0x0000000000019a prog.kernel_launch(%1424:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%1424:tensor<[1, 192, 1536], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,1536]}]
        addr:0x0000000000019b prog.kernel_launch(%1424:tensor<[1, 192, 1536], Float32, CPU>) -> (%1425:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.5.self_attn.o_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x0000000000019c prog.free(%1424:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x0000000000019d prog.ret
        addr:0x0000000000019e prog.label[symbol:model.layers.5.mlp.__entry]
        addr:0x0000000000019f prog.kernel_launch(%1427:tensor<[1, 192, 1536], Float32, CPU>) -> (%1428:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.5.mlp.gate_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x000000000001a0 prog.kernel_launch(%1428:tensor<[1, 192, 8960], Float32, CPU>) -> (%1429:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.5.mlp.act, op_type:SiLU, op_options:null]
        addr:0x000000000001a1 prog.free(%1428:tensor<[1, 192, 8960], Float32, CPU>) -> ()
        addr:0x000000000001a2 prog.kernel_launch(%1427:tensor<[1, 192, 1536], Float32, CPU>) -> (%1430:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.5.mlp.up_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x000000000001a3 prog.free(%1427:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x000000000001a4 prog.kernel_launch(%1429:tensor<[1, 192, 8960], Float32, CPU>, %1430:tensor<[1, 192, 8960], Float32, CPU>) -> (%1431:tensor<[1, 192, 8960], Float32, CPU>)[op_type:Mul, op_options:null]
        addr:0x000000000001a5 prog.free(%1430:tensor<[1, 192, 8960], Float32, CPU>) -> ()
        addr:0x000000000001a6 prog.free(%1429:tensor<[1, 192, 8960], Float32, CPU>) -> ()
        addr:0x000000000001a7 prog.kernel_launch(%1431:tensor<[1, 192, 8960], Float32, CPU>) -> (%1432:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.5.mlp.down_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":8960,"out_channels":1536}]
        addr:0x000000000001a8 prog.free(%1431:tensor<[1, 192, 8960], Float32, CPU>) -> ()
        addr:0x000000000001a9 prog.ret
        addr:0x000000000001aa prog.label[symbol:model.layers.6.__entry]
        addr:0x000000000001ab prog.kernel_launch(%1433:tensor<[1, 192, 1536], Float32, CPU>) -> (%1434:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.6.input_layernorm, op_type:RMSNorm, op_options:{"add_unit_offset":false,"epsilon":9.999999974752427e-07}]
        addr:0x000000000001ac prog.jump model.layers.6.self_attn.__entry[offset:10]
        addr:0x000000000001ad prog.kernel_launch(%1452:tensor<[1, 192, 1536], Float32, CPU>, %1433:tensor<[1, 192, 1536], Float32, CPU>) -> (%1453:tensor<[1, 192, 1536], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x000000000001ae prog.free(%1433:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x000000000001af prog.free(%1452:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x000000000001b0 prog.kernel_launch(%1453:tensor<[1, 192, 1536], Float32, CPU>) -> (%1454:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.6.post_attention_layernorm, op_type:RMSNorm, op_options:{"add_unit_offset":false,"epsilon":9.999999974752427e-07}]
        addr:0x000000000001b1 prog.jump model.layers.6.mlp.__entry[offset:45]
        addr:0x000000000001b2 prog.kernel_launch(%1459:tensor<[1, 192, 1536], Float32, CPU>, %1453:tensor<[1, 192, 1536], Float32, CPU>) -> (%1460:tensor<[1, 192, 1536], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x000000000001b3 prog.free(%1453:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x000000000001b4 prog.free(%1459:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x000000000001b5 prog.ret
        addr:0x000000000001b6 prog.label[symbol:model.layers.6.self_attn.__entry]
        addr:0x000000000001b7 prog.kernel_launch(%1434:tensor<[1, 192, 1536], Float32, CPU>) -> (%1435:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.6.self_attn.q_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x000000000001b8 prog.kernel_launch(%1434:tensor<[1, 192, 1536], Float32, CPU>) -> (%1436:tensor<[1, 192, 256], Float32, CPU>)[symbol_name:model.layers.6.self_attn.k_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x000000000001b9 prog.kernel_launch(%1434:tensor<[1, 192, 1536], Float32, CPU>) -> (%1437:tensor<[1, 192, 256], Float32, CPU>)[symbol_name:model.layers.6.self_attn.v_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x000000000001ba prog.free(%1434:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x000000000001bb prog.kernel_launch(%1435:tensor<[1, 192, 1536], Float32, CPU>) -> (%1435:tensor<[1, 192, 12, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,12,128]}]
        addr:0x000000000001bc prog.kernel_launch(%1436:tensor<[1, 192, 256], Float32, CPU>) -> (%1436:tensor<[1, 192, 2, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,2,128]}]
        addr:0x000000000001bd prog.kernel_launch(%1437:tensor<[1, 192, 256], Float32, CPU>) -> (%1437:tensor<[1, 192, 2, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,2,128]}]
        addr:0x000000000001be prog.kernel_launch(%1435:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%1438:tensor<[1, 12, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x000000000001bf prog.free(%1435:tensor<[1, 192, 12, 128], Float32, CPU>) -> ()
        addr:0x000000000001c0 prog.kernel_launch(%1436:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%1439:tensor<[1, 2, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x000000000001c1 prog.free(%1436:tensor<[1, 192, 2, 128], Float32, CPU>) -> ()
        addr:0x000000000001c2 prog.kernel_launch(%1437:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%1440:tensor<[1, 2, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x000000000001c3 prog.free(%1437:tensor<[1, 192, 2, 128], Float32, CPU>) -> ()
        addr:0x000000000001c4 prog.kernel_launch(%1438:tensor<[1, 12, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1441:tensor<[1, 12, 192, 128], Float32, CPU>)[symbol_name:model.layers.6.self_attn.q_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x000000000001c5 prog.free(%1438:tensor<[1, 12, 192, 128], Float32, CPU>) -> ()
        addr:0x000000000001c6 prog.kernel_launch(%1439:tensor<[1, 2, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1442:tensor<[1, 2, 192, 128], Float32, CPU>)[symbol_name:model.layers.6.self_attn.k_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x000000000001c7 prog.free(%1439:tensor<[1, 2, 192, 128], Float32, CPU>) -> ()
        addr:0x000000000001c8 prog.kernel_launch(%1442:tensor<[1, 2, 192, 128], Float32, CPU>, %1440:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1443:tensor<[1, 2, 192, 128], Float32, CPU>, %1444:tensor<[1, 2, 192, 128], Float32, CPU>)[symbol_name:model.layers.6.self_attn.kv_cache, op_type:KVCache, op_options:{"head_dim":128,"kv_head":2,"layer_idx":0,"q_head":12,"use_fa2":false}]
        addr:0x000000000001c9 prog.free(%1440:tensor<[1, 2, 192, 128], Float32, CPU>) -> ()
        addr:0x000000000001ca prog.free(%1442:tensor<[1, 2, 192, 128], Float32, CPU>) -> ()
        addr:0x000000000001cb prog.kernel_launch(%1441:tensor<[1, 12, 192, 128], Float32, CPU>, %1443:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1445:tensor<[1, 12, 192, 192], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":true}]
        addr:0x000000000001cc prog.free(%1443:tensor<[1, 2, 192, 128], Float32, CPU>) -> ()
        addr:0x000000000001cd prog.free(%1441:tensor<[1, 12, 192, 128], Float32, CPU>) -> ()
        addr:0x000000000001ce prog.kernel_launch(%1445:tensor<[1, 12, 192, 192], Float32, CPU>, %1446:tensor<[1], Float32, CPU>) -> (%1447:tensor<[1, 12, 192, 192], Float32, CPU>)[op_type:Mul, op_options:null]
        addr:0x000000000001cf prog.free(%1446:tensor<[1], Float32, CPU>) -> ()
        addr:0x000000000001d0 prog.free(%1445:tensor<[1, 12, 192, 192], Float32, CPU>) -> ()
        addr:0x000000000001d1 prog.kernel_launch(%1447:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%1448:tensor<[1, 12, 192, 192], Float32, CPU>)[symbol_name:model.layers.6.self_attn.mask, op_type:CausalMask, op_options:{"sliding_window":false,"window_size":0}]
        addr:0x000000000001d2 prog.free(%1447:tensor<[1, 12, 192, 192], Float32, CPU>) -> ()
        addr:0x000000000001d3 prog.kernel_launch(%1448:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%1449:tensor<[1, 12, 192, 192], Float32, CPU>)[symbol_name:model.layers.6.self_attn.softmax, op_type:Softmax, op_options:{"axis":-1}]
        addr:0x000000000001d4 prog.free(%1448:tensor<[1, 12, 192, 192], Float32, CPU>) -> ()
        addr:0x000000000001d5 prog.kernel_launch(%1449:tensor<[1, 12, 192, 192], Float32, CPU>, %1444:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1450:tensor<[1, 12, 192, 128], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":false}]
        addr:0x000000000001d6 prog.free(%1444:tensor<[1, 2, 192, 128], Float32, CPU>) -> ()
        addr:0x000000000001d7 prog.free(%1449:tensor<[1, 12, 192, 192], Float32, CPU>) -> ()
        addr:0x000000000001d8 prog.kernel_launch(%1450:tensor<[1, 12, 192, 128], Float32, CPU>) -> (%1451:tensor<[1, 192, 12, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x000000000001d9 prog.free(%1450:tensor<[1, 12, 192, 128], Float32, CPU>) -> ()
        addr:0x000000000001da prog.kernel_launch(%1451:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%1451:tensor<[1, 192, 1536], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,1536]}]
        addr:0x000000000001db prog.kernel_launch(%1451:tensor<[1, 192, 1536], Float32, CPU>) -> (%1452:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.6.self_attn.o_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x000000000001dc prog.free(%1451:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x000000000001dd prog.ret
        addr:0x000000000001de prog.label[symbol:model.layers.6.mlp.__entry]
        addr:0x000000000001df prog.kernel_launch(%1454:tensor<[1, 192, 1536], Float32, CPU>) -> (%1455:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.6.mlp.gate_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x000000000001e0 prog.kernel_launch(%1455:tensor<[1, 192, 8960], Float32, CPU>) -> (%1456:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.6.mlp.act, op_type:SiLU, op_options:null]
        addr:0x000000000001e1 prog.free(%1455:tensor<[1, 192, 8960], Float32, CPU>) -> ()
        addr:0x000000000001e2 prog.kernel_launch(%1454:tensor<[1, 192, 1536], Float32, CPU>) -> (%1457:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.6.mlp.up_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x000000000001e3 prog.free(%1454:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x000000000001e4 prog.kernel_launch(%1456:tensor<[1, 192, 8960], Float32, CPU>, %1457:tensor<[1, 192, 8960], Float32, CPU>) -> (%1458:tensor<[1, 192, 8960], Float32, CPU>)[op_type:Mul, op_options:null]
        addr:0x000000000001e5 prog.free(%1457:tensor<[1, 192, 8960], Float32, CPU>) -> ()
        addr:0x000000000001e6 prog.free(%1456:tensor<[1, 192, 8960], Float32, CPU>) -> ()
        addr:0x000000000001e7 prog.kernel_launch(%1458:tensor<[1, 192, 8960], Float32, CPU>) -> (%1459:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.6.mlp.down_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":8960,"out_channels":1536}]
        addr:0x000000000001e8 prog.free(%1458:tensor<[1, 192, 8960], Float32, CPU>) -> ()
        addr:0x000000000001e9 prog.ret
        addr:0x000000000001ea prog.label[symbol:model.layers.7.__entry]
        addr:0x000000000001eb prog.kernel_launch(%1460:tensor<[1, 192, 1536], Float32, CPU>) -> (%1461:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.7.input_layernorm, op_type:RMSNorm, op_options:{"add_unit_offset":false,"epsilon":9.999999974752427e-07}]
        addr:0x000000000001ec prog.jump model.layers.7.self_attn.__entry[offset:10]
        addr:0x000000000001ed prog.kernel_launch(%1479:tensor<[1, 192, 1536], Float32, CPU>, %1460:tensor<[1, 192, 1536], Float32, CPU>) -> (%1480:tensor<[1, 192, 1536], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x000000000001ee prog.free(%1460:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x000000000001ef prog.free(%1479:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x000000000001f0 prog.kernel_launch(%1480:tensor<[1, 192, 1536], Float32, CPU>) -> (%1481:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.7.post_attention_layernorm, op_type:RMSNorm, op_options:{"add_unit_offset":false,"epsilon":9.999999974752427e-07}]
        addr:0x000000000001f1 prog.jump model.layers.7.mlp.__entry[offset:45]
        addr:0x000000000001f2 prog.kernel_launch(%1486:tensor<[1, 192, 1536], Float32, CPU>, %1480:tensor<[1, 192, 1536], Float32, CPU>) -> (%1487:tensor<[1, 192, 1536], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x000000000001f3 prog.free(%1480:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x000000000001f4 prog.free(%1486:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x000000000001f5 prog.ret
        addr:0x000000000001f6 prog.label[symbol:model.layers.7.self_attn.__entry]
        addr:0x000000000001f7 prog.kernel_launch(%1461:tensor<[1, 192, 1536], Float32, CPU>) -> (%1462:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.7.self_attn.q_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x000000000001f8 prog.kernel_launch(%1461:tensor<[1, 192, 1536], Float32, CPU>) -> (%1463:tensor<[1, 192, 256], Float32, CPU>)[symbol_name:model.layers.7.self_attn.k_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x000000000001f9 prog.kernel_launch(%1461:tensor<[1, 192, 1536], Float32, CPU>) -> (%1464:tensor<[1, 192, 256], Float32, CPU>)[symbol_name:model.layers.7.self_attn.v_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x000000000001fa prog.free(%1461:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x000000000001fb prog.kernel_launch(%1462:tensor<[1, 192, 1536], Float32, CPU>) -> (%1462:tensor<[1, 192, 12, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,12,128]}]
        addr:0x000000000001fc prog.kernel_launch(%1463:tensor<[1, 192, 256], Float32, CPU>) -> (%1463:tensor<[1, 192, 2, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,2,128]}]
        addr:0x000000000001fd prog.kernel_launch(%1464:tensor<[1, 192, 256], Float32, CPU>) -> (%1464:tensor<[1, 192, 2, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,2,128]}]
        addr:0x000000000001fe prog.kernel_launch(%1462:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%1465:tensor<[1, 12, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x000000000001ff prog.free(%1462:tensor<[1, 192, 12, 128], Float32, CPU>) -> ()
        addr:0x00000000000200 prog.kernel_launch(%1463:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%1466:tensor<[1, 2, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x00000000000201 prog.free(%1463:tensor<[1, 192, 2, 128], Float32, CPU>) -> ()
        addr:0x00000000000202 prog.kernel_launch(%1464:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%1467:tensor<[1, 2, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x00000000000203 prog.free(%1464:tensor<[1, 192, 2, 128], Float32, CPU>) -> ()
        addr:0x00000000000204 prog.kernel_launch(%1465:tensor<[1, 12, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1468:tensor<[1, 12, 192, 128], Float32, CPU>)[symbol_name:model.layers.7.self_attn.q_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x00000000000205 prog.free(%1465:tensor<[1, 12, 192, 128], Float32, CPU>) -> ()
        addr:0x00000000000206 prog.kernel_launch(%1466:tensor<[1, 2, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1469:tensor<[1, 2, 192, 128], Float32, CPU>)[symbol_name:model.layers.7.self_attn.k_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x00000000000207 prog.free(%1466:tensor<[1, 2, 192, 128], Float32, CPU>) -> ()
        addr:0x00000000000208 prog.kernel_launch(%1469:tensor<[1, 2, 192, 128], Float32, CPU>, %1467:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1470:tensor<[1, 2, 192, 128], Float32, CPU>, %1471:tensor<[1, 2, 192, 128], Float32, CPU>)[symbol_name:model.layers.7.self_attn.kv_cache, op_type:KVCache, op_options:{"head_dim":128,"kv_head":2,"layer_idx":0,"q_head":12,"use_fa2":false}]
        addr:0x00000000000209 prog.free(%1467:tensor<[1, 2, 192, 128], Float32, CPU>) -> ()
        addr:0x0000000000020a prog.free(%1469:tensor<[1, 2, 192, 128], Float32, CPU>) -> ()
        addr:0x0000000000020b prog.kernel_launch(%1468:tensor<[1, 12, 192, 128], Float32, CPU>, %1470:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1472:tensor<[1, 12, 192, 192], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":true}]
        addr:0x0000000000020c prog.free(%1470:tensor<[1, 2, 192, 128], Float32, CPU>) -> ()
        addr:0x0000000000020d prog.free(%1468:tensor<[1, 12, 192, 128], Float32, CPU>) -> ()
        addr:0x0000000000020e prog.kernel_launch(%1472:tensor<[1, 12, 192, 192], Float32, CPU>, %1473:tensor<[1], Float32, CPU>) -> (%1474:tensor<[1, 12, 192, 192], Float32, CPU>)[op_type:Mul, op_options:null]
        addr:0x0000000000020f prog.free(%1473:tensor<[1], Float32, CPU>) -> ()
        addr:0x00000000000210 prog.free(%1472:tensor<[1, 12, 192, 192], Float32, CPU>) -> ()
        addr:0x00000000000211 prog.kernel_launch(%1474:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%1475:tensor<[1, 12, 192, 192], Float32, CPU>)[symbol_name:model.layers.7.self_attn.mask, op_type:CausalMask, op_options:{"sliding_window":false,"window_size":0}]
        addr:0x00000000000212 prog.free(%1474:tensor<[1, 12, 192, 192], Float32, CPU>) -> ()
        addr:0x00000000000213 prog.kernel_launch(%1475:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%1476:tensor<[1, 12, 192, 192], Float32, CPU>)[symbol_name:model.layers.7.self_attn.softmax, op_type:Softmax, op_options:{"axis":-1}]
        addr:0x00000000000214 prog.free(%1475:tensor<[1, 12, 192, 192], Float32, CPU>) -> ()
        addr:0x00000000000215 prog.kernel_launch(%1476:tensor<[1, 12, 192, 192], Float32, CPU>, %1471:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1477:tensor<[1, 12, 192, 128], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":false}]
        addr:0x00000000000216 prog.free(%1471:tensor<[1, 2, 192, 128], Float32, CPU>) -> ()
        addr:0x00000000000217 prog.free(%1476:tensor<[1, 12, 192, 192], Float32, CPU>) -> ()
        addr:0x00000000000218 prog.kernel_launch(%1477:tensor<[1, 12, 192, 128], Float32, CPU>) -> (%1478:tensor<[1, 192, 12, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x00000000000219 prog.free(%1477:tensor<[1, 12, 192, 128], Float32, CPU>) -> ()
        addr:0x0000000000021a prog.kernel_launch(%1478:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%1478:tensor<[1, 192, 1536], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,1536]}]
        addr:0x0000000000021b prog.kernel_launch(%1478:tensor<[1, 192, 1536], Float32, CPU>) -> (%1479:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.7.self_attn.o_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x0000000000021c prog.free(%1478:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x0000000000021d prog.ret
        addr:0x0000000000021e prog.label[symbol:model.layers.7.mlp.__entry]
        addr:0x0000000000021f prog.kernel_launch(%1481:tensor<[1, 192, 1536], Float32, CPU>) -> (%1482:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.7.mlp.gate_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x00000000000220 prog.kernel_launch(%1482:tensor<[1, 192, 8960], Float32, CPU>) -> (%1483:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.7.mlp.act, op_type:SiLU, op_options:null]
        addr:0x00000000000221 prog.free(%1482:tensor<[1, 192, 8960], Float32, CPU>) -> ()
        addr:0x00000000000222 prog.kernel_launch(%1481:tensor<[1, 192, 1536], Float32, CPU>) -> (%1484:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.7.mlp.up_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x00000000000223 prog.free(%1481:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x00000000000224 prog.kernel_launch(%1483:tensor<[1, 192, 8960], Float32, CPU>, %1484:tensor<[1, 192, 8960], Float32, CPU>) -> (%1485:tensor<[1, 192, 8960], Float32, CPU>)[op_type:Mul, op_options:null]
        addr:0x00000000000225 prog.free(%1484:tensor<[1, 192, 8960], Float32, CPU>) -> ()
        addr:0x00000000000226 prog.free(%1483:tensor<[1, 192, 8960], Float32, CPU>) -> ()
        addr:0x00000000000227 prog.kernel_launch(%1485:tensor<[1, 192, 8960], Float32, CPU>) -> (%1486:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.7.mlp.down_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":8960,"out_channels":1536}]
        addr:0x00000000000228 prog.free(%1485:tensor<[1, 192, 8960], Float32, CPU>) -> ()
        addr:0x00000000000229 prog.ret
        addr:0x0000000000022a prog.label[symbol:model.layers.8.__entry]
        addr:0x0000000000022b prog.kernel_launch(%1487:tensor<[1, 192, 1536], Float32, CPU>) -> (%1488:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.8.input_layernorm, op_type:RMSNorm, op_options:{"add_unit_offset":false,"epsilon":9.999999974752427e-07}]
        addr:0x0000000000022c prog.jump model.layers.8.self_attn.__entry[offset:10]
        addr:0x0000000000022d prog.kernel_launch(%1506:tensor<[1, 192, 1536], Float32, CPU>, %1487:tensor<[1, 192, 1536], Float32, CPU>) -> (%1507:tensor<[1, 192, 1536], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x0000000000022e prog.free(%1487:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x0000000000022f prog.free(%1506:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x00000000000230 prog.kernel_launch(%1507:tensor<[1, 192, 1536], Float32, CPU>) -> (%1508:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.8.post_attention_layernorm, op_type:RMSNorm, op_options:{"add_unit_offset":false,"epsilon":9.999999974752427e-07}]
        addr:0x00000000000231 prog.jump model.layers.8.mlp.__entry[offset:45]
        addr:0x00000000000232 prog.kernel_launch(%1513:tensor<[1, 192, 1536], Float32, CPU>, %1507:tensor<[1, 192, 1536], Float32, CPU>) -> (%1514:tensor<[1, 192, 1536], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x00000000000233 prog.free(%1507:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x00000000000234 prog.free(%1513:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x00000000000235 prog.ret
        addr:0x00000000000236 prog.label[symbol:model.layers.8.self_attn.__entry]
        addr:0x00000000000237 prog.kernel_launch(%1488:tensor<[1, 192, 1536], Float32, CPU>) -> (%1489:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.8.self_attn.q_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x00000000000238 prog.kernel_launch(%1488:tensor<[1, 192, 1536], Float32, CPU>) -> (%1490:tensor<[1, 192, 256], Float32, CPU>)[symbol_name:model.layers.8.self_attn.k_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x00000000000239 prog.kernel_launch(%1488:tensor<[1, 192, 1536], Float32, CPU>) -> (%1491:tensor<[1, 192, 256], Float32, CPU>)[symbol_name:model.layers.8.self_attn.v_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x0000000000023a prog.free(%1488:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x0000000000023b prog.kernel_launch(%1489:tensor<[1, 192, 1536], Float32, CPU>) -> (%1489:tensor<[1, 192, 12, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,12,128]}]
        addr:0x0000000000023c prog.kernel_launch(%1490:tensor<[1, 192, 256], Float32, CPU>) -> (%1490:tensor<[1, 192, 2, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,2,128]}]
        addr:0x0000000000023d prog.kernel_launch(%1491:tensor<[1, 192, 256], Float32, CPU>) -> (%1491:tensor<[1, 192, 2, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,2,128]}]
        addr:0x0000000000023e prog.kernel_launch(%1489:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%1492:tensor<[1, 12, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x0000000000023f prog.free(%1489:tensor<[1, 192, 12, 128], Float32, CPU>) -> ()
        addr:0x00000000000240 prog.kernel_launch(%1490:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%1493:tensor<[1, 2, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x00000000000241 prog.free(%1490:tensor<[1, 192, 2, 128], Float32, CPU>) -> ()
        addr:0x00000000000242 prog.kernel_launch(%1491:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%1494:tensor<[1, 2, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x00000000000243 prog.free(%1491:tensor<[1, 192, 2, 128], Float32, CPU>) -> ()
        addr:0x00000000000244 prog.kernel_launch(%1492:tensor<[1, 12, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1495:tensor<[1, 12, 192, 128], Float32, CPU>)[symbol_name:model.layers.8.self_attn.q_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x00000000000245 prog.free(%1492:tensor<[1, 12, 192, 128], Float32, CPU>) -> ()
        addr:0x00000000000246 prog.kernel_launch(%1493:tensor<[1, 2, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1496:tensor<[1, 2, 192, 128], Float32, CPU>)[symbol_name:model.layers.8.self_attn.k_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x00000000000247 prog.free(%1493:tensor<[1, 2, 192, 128], Float32, CPU>) -> ()
        addr:0x00000000000248 prog.kernel_launch(%1496:tensor<[1, 2, 192, 128], Float32, CPU>, %1494:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1497:tensor<[1, 2, 192, 128], Float32, CPU>, %1498:tensor<[1, 2, 192, 128], Float32, CPU>)[symbol_name:model.layers.8.self_attn.kv_cache, op_type:KVCache, op_options:{"head_dim":128,"kv_head":2,"layer_idx":0,"q_head":12,"use_fa2":false}]
        addr:0x00000000000249 prog.free(%1494:tensor<[1, 2, 192, 128], Float32, CPU>) -> ()
        addr:0x0000000000024a prog.free(%1496:tensor<[1, 2, 192, 128], Float32, CPU>) -> ()
        addr:0x0000000000024b prog.kernel_launch(%1495:tensor<[1, 12, 192, 128], Float32, CPU>, %1497:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1499:tensor<[1, 12, 192, 192], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":true}]
        addr:0x0000000000024c prog.free(%1497:tensor<[1, 2, 192, 128], Float32, CPU>) -> ()
        addr:0x0000000000024d prog.free(%1495:tensor<[1, 12, 192, 128], Float32, CPU>) -> ()
        addr:0x0000000000024e prog.kernel_launch(%1499:tensor<[1, 12, 192, 192], Float32, CPU>, %1500:tensor<[1], Float32, CPU>) -> (%1501:tensor<[1, 12, 192, 192], Float32, CPU>)[op_type:Mul, op_options:null]
        addr:0x0000000000024f prog.free(%1500:tensor<[1], Float32, CPU>) -> ()
        addr:0x00000000000250 prog.free(%1499:tensor<[1, 12, 192, 192], Float32, CPU>) -> ()
        addr:0x00000000000251 prog.kernel_launch(%1501:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%1502:tensor<[1, 12, 192, 192], Float32, CPU>)[symbol_name:model.layers.8.self_attn.mask, op_type:CausalMask, op_options:{"sliding_window":false,"window_size":0}]
        addr:0x00000000000252 prog.free(%1501:tensor<[1, 12, 192, 192], Float32, CPU>) -> ()
        addr:0x00000000000253 prog.kernel_launch(%1502:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%1503:tensor<[1, 12, 192, 192], Float32, CPU>)[symbol_name:model.layers.8.self_attn.softmax, op_type:Softmax, op_options:{"axis":-1}]
        addr:0x00000000000254 prog.free(%1502:tensor<[1, 12, 192, 192], Float32, CPU>) -> ()
        addr:0x00000000000255 prog.kernel_launch(%1503:tensor<[1, 12, 192, 192], Float32, CPU>, %1498:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1504:tensor<[1, 12, 192, 128], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":false}]
        addr:0x00000000000256 prog.free(%1498:tensor<[1, 2, 192, 128], Float32, CPU>) -> ()
        addr:0x00000000000257 prog.free(%1503:tensor<[1, 12, 192, 192], Float32, CPU>) -> ()
        addr:0x00000000000258 prog.kernel_launch(%1504:tensor<[1, 12, 192, 128], Float32, CPU>) -> (%1505:tensor<[1, 192, 12, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x00000000000259 prog.free(%1504:tensor<[1, 12, 192, 128], Float32, CPU>) -> ()
        addr:0x0000000000025a prog.kernel_launch(%1505:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%1505:tensor<[1, 192, 1536], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,1536]}]
        addr:0x0000000000025b prog.kernel_launch(%1505:tensor<[1, 192, 1536], Float32, CPU>) -> (%1506:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.8.self_attn.o_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x0000000000025c prog.free(%1505:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x0000000000025d prog.ret
        addr:0x0000000000025e prog.label[symbol:model.layers.8.mlp.__entry]
        addr:0x0000000000025f prog.kernel_launch(%1508:tensor<[1, 192, 1536], Float32, CPU>) -> (%1509:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.8.mlp.gate_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x00000000000260 prog.kernel_launch(%1509:tensor<[1, 192, 8960], Float32, CPU>) -> (%1510:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.8.mlp.act, op_type:SiLU, op_options:null]
        addr:0x00000000000261 prog.free(%1509:tensor<[1, 192, 8960], Float32, CPU>) -> ()
        addr:0x00000000000262 prog.kernel_launch(%1508:tensor<[1, 192, 1536], Float32, CPU>) -> (%1511:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.8.mlp.up_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x00000000000263 prog.free(%1508:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x00000000000264 prog.kernel_launch(%1510:tensor<[1, 192, 8960], Float32, CPU>, %1511:tensor<[1, 192, 8960], Float32, CPU>) -> (%1512:tensor<[1, 192, 8960], Float32, CPU>)[op_type:Mul, op_options:null]
        addr:0x00000000000265 prog.free(%1511:tensor<[1, 192, 8960], Float32, CPU>) -> ()
        addr:0x00000000000266 prog.free(%1510:tensor<[1, 192, 8960], Float32, CPU>) -> ()
        addr:0x00000000000267 prog.kernel_launch(%1512:tensor<[1, 192, 8960], Float32, CPU>) -> (%1513:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.8.mlp.down_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":8960,"out_channels":1536}]
        addr:0x00000000000268 prog.free(%1512:tensor<[1, 192, 8960], Float32, CPU>) -> ()
        addr:0x00000000000269 prog.ret
        addr:0x0000000000026a prog.label[symbol:model.layers.9.__entry]
        addr:0x0000000000026b prog.kernel_launch(%1514:tensor<[1, 192, 1536], Float32, CPU>) -> (%1515:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.9.input_layernorm, op_type:RMSNorm, op_options:{"add_unit_offset":false,"epsilon":9.999999974752427e-07}]
        addr:0x0000000000026c prog.jump model.layers.9.self_attn.__entry[offset:10]
        addr:0x0000000000026d prog.kernel_launch(%1533:tensor<[1, 192, 1536], Float32, CPU>, %1514:tensor<[1, 192, 1536], Float32, CPU>) -> (%1534:tensor<[1, 192, 1536], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x0000000000026e prog.free(%1514:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x0000000000026f prog.free(%1533:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x00000000000270 prog.kernel_launch(%1534:tensor<[1, 192, 1536], Float32, CPU>) -> (%1535:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.9.post_attention_layernorm, op_type:RMSNorm, op_options:{"add_unit_offset":false,"epsilon":9.999999974752427e-07}]
        addr:0x00000000000271 prog.jump model.layers.9.mlp.__entry[offset:45]
        addr:0x00000000000272 prog.kernel_launch(%1540:tensor<[1, 192, 1536], Float32, CPU>, %1534:tensor<[1, 192, 1536], Float32, CPU>) -> (%1541:tensor<[1, 192, 1536], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x00000000000273 prog.free(%1534:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x00000000000274 prog.free(%1540:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x00000000000275 prog.ret
        addr:0x00000000000276 prog.label[symbol:model.layers.9.self_attn.__entry]
        addr:0x00000000000277 prog.kernel_launch(%1515:tensor<[1, 192, 1536], Float32, CPU>) -> (%1516:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.9.self_attn.q_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x00000000000278 prog.kernel_launch(%1515:tensor<[1, 192, 1536], Float32, CPU>) -> (%1517:tensor<[1, 192, 256], Float32, CPU>)[symbol_name:model.layers.9.self_attn.k_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x00000000000279 prog.kernel_launch(%1515:tensor<[1, 192, 1536], Float32, CPU>) -> (%1518:tensor<[1, 192, 256], Float32, CPU>)[symbol_name:model.layers.9.self_attn.v_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x0000000000027a prog.free(%1515:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x0000000000027b prog.kernel_launch(%1516:tensor<[1, 192, 1536], Float32, CPU>) -> (%1516:tensor<[1, 192, 12, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,12,128]}]
        addr:0x0000000000027c prog.kernel_launch(%1517:tensor<[1, 192, 256], Float32, CPU>) -> (%1517:tensor<[1, 192, 2, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,2,128]}]
        addr:0x0000000000027d prog.kernel_launch(%1518:tensor<[1, 192, 256], Float32, CPU>) -> (%1518:tensor<[1, 192, 2, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,2,128]}]
        addr:0x0000000000027e prog.kernel_launch(%1516:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%1519:tensor<[1, 12, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x0000000000027f prog.free(%1516:tensor<[1, 192, 12, 128], Float32, CPU>) -> ()
        addr:0x00000000000280 prog.kernel_launch(%1517:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%1520:tensor<[1, 2, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x00000000000281 prog.free(%1517:tensor<[1, 192, 2, 128], Float32, CPU>) -> ()
        addr:0x00000000000282 prog.kernel_launch(%1518:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%1521:tensor<[1, 2, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x00000000000283 prog.free(%1518:tensor<[1, 192, 2, 128], Float32, CPU>) -> ()
        addr:0x00000000000284 prog.kernel_launch(%1519:tensor<[1, 12, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1522:tensor<[1, 12, 192, 128], Float32, CPU>)[symbol_name:model.layers.9.self_attn.q_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x00000000000285 prog.free(%1519:tensor<[1, 12, 192, 128], Float32, CPU>) -> ()
        addr:0x00000000000286 prog.kernel_launch(%1520:tensor<[1, 2, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1523:tensor<[1, 2, 192, 128], Float32, CPU>)[symbol_name:model.layers.9.self_attn.k_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x00000000000287 prog.free(%1520:tensor<[1, 2, 192, 128], Float32, CPU>) -> ()
        addr:0x00000000000288 prog.kernel_launch(%1523:tensor<[1, 2, 192, 128], Float32, CPU>, %1521:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1524:tensor<[1, 2, 192, 128], Float32, CPU>, %1525:tensor<[1, 2, 192, 128], Float32, CPU>)[symbol_name:model.layers.9.self_attn.kv_cache, op_type:KVCache, op_options:{"head_dim":128,"kv_head":2,"layer_idx":0,"q_head":12,"use_fa2":false}]
        addr:0x00000000000289 prog.free(%1521:tensor<[1, 2, 192, 128], Float32, CPU>) -> ()
        addr:0x0000000000028a prog.free(%1523:tensor<[1, 2, 192, 128], Float32, CPU>) -> ()
        addr:0x0000000000028b prog.kernel_launch(%1522:tensor<[1, 12, 192, 128], Float32, CPU>, %1524:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1526:tensor<[1, 12, 192, 192], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":true}]
        addr:0x0000000000028c prog.free(%1524:tensor<[1, 2, 192, 128], Float32, CPU>) -> ()
        addr:0x0000000000028d prog.free(%1522:tensor<[1, 12, 192, 128], Float32, CPU>) -> ()
        addr:0x0000000000028e prog.kernel_launch(%1526:tensor<[1, 12, 192, 192], Float32, CPU>, %1527:tensor<[1], Float32, CPU>) -> (%1528:tensor<[1, 12, 192, 192], Float32, CPU>)[op_type:Mul, op_options:null]
        addr:0x0000000000028f prog.free(%1527:tensor<[1], Float32, CPU>) -> ()
        addr:0x00000000000290 prog.free(%1526:tensor<[1, 12, 192, 192], Float32, CPU>) -> ()
        addr:0x00000000000291 prog.kernel_launch(%1528:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%1529:tensor<[1, 12, 192, 192], Float32, CPU>)[symbol_name:model.layers.9.self_attn.mask, op_type:CausalMask, op_options:{"sliding_window":false,"window_size":0}]
        addr:0x00000000000292 prog.free(%1528:tensor<[1, 12, 192, 192], Float32, CPU>) -> ()
        addr:0x00000000000293 prog.kernel_launch(%1529:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%1530:tensor<[1, 12, 192, 192], Float32, CPU>)[symbol_name:model.layers.9.self_attn.softmax, op_type:Softmax, op_options:{"axis":-1}]
        addr:0x00000000000294 prog.free(%1529:tensor<[1, 12, 192, 192], Float32, CPU>) -> ()
        addr:0x00000000000295 prog.kernel_launch(%1530:tensor<[1, 12, 192, 192], Float32, CPU>, %1525:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1531:tensor<[1, 12, 192, 128], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":false}]
        addr:0x00000000000296 prog.free(%1525:tensor<[1, 2, 192, 128], Float32, CPU>) -> ()
        addr:0x00000000000297 prog.free(%1530:tensor<[1, 12, 192, 192], Float32, CPU>) -> ()
        addr:0x00000000000298 prog.kernel_launch(%1531:tensor<[1, 12, 192, 128], Float32, CPU>) -> (%1532:tensor<[1, 192, 12, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x00000000000299 prog.free(%1531:tensor<[1, 12, 192, 128], Float32, CPU>) -> ()
        addr:0x0000000000029a prog.kernel_launch(%1532:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%1532:tensor<[1, 192, 1536], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,1536]}]
        addr:0x0000000000029b prog.kernel_launch(%1532:tensor<[1, 192, 1536], Float32, CPU>) -> (%1533:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.9.self_attn.o_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x0000000000029c prog.free(%1532:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x0000000000029d prog.ret
        addr:0x0000000000029e prog.label[symbol:model.layers.9.mlp.__entry]
        addr:0x0000000000029f prog.kernel_launch(%1535:tensor<[1, 192, 1536], Float32, CPU>) -> (%1536:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.9.mlp.gate_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x000000000002a0 prog.kernel_launch(%1536:tensor<[1, 192, 8960], Float32, CPU>) -> (%1537:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.9.mlp.act, op_type:SiLU, op_options:null]
        addr:0x000000000002a1 prog.free(%1536:tensor<[1, 192, 8960], Float32, CPU>) -> ()
        addr:0x000000000002a2 prog.kernel_launch(%1535:tensor<[1, 192, 1536], Float32, CPU>) -> (%1538:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.9.mlp.up_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x000000000002a3 prog.free(%1535:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x000000000002a4 prog.kernel_launch(%1537:tensor<[1, 192, 8960], Float32, CPU>, %1538:tensor<[1, 192, 8960], Float32, CPU>) -> (%1539:tensor<[1, 192, 8960], Float32, CPU>)[op_type:Mul, op_options:null]
        addr:0x000000000002a5 prog.free(%1538:tensor<[1, 192, 8960], Float32, CPU>) -> ()
        addr:0x000000000002a6 prog.free(%1537:tensor<[1, 192, 8960], Float32, CPU>) -> ()
        addr:0x000000000002a7 prog.kernel_launch(%1539:tensor<[1, 192, 8960], Float32, CPU>) -> (%1540:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.9.mlp.down_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":8960,"out_channels":1536}]
        addr:0x000000000002a8 prog.free(%1539:tensor<[1, 192, 8960], Float32, CPU>) -> ()
        addr:0x000000000002a9 prog.ret
        addr:0x000000000002aa prog.label[symbol:model.layers.10.__entry]
        addr:0x000000000002ab prog.kernel_launch(%1541:tensor<[1, 192, 1536], Float32, CPU>) -> (%1542:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.10.input_layernorm, op_type:RMSNorm, op_options:{"add_unit_offset":false,"epsilon":9.999999974752427e-07}]
        addr:0x000000000002ac prog.jump model.layers.10.self_attn.__entry[offset:10]
        addr:0x000000000002ad prog.kernel_launch(%1560:tensor<[1, 192, 1536], Float32, CPU>, %1541:tensor<[1, 192, 1536], Float32, CPU>) -> (%1561:tensor<[1, 192, 1536], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x000000000002ae prog.free(%1541:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x000000000002af prog.free(%1560:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x000000000002b0 prog.kernel_launch(%1561:tensor<[1, 192, 1536], Float32, CPU>) -> (%1562:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.10.post_attention_layernorm, op_type:RMSNorm, op_options:{"add_unit_offset":false,"epsilon":9.999999974752427e-07}]
        addr:0x000000000002b1 prog.jump model.layers.10.mlp.__entry[offset:45]
        addr:0x000000000002b2 prog.kernel_launch(%1567:tensor<[1, 192, 1536], Float32, CPU>, %1561:tensor<[1, 192, 1536], Float32, CPU>) -> (%1568:tensor<[1, 192, 1536], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x000000000002b3 prog.free(%1561:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x000000000002b4 prog.free(%1567:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x000000000002b5 prog.ret
        addr:0x000000000002b6 prog.label[symbol:model.layers.10.self_attn.__entry]
        addr:0x000000000002b7 prog.kernel_launch(%1542:tensor<[1, 192, 1536], Float32, CPU>) -> (%1543:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.10.self_attn.q_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x000000000002b8 prog.kernel_launch(%1542:tensor<[1, 192, 1536], Float32, CPU>) -> (%1544:tensor<[1, 192, 256], Float32, CPU>)[symbol_name:model.layers.10.self_attn.k_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x000000000002b9 prog.kernel_launch(%1542:tensor<[1, 192, 1536], Float32, CPU>) -> (%1545:tensor<[1, 192, 256], Float32, CPU>)[symbol_name:model.layers.10.self_attn.v_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x000000000002ba prog.free(%1542:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x000000000002bb prog.kernel_launch(%1543:tensor<[1, 192, 1536], Float32, CPU>) -> (%1543:tensor<[1, 192, 12, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,12,128]}]
        addr:0x000000000002bc prog.kernel_launch(%1544:tensor<[1, 192, 256], Float32, CPU>) -> (%1544:tensor<[1, 192, 2, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,2,128]}]
        addr:0x000000000002bd prog.kernel_launch(%1545:tensor<[1, 192, 256], Float32, CPU>) -> (%1545:tensor<[1, 192, 2, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,2,128]}]
        addr:0x000000000002be prog.kernel_launch(%1543:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%1546:tensor<[1, 12, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x000000000002bf prog.free(%1543:tensor<[1, 192, 12, 128], Float32, CPU>) -> ()
        addr:0x000000000002c0 prog.kernel_launch(%1544:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%1547:tensor<[1, 2, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x000000000002c1 prog.free(%1544:tensor<[1, 192, 2, 128], Float32, CPU>) -> ()
        addr:0x000000000002c2 prog.kernel_launch(%1545:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%1548:tensor<[1, 2, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x000000000002c3 prog.free(%1545:tensor<[1, 192, 2, 128], Float32, CPU>) -> ()
        addr:0x000000000002c4 prog.kernel_launch(%1546:tensor<[1, 12, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1549:tensor<[1, 12, 192, 128], Float32, CPU>)[symbol_name:model.layers.10.self_attn.q_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x000000000002c5 prog.free(%1546:tensor<[1, 12, 192, 128], Float32, CPU>) -> ()
        addr:0x000000000002c6 prog.kernel_launch(%1547:tensor<[1, 2, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1550:tensor<[1, 2, 192, 128], Float32, CPU>)[symbol_name:model.layers.10.self_attn.k_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x000000000002c7 prog.free(%1547:tensor<[1, 2, 192, 128], Float32, CPU>) -> ()
        addr:0x000000000002c8 prog.kernel_launch(%1550:tensor<[1, 2, 192, 128], Float32, CPU>, %1548:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1551:tensor<[1, 2, 192, 128], Float32, CPU>, %1552:tensor<[1, 2, 192, 128], Float32, CPU>)[symbol_name:model.layers.10.self_attn.kv_cache, op_type:KVCache, op_options:{"head_dim":128,"kv_head":2,"layer_idx":0,"q_head":12,"use_fa2":false}]
        addr:0x000000000002c9 prog.free(%1548:tensor<[1, 2, 192, 128], Float32, CPU>) -> ()
        addr:0x000000000002ca prog.free(%1550:tensor<[1, 2, 192, 128], Float32, CPU>) -> ()
        addr:0x000000000002cb prog.kernel_launch(%1549:tensor<[1, 12, 192, 128], Float32, CPU>, %1551:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1553:tensor<[1, 12, 192, 192], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":true}]
        addr:0x000000000002cc prog.free(%1551:tensor<[1, 2, 192, 128], Float32, CPU>) -> ()
        addr:0x000000000002cd prog.free(%1549:tensor<[1, 12, 192, 128], Float32, CPU>) -> ()
        addr:0x000000000002ce prog.kernel_launch(%1553:tensor<[1, 12, 192, 192], Float32, CPU>, %1554:tensor<[1], Float32, CPU>) -> (%1555:tensor<[1, 12, 192, 192], Float32, CPU>)[op_type:Mul, op_options:null]
        addr:0x000000000002cf prog.free(%1554:tensor<[1], Float32, CPU>) -> ()
        addr:0x000000000002d0 prog.free(%1553:tensor<[1, 12, 192, 192], Float32, CPU>) -> ()
        addr:0x000000000002d1 prog.kernel_launch(%1555:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%1556:tensor<[1, 12, 192, 192], Float32, CPU>)[symbol_name:model.layers.10.self_attn.mask, op_type:CausalMask, op_options:{"sliding_window":false,"window_size":0}]
        addr:0x000000000002d2 prog.free(%1555:tensor<[1, 12, 192, 192], Float32, CPU>) -> ()
        addr:0x000000000002d3 prog.kernel_launch(%1556:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%1557:tensor<[1, 12, 192, 192], Float32, CPU>)[symbol_name:model.layers.10.self_attn.softmax, op_type:Softmax, op_options:{"axis":-1}]
        addr:0x000000000002d4 prog.free(%1556:tensor<[1, 12, 192, 192], Float32, CPU>) -> ()
        addr:0x000000000002d5 prog.kernel_launch(%1557:tensor<[1, 12, 192, 192], Float32, CPU>, %1552:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1558:tensor<[1, 12, 192, 128], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":false}]
        addr:0x000000000002d6 prog.free(%1552:tensor<[1, 2, 192, 128], Float32, CPU>) -> ()
        addr:0x000000000002d7 prog.free(%1557:tensor<[1, 12, 192, 192], Float32, CPU>) -> ()
        addr:0x000000000002d8 prog.kernel_launch(%1558:tensor<[1, 12, 192, 128], Float32, CPU>) -> (%1559:tensor<[1, 192, 12, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x000000000002d9 prog.free(%1558:tensor<[1, 12, 192, 128], Float32, CPU>) -> ()
        addr:0x000000000002da prog.kernel_launch(%1559:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%1559:tensor<[1, 192, 1536], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,1536]}]
        addr:0x000000000002db prog.kernel_launch(%1559:tensor<[1, 192, 1536], Float32, CPU>) -> (%1560:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.10.self_attn.o_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x000000000002dc prog.free(%1559:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x000000000002dd prog.ret
        addr:0x000000000002de prog.label[symbol:model.layers.10.mlp.__entry]
        addr:0x000000000002df prog.kernel_launch(%1562:tensor<[1, 192, 1536], Float32, CPU>) -> (%1563:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.10.mlp.gate_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x000000000002e0 prog.kernel_launch(%1563:tensor<[1, 192, 8960], Float32, CPU>) -> (%1564:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.10.mlp.act, op_type:SiLU, op_options:null]
        addr:0x000000000002e1 prog.free(%1563:tensor<[1, 192, 8960], Float32, CPU>) -> ()
        addr:0x000000000002e2 prog.kernel_launch(%1562:tensor<[1, 192, 1536], Float32, CPU>) -> (%1565:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.10.mlp.up_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x000000000002e3 prog.free(%1562:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x000000000002e4 prog.kernel_launch(%1564:tensor<[1, 192, 8960], Float32, CPU>, %1565:tensor<[1, 192, 8960], Float32, CPU>) -> (%1566:tensor<[1, 192, 8960], Float32, CPU>)[op_type:Mul, op_options:null]
        addr:0x000000000002e5 prog.free(%1565:tensor<[1, 192, 8960], Float32, CPU>) -> ()
        addr:0x000000000002e6 prog.free(%1564:tensor<[1, 192, 8960], Float32, CPU>) -> ()
        addr:0x000000000002e7 prog.kernel_launch(%1566:tensor<[1, 192, 8960], Float32, CPU>) -> (%1567:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.10.mlp.down_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":8960,"out_channels":1536}]
        addr:0x000000000002e8 prog.free(%1566:tensor<[1, 192, 8960], Float32, CPU>) -> ()
        addr:0x000000000002e9 prog.ret
        addr:0x000000000002ea prog.label[symbol:model.layers.11.__entry]
        addr:0x000000000002eb prog.kernel_launch(%1568:tensor<[1, 192, 1536], Float32, CPU>) -> (%1569:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.11.input_layernorm, op_type:RMSNorm, op_options:{"add_unit_offset":false,"epsilon":9.999999974752427e-07}]
        addr:0x000000000002ec prog.jump model.layers.11.self_attn.__entry[offset:10]
        addr:0x000000000002ed prog.kernel_launch(%1587:tensor<[1, 192, 1536], Float32, CPU>, %1568:tensor<[1, 192, 1536], Float32, CPU>) -> (%1588:tensor<[1, 192, 1536], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x000000000002ee prog.free(%1568:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x000000000002ef prog.free(%1587:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x000000000002f0 prog.kernel_launch(%1588:tensor<[1, 192, 1536], Float32, CPU>) -> (%1589:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.11.post_attention_layernorm, op_type:RMSNorm, op_options:{"add_unit_offset":false,"epsilon":9.999999974752427e-07}]
        addr:0x000000000002f1 prog.jump model.layers.11.mlp.__entry[offset:45]
        addr:0x000000000002f2 prog.kernel_launch(%1594:tensor<[1, 192, 1536], Float32, CPU>, %1588:tensor<[1, 192, 1536], Float32, CPU>) -> (%1595:tensor<[1, 192, 1536], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x000000000002f3 prog.free(%1588:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x000000000002f4 prog.free(%1594:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x000000000002f5 prog.ret
        addr:0x000000000002f6 prog.label[symbol:model.layers.11.self_attn.__entry]
        addr:0x000000000002f7 prog.kernel_launch(%1569:tensor<[1, 192, 1536], Float32, CPU>) -> (%1570:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.11.self_attn.q_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x000000000002f8 prog.kernel_launch(%1569:tensor<[1, 192, 1536], Float32, CPU>) -> (%1571:tensor<[1, 192, 256], Float32, CPU>)[symbol_name:model.layers.11.self_attn.k_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x000000000002f9 prog.kernel_launch(%1569:tensor<[1, 192, 1536], Float32, CPU>) -> (%1572:tensor<[1, 192, 256], Float32, CPU>)[symbol_name:model.layers.11.self_attn.v_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x000000000002fa prog.free(%1569:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x000000000002fb prog.kernel_launch(%1570:tensor<[1, 192, 1536], Float32, CPU>) -> (%1570:tensor<[1, 192, 12, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,12,128]}]
        addr:0x000000000002fc prog.kernel_launch(%1571:tensor<[1, 192, 256], Float32, CPU>) -> (%1571:tensor<[1, 192, 2, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,2,128]}]
        addr:0x000000000002fd prog.kernel_launch(%1572:tensor<[1, 192, 256], Float32, CPU>) -> (%1572:tensor<[1, 192, 2, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,2,128]}]
        addr:0x000000000002fe prog.kernel_launch(%1570:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%1573:tensor<[1, 12, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x000000000002ff prog.free(%1570:tensor<[1, 192, 12, 128], Float32, CPU>) -> ()
        addr:0x00000000000300 prog.kernel_launch(%1571:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%1574:tensor<[1, 2, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x00000000000301 prog.free(%1571:tensor<[1, 192, 2, 128], Float32, CPU>) -> ()
        addr:0x00000000000302 prog.kernel_launch(%1572:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%1575:tensor<[1, 2, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x00000000000303 prog.free(%1572:tensor<[1, 192, 2, 128], Float32, CPU>) -> ()
        addr:0x00000000000304 prog.kernel_launch(%1573:tensor<[1, 12, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1576:tensor<[1, 12, 192, 128], Float32, CPU>)[symbol_name:model.layers.11.self_attn.q_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x00000000000305 prog.free(%1573:tensor<[1, 12, 192, 128], Float32, CPU>) -> ()
        addr:0x00000000000306 prog.kernel_launch(%1574:tensor<[1, 2, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1577:tensor<[1, 2, 192, 128], Float32, CPU>)[symbol_name:model.layers.11.self_attn.k_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x00000000000307 prog.free(%1574:tensor<[1, 2, 192, 128], Float32, CPU>) -> ()
        addr:0x00000000000308 prog.kernel_launch(%1577:tensor<[1, 2, 192, 128], Float32, CPU>, %1575:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1578:tensor<[1, 2, 192, 128], Float32, CPU>, %1579:tensor<[1, 2, 192, 128], Float32, CPU>)[symbol_name:model.layers.11.self_attn.kv_cache, op_type:KVCache, op_options:{"head_dim":128,"kv_head":2,"layer_idx":0,"q_head":12,"use_fa2":false}]
        addr:0x00000000000309 prog.free(%1575:tensor<[1, 2, 192, 128], Float32, CPU>) -> ()
        addr:0x0000000000030a prog.free(%1577:tensor<[1, 2, 192, 128], Float32, CPU>) -> ()
        addr:0x0000000000030b prog.kernel_launch(%1576:tensor<[1, 12, 192, 128], Float32, CPU>, %1578:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1580:tensor<[1, 12, 192, 192], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":true}]
        addr:0x0000000000030c prog.free(%1578:tensor<[1, 2, 192, 128], Float32, CPU>) -> ()
        addr:0x0000000000030d prog.free(%1576:tensor<[1, 12, 192, 128], Float32, CPU>) -> ()
        addr:0x0000000000030e prog.kernel_launch(%1580:tensor<[1, 12, 192, 192], Float32, CPU>, %1581:tensor<[1], Float32, CPU>) -> (%1582:tensor<[1, 12, 192, 192], Float32, CPU>)[op_type:Mul, op_options:null]
        addr:0x0000000000030f prog.free(%1581:tensor<[1], Float32, CPU>) -> ()
        addr:0x00000000000310 prog.free(%1580:tensor<[1, 12, 192, 192], Float32, CPU>) -> ()
        addr:0x00000000000311 prog.kernel_launch(%1582:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%1583:tensor<[1, 12, 192, 192], Float32, CPU>)[symbol_name:model.layers.11.self_attn.mask, op_type:CausalMask, op_options:{"sliding_window":false,"window_size":0}]
        addr:0x00000000000312 prog.free(%1582:tensor<[1, 12, 192, 192], Float32, CPU>) -> ()
        addr:0x00000000000313 prog.kernel_launch(%1583:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%1584:tensor<[1, 12, 192, 192], Float32, CPU>)[symbol_name:model.layers.11.self_attn.softmax, op_type:Softmax, op_options:{"axis":-1}]
        addr:0x00000000000314 prog.free(%1583:tensor<[1, 12, 192, 192], Float32, CPU>) -> ()
        addr:0x00000000000315 prog.kernel_launch(%1584:tensor<[1, 12, 192, 192], Float32, CPU>, %1579:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1585:tensor<[1, 12, 192, 128], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":false}]
        addr:0x00000000000316 prog.free(%1579:tensor<[1, 2, 192, 128], Float32, CPU>) -> ()
        addr:0x00000000000317 prog.free(%1584:tensor<[1, 12, 192, 192], Float32, CPU>) -> ()
        addr:0x00000000000318 prog.kernel_launch(%1585:tensor<[1, 12, 192, 128], Float32, CPU>) -> (%1586:tensor<[1, 192, 12, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x00000000000319 prog.free(%1585:tensor<[1, 12, 192, 128], Float32, CPU>) -> ()
        addr:0x0000000000031a prog.kernel_launch(%1586:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%1586:tensor<[1, 192, 1536], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,1536]}]
        addr:0x0000000000031b prog.kernel_launch(%1586:tensor<[1, 192, 1536], Float32, CPU>) -> (%1587:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.11.self_attn.o_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x0000000000031c prog.free(%1586:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x0000000000031d prog.ret
        addr:0x0000000000031e prog.label[symbol:model.layers.11.mlp.__entry]
        addr:0x0000000000031f prog.kernel_launch(%1589:tensor<[1, 192, 1536], Float32, CPU>) -> (%1590:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.11.mlp.gate_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x00000000000320 prog.kernel_launch(%1590:tensor<[1, 192, 8960], Float32, CPU>) -> (%1591:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.11.mlp.act, op_type:SiLU, op_options:null]
        addr:0x00000000000321 prog.free(%1590:tensor<[1, 192, 8960], Float32, CPU>) -> ()
        addr:0x00000000000322 prog.kernel_launch(%1589:tensor<[1, 192, 1536], Float32, CPU>) -> (%1592:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.11.mlp.up_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x00000000000323 prog.free(%1589:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x00000000000324 prog.kernel_launch(%1591:tensor<[1, 192, 8960], Float32, CPU>, %1592:tensor<[1, 192, 8960], Float32, CPU>) -> (%1593:tensor<[1, 192, 8960], Float32, CPU>)[op_type:Mul, op_options:null]
        addr:0x00000000000325 prog.free(%1592:tensor<[1, 192, 8960], Float32, CPU>) -> ()
        addr:0x00000000000326 prog.free(%1591:tensor<[1, 192, 8960], Float32, CPU>) -> ()
        addr:0x00000000000327 prog.kernel_launch(%1593:tensor<[1, 192, 8960], Float32, CPU>) -> (%1594:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.11.mlp.down_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":8960,"out_channels":1536}]
        addr:0x00000000000328 prog.free(%1593:tensor<[1, 192, 8960], Float32, CPU>) -> ()
        addr:0x00000000000329 prog.ret
        addr:0x0000000000032a prog.label[symbol:model.layers.12.__entry]
        addr:0x0000000000032b prog.kernel_launch(%1595:tensor<[1, 192, 1536], Float32, CPU>) -> (%1596:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.12.input_layernorm, op_type:RMSNorm, op_options:{"add_unit_offset":false,"epsilon":9.999999974752427e-07}]
        addr:0x0000000000032c prog.jump model.layers.12.self_attn.__entry[offset:10]
        addr:0x0000000000032d prog.kernel_launch(%1614:tensor<[1, 192, 1536], Float32, CPU>, %1595:tensor<[1, 192, 1536], Float32, CPU>) -> (%1615:tensor<[1, 192, 1536], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x0000000000032e prog.free(%1595:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x0000000000032f prog.free(%1614:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x00000000000330 prog.kernel_launch(%1615:tensor<[1, 192, 1536], Float32, CPU>) -> (%1616:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.12.post_attention_layernorm, op_type:RMSNorm, op_options:{"add_unit_offset":false,"epsilon":9.999999974752427e-07}]
        addr:0x00000000000331 prog.jump model.layers.12.mlp.__entry[offset:45]
        addr:0x00000000000332 prog.kernel_launch(%1621:tensor<[1, 192, 1536], Float32, CPU>, %1615:tensor<[1, 192, 1536], Float32, CPU>) -> (%1622:tensor<[1, 192, 1536], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x00000000000333 prog.free(%1615:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x00000000000334 prog.free(%1621:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x00000000000335 prog.ret
        addr:0x00000000000336 prog.label[symbol:model.layers.12.self_attn.__entry]
        addr:0x00000000000337 prog.kernel_launch(%1596:tensor<[1, 192, 1536], Float32, CPU>) -> (%1597:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.12.self_attn.q_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x00000000000338 prog.kernel_launch(%1596:tensor<[1, 192, 1536], Float32, CPU>) -> (%1598:tensor<[1, 192, 256], Float32, CPU>)[symbol_name:model.layers.12.self_attn.k_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x00000000000339 prog.kernel_launch(%1596:tensor<[1, 192, 1536], Float32, CPU>) -> (%1599:tensor<[1, 192, 256], Float32, CPU>)[symbol_name:model.layers.12.self_attn.v_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x0000000000033a prog.free(%1596:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x0000000000033b prog.kernel_launch(%1597:tensor<[1, 192, 1536], Float32, CPU>) -> (%1597:tensor<[1, 192, 12, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,12,128]}]
        addr:0x0000000000033c prog.kernel_launch(%1598:tensor<[1, 192, 256], Float32, CPU>) -> (%1598:tensor<[1, 192, 2, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,2,128]}]
        addr:0x0000000000033d prog.kernel_launch(%1599:tensor<[1, 192, 256], Float32, CPU>) -> (%1599:tensor<[1, 192, 2, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,2,128]}]
        addr:0x0000000000033e prog.kernel_launch(%1597:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%1600:tensor<[1, 12, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x0000000000033f prog.free(%1597:tensor<[1, 192, 12, 128], Float32, CPU>) -> ()
        addr:0x00000000000340 prog.kernel_launch(%1598:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%1601:tensor<[1, 2, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x00000000000341 prog.free(%1598:tensor<[1, 192, 2, 128], Float32, CPU>) -> ()
        addr:0x00000000000342 prog.kernel_launch(%1599:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%1602:tensor<[1, 2, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x00000000000343 prog.free(%1599:tensor<[1, 192, 2, 128], Float32, CPU>) -> ()
        addr:0x00000000000344 prog.kernel_launch(%1600:tensor<[1, 12, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1603:tensor<[1, 12, 192, 128], Float32, CPU>)[symbol_name:model.layers.12.self_attn.q_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x00000000000345 prog.free(%1600:tensor<[1, 12, 192, 128], Float32, CPU>) -> ()
        addr:0x00000000000346 prog.kernel_launch(%1601:tensor<[1, 2, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1604:tensor<[1, 2, 192, 128], Float32, CPU>)[symbol_name:model.layers.12.self_attn.k_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x00000000000347 prog.free(%1601:tensor<[1, 2, 192, 128], Float32, CPU>) -> ()
        addr:0x00000000000348 prog.kernel_launch(%1604:tensor<[1, 2, 192, 128], Float32, CPU>, %1602:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1605:tensor<[1, 2, 192, 128], Float32, CPU>, %1606:tensor<[1, 2, 192, 128], Float32, CPU>)[symbol_name:model.layers.12.self_attn.kv_cache, op_type:KVCache, op_options:{"head_dim":128,"kv_head":2,"layer_idx":0,"q_head":12,"use_fa2":false}]
        addr:0x00000000000349 prog.free(%1602:tensor<[1, 2, 192, 128], Float32, CPU>) -> ()
        addr:0x0000000000034a prog.free(%1604:tensor<[1, 2, 192, 128], Float32, CPU>) -> ()
        addr:0x0000000000034b prog.kernel_launch(%1603:tensor<[1, 12, 192, 128], Float32, CPU>, %1605:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1607:tensor<[1, 12, 192, 192], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":true}]
        addr:0x0000000000034c prog.free(%1605:tensor<[1, 2, 192, 128], Float32, CPU>) -> ()
        addr:0x0000000000034d prog.free(%1603:tensor<[1, 12, 192, 128], Float32, CPU>) -> ()
        addr:0x0000000000034e prog.kernel_launch(%1607:tensor<[1, 12, 192, 192], Float32, CPU>, %1608:tensor<[1], Float32, CPU>) -> (%1609:tensor<[1, 12, 192, 192], Float32, CPU>)[op_type:Mul, op_options:null]
        addr:0x0000000000034f prog.free(%1608:tensor<[1], Float32, CPU>) -> ()
        addr:0x00000000000350 prog.free(%1607:tensor<[1, 12, 192, 192], Float32, CPU>) -> ()
        addr:0x00000000000351 prog.kernel_launch(%1609:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%1610:tensor<[1, 12, 192, 192], Float32, CPU>)[symbol_name:model.layers.12.self_attn.mask, op_type:CausalMask, op_options:{"sliding_window":false,"window_size":0}]
        addr:0x00000000000352 prog.free(%1609:tensor<[1, 12, 192, 192], Float32, CPU>) -> ()
        addr:0x00000000000353 prog.kernel_launch(%1610:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%1611:tensor<[1, 12, 192, 192], Float32, CPU>)[symbol_name:model.layers.12.self_attn.softmax, op_type:Softmax, op_options:{"axis":-1}]
        addr:0x00000000000354 prog.free(%1610:tensor<[1, 12, 192, 192], Float32, CPU>) -> ()
        addr:0x00000000000355 prog.kernel_launch(%1611:tensor<[1, 12, 192, 192], Float32, CPU>, %1606:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1612:tensor<[1, 12, 192, 128], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":false}]
        addr:0x00000000000356 prog.free(%1606:tensor<[1, 2, 192, 128], Float32, CPU>) -> ()
        addr:0x00000000000357 prog.free(%1611:tensor<[1, 12, 192, 192], Float32, CPU>) -> ()
        addr:0x00000000000358 prog.kernel_launch(%1612:tensor<[1, 12, 192, 128], Float32, CPU>) -> (%1613:tensor<[1, 192, 12, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x00000000000359 prog.free(%1612:tensor<[1, 12, 192, 128], Float32, CPU>) -> ()
        addr:0x0000000000035a prog.kernel_launch(%1613:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%1613:tensor<[1, 192, 1536], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,1536]}]
        addr:0x0000000000035b prog.kernel_launch(%1613:tensor<[1, 192, 1536], Float32, CPU>) -> (%1614:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.12.self_attn.o_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x0000000000035c prog.free(%1613:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x0000000000035d prog.ret
        addr:0x0000000000035e prog.label[symbol:model.layers.12.mlp.__entry]
        addr:0x0000000000035f prog.kernel_launch(%1616:tensor<[1, 192, 1536], Float32, CPU>) -> (%1617:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.12.mlp.gate_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x00000000000360 prog.kernel_launch(%1617:tensor<[1, 192, 8960], Float32, CPU>) -> (%1618:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.12.mlp.act, op_type:SiLU, op_options:null]
        addr:0x00000000000361 prog.free(%1617:tensor<[1, 192, 8960], Float32, CPU>) -> ()
        addr:0x00000000000362 prog.kernel_launch(%1616:tensor<[1, 192, 1536], Float32, CPU>) -> (%1619:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.12.mlp.up_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x00000000000363 prog.free(%1616:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x00000000000364 prog.kernel_launch(%1618:tensor<[1, 192, 8960], Float32, CPU>, %1619:tensor<[1, 192, 8960], Float32, CPU>) -> (%1620:tensor<[1, 192, 8960], Float32, CPU>)[op_type:Mul, op_options:null]
        addr:0x00000000000365 prog.free(%1619:tensor<[1, 192, 8960], Float32, CPU>) -> ()
        addr:0x00000000000366 prog.free(%1618:tensor<[1, 192, 8960], Float32, CPU>) -> ()
        addr:0x00000000000367 prog.kernel_launch(%1620:tensor<[1, 192, 8960], Float32, CPU>) -> (%1621:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.12.mlp.down_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":8960,"out_channels":1536}]
        addr:0x00000000000368 prog.free(%1620:tensor<[1, 192, 8960], Float32, CPU>) -> ()
        addr:0x00000000000369 prog.ret
        addr:0x0000000000036a prog.label[symbol:model.layers.13.__entry]
        addr:0x0000000000036b prog.kernel_launch(%1622:tensor<[1, 192, 1536], Float32, CPU>) -> (%1623:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.13.input_layernorm, op_type:RMSNorm, op_options:{"add_unit_offset":false,"epsilon":9.999999974752427e-07}]
        addr:0x0000000000036c prog.jump model.layers.13.self_attn.__entry[offset:10]
        addr:0x0000000000036d prog.kernel_launch(%1641:tensor<[1, 192, 1536], Float32, CPU>, %1622:tensor<[1, 192, 1536], Float32, CPU>) -> (%1642:tensor<[1, 192, 1536], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x0000000000036e prog.free(%1622:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x0000000000036f prog.free(%1641:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x00000000000370 prog.kernel_launch(%1642:tensor<[1, 192, 1536], Float32, CPU>) -> (%1643:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.13.post_attention_layernorm, op_type:RMSNorm, op_options:{"add_unit_offset":false,"epsilon":9.999999974752427e-07}]
        addr:0x00000000000371 prog.jump model.layers.13.mlp.__entry[offset:45]
        addr:0x00000000000372 prog.kernel_launch(%1648:tensor<[1, 192, 1536], Float32, CPU>, %1642:tensor<[1, 192, 1536], Float32, CPU>) -> (%1649:tensor<[1, 192, 1536], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x00000000000373 prog.free(%1642:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x00000000000374 prog.free(%1648:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x00000000000375 prog.ret
        addr:0x00000000000376 prog.label[symbol:model.layers.13.self_attn.__entry]
        addr:0x00000000000377 prog.kernel_launch(%1623:tensor<[1, 192, 1536], Float32, CPU>) -> (%1624:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.13.self_attn.q_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x00000000000378 prog.kernel_launch(%1623:tensor<[1, 192, 1536], Float32, CPU>) -> (%1625:tensor<[1, 192, 256], Float32, CPU>)[symbol_name:model.layers.13.self_attn.k_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x00000000000379 prog.kernel_launch(%1623:tensor<[1, 192, 1536], Float32, CPU>) -> (%1626:tensor<[1, 192, 256], Float32, CPU>)[symbol_name:model.layers.13.self_attn.v_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x0000000000037a prog.free(%1623:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x0000000000037b prog.kernel_launch(%1624:tensor<[1, 192, 1536], Float32, CPU>) -> (%1624:tensor<[1, 192, 12, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,12,128]}]
        addr:0x0000000000037c prog.kernel_launch(%1625:tensor<[1, 192, 256], Float32, CPU>) -> (%1625:tensor<[1, 192, 2, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,2,128]}]
        addr:0x0000000000037d prog.kernel_launch(%1626:tensor<[1, 192, 256], Float32, CPU>) -> (%1626:tensor<[1, 192, 2, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,2,128]}]
        addr:0x0000000000037e prog.kernel_launch(%1624:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%1627:tensor<[1, 12, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x0000000000037f prog.free(%1624:tensor<[1, 192, 12, 128], Float32, CPU>) -> ()
        addr:0x00000000000380 prog.kernel_launch(%1625:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%1628:tensor<[1, 2, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x00000000000381 prog.free(%1625:tensor<[1, 192, 2, 128], Float32, CPU>) -> ()
        addr:0x00000000000382 prog.kernel_launch(%1626:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%1629:tensor<[1, 2, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x00000000000383 prog.free(%1626:tensor<[1, 192, 2, 128], Float32, CPU>) -> ()
        addr:0x00000000000384 prog.kernel_launch(%1627:tensor<[1, 12, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1630:tensor<[1, 12, 192, 128], Float32, CPU>)[symbol_name:model.layers.13.self_attn.q_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x00000000000385 prog.free(%1627:tensor<[1, 12, 192, 128], Float32, CPU>) -> ()
        addr:0x00000000000386 prog.kernel_launch(%1628:tensor<[1, 2, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1631:tensor<[1, 2, 192, 128], Float32, CPU>)[symbol_name:model.layers.13.self_attn.k_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x00000000000387 prog.free(%1628:tensor<[1, 2, 192, 128], Float32, CPU>) -> ()
        addr:0x00000000000388 prog.kernel_launch(%1631:tensor<[1, 2, 192, 128], Float32, CPU>, %1629:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1632:tensor<[1, 2, 192, 128], Float32, CPU>, %1633:tensor<[1, 2, 192, 128], Float32, CPU>)[symbol_name:model.layers.13.self_attn.kv_cache, op_type:KVCache, op_options:{"head_dim":128,"kv_head":2,"layer_idx":0,"q_head":12,"use_fa2":false}]
        addr:0x00000000000389 prog.free(%1629:tensor<[1, 2, 192, 128], Float32, CPU>) -> ()
        addr:0x0000000000038a prog.free(%1631:tensor<[1, 2, 192, 128], Float32, CPU>) -> ()
        addr:0x0000000000038b prog.kernel_launch(%1630:tensor<[1, 12, 192, 128], Float32, CPU>, %1632:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1634:tensor<[1, 12, 192, 192], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":true}]
        addr:0x0000000000038c prog.free(%1632:tensor<[1, 2, 192, 128], Float32, CPU>) -> ()
        addr:0x0000000000038d prog.free(%1630:tensor<[1, 12, 192, 128], Float32, CPU>) -> ()
        addr:0x0000000000038e prog.kernel_launch(%1634:tensor<[1, 12, 192, 192], Float32, CPU>, %1635:tensor<[1], Float32, CPU>) -> (%1636:tensor<[1, 12, 192, 192], Float32, CPU>)[op_type:Mul, op_options:null]
        addr:0x0000000000038f prog.free(%1635:tensor<[1], Float32, CPU>) -> ()
        addr:0x00000000000390 prog.free(%1634:tensor<[1, 12, 192, 192], Float32, CPU>) -> ()
        addr:0x00000000000391 prog.kernel_launch(%1636:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%1637:tensor<[1, 12, 192, 192], Float32, CPU>)[symbol_name:model.layers.13.self_attn.mask, op_type:CausalMask, op_options:{"sliding_window":false,"window_size":0}]
        addr:0x00000000000392 prog.free(%1636:tensor<[1, 12, 192, 192], Float32, CPU>) -> ()
        addr:0x00000000000393 prog.kernel_launch(%1637:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%1638:tensor<[1, 12, 192, 192], Float32, CPU>)[symbol_name:model.layers.13.self_attn.softmax, op_type:Softmax, op_options:{"axis":-1}]
        addr:0x00000000000394 prog.free(%1637:tensor<[1, 12, 192, 192], Float32, CPU>) -> ()
        addr:0x00000000000395 prog.kernel_launch(%1638:tensor<[1, 12, 192, 192], Float32, CPU>, %1633:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1639:tensor<[1, 12, 192, 128], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":false}]
        addr:0x00000000000396 prog.free(%1633:tensor<[1, 2, 192, 128], Float32, CPU>) -> ()
        addr:0x00000000000397 prog.free(%1638:tensor<[1, 12, 192, 192], Float32, CPU>) -> ()
        addr:0x00000000000398 prog.kernel_launch(%1639:tensor<[1, 12, 192, 128], Float32, CPU>) -> (%1640:tensor<[1, 192, 12, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x00000000000399 prog.free(%1639:tensor<[1, 12, 192, 128], Float32, CPU>) -> ()
        addr:0x0000000000039a prog.kernel_launch(%1640:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%1640:tensor<[1, 192, 1536], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,1536]}]
        addr:0x0000000000039b prog.kernel_launch(%1640:tensor<[1, 192, 1536], Float32, CPU>) -> (%1641:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.13.self_attn.o_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x0000000000039c prog.free(%1640:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x0000000000039d prog.ret
        addr:0x0000000000039e prog.label[symbol:model.layers.13.mlp.__entry]
        addr:0x0000000000039f prog.kernel_launch(%1643:tensor<[1, 192, 1536], Float32, CPU>) -> (%1644:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.13.mlp.gate_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x000000000003a0 prog.kernel_launch(%1644:tensor<[1, 192, 8960], Float32, CPU>) -> (%1645:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.13.mlp.act, op_type:SiLU, op_options:null]
        addr:0x000000000003a1 prog.free(%1644:tensor<[1, 192, 8960], Float32, CPU>) -> ()
        addr:0x000000000003a2 prog.kernel_launch(%1643:tensor<[1, 192, 1536], Float32, CPU>) -> (%1646:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.13.mlp.up_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x000000000003a3 prog.free(%1643:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x000000000003a4 prog.kernel_launch(%1645:tensor<[1, 192, 8960], Float32, CPU>, %1646:tensor<[1, 192, 8960], Float32, CPU>) -> (%1647:tensor<[1, 192, 8960], Float32, CPU>)[op_type:Mul, op_options:null]
        addr:0x000000000003a5 prog.free(%1646:tensor<[1, 192, 8960], Float32, CPU>) -> ()
        addr:0x000000000003a6 prog.free(%1645:tensor<[1, 192, 8960], Float32, CPU>) -> ()
        addr:0x000000000003a7 prog.kernel_launch(%1647:tensor<[1, 192, 8960], Float32, CPU>) -> (%1648:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.13.mlp.down_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":8960,"out_channels":1536}]
        addr:0x000000000003a8 prog.free(%1647:tensor<[1, 192, 8960], Float32, CPU>) -> ()
        addr:0x000000000003a9 prog.ret
        addr:0x000000000003aa prog.label[symbol:model.layers.14.__entry]
        addr:0x000000000003ab prog.kernel_launch(%1649:tensor<[1, 192, 1536], Float32, CPU>) -> (%1650:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.14.input_layernorm, op_type:RMSNorm, op_options:{"add_unit_offset":false,"epsilon":9.999999974752427e-07}]
        addr:0x000000000003ac prog.jump model.layers.14.self_attn.__entry[offset:10]
        addr:0x000000000003ad prog.kernel_launch(%1668:tensor<[1, 192, 1536], Float32, CPU>, %1649:tensor<[1, 192, 1536], Float32, CPU>) -> (%1669:tensor<[1, 192, 1536], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x000000000003ae prog.free(%1649:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x000000000003af prog.free(%1668:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x000000000003b0 prog.kernel_launch(%1669:tensor<[1, 192, 1536], Float32, CPU>) -> (%1670:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.14.post_attention_layernorm, op_type:RMSNorm, op_options:{"add_unit_offset":false,"epsilon":9.999999974752427e-07}]
        addr:0x000000000003b1 prog.jump model.layers.14.mlp.__entry[offset:45]
        addr:0x000000000003b2 prog.kernel_launch(%1675:tensor<[1, 192, 1536], Float32, CPU>, %1669:tensor<[1, 192, 1536], Float32, CPU>) -> (%1676:tensor<[1, 192, 1536], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x000000000003b3 prog.free(%1669:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x000000000003b4 prog.free(%1675:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x000000000003b5 prog.ret
        addr:0x000000000003b6 prog.label[symbol:model.layers.14.self_attn.__entry]
        addr:0x000000000003b7 prog.kernel_launch(%1650:tensor<[1, 192, 1536], Float32, CPU>) -> (%1651:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.14.self_attn.q_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x000000000003b8 prog.kernel_launch(%1650:tensor<[1, 192, 1536], Float32, CPU>) -> (%1652:tensor<[1, 192, 256], Float32, CPU>)[symbol_name:model.layers.14.self_attn.k_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x000000000003b9 prog.kernel_launch(%1650:tensor<[1, 192, 1536], Float32, CPU>) -> (%1653:tensor<[1, 192, 256], Float32, CPU>)[symbol_name:model.layers.14.self_attn.v_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x000000000003ba prog.free(%1650:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x000000000003bb prog.kernel_launch(%1651:tensor<[1, 192, 1536], Float32, CPU>) -> (%1651:tensor<[1, 192, 12, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,12,128]}]
        addr:0x000000000003bc prog.kernel_launch(%1652:tensor<[1, 192, 256], Float32, CPU>) -> (%1652:tensor<[1, 192, 2, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,2,128]}]
        addr:0x000000000003bd prog.kernel_launch(%1653:tensor<[1, 192, 256], Float32, CPU>) -> (%1653:tensor<[1, 192, 2, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,2,128]}]
        addr:0x000000000003be prog.kernel_launch(%1651:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%1654:tensor<[1, 12, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x000000000003bf prog.free(%1651:tensor<[1, 192, 12, 128], Float32, CPU>) -> ()
        addr:0x000000000003c0 prog.kernel_launch(%1652:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%1655:tensor<[1, 2, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x000000000003c1 prog.free(%1652:tensor<[1, 192, 2, 128], Float32, CPU>) -> ()
        addr:0x000000000003c2 prog.kernel_launch(%1653:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%1656:tensor<[1, 2, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x000000000003c3 prog.free(%1653:tensor<[1, 192, 2, 128], Float32, CPU>) -> ()
        addr:0x000000000003c4 prog.kernel_launch(%1654:tensor<[1, 12, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1657:tensor<[1, 12, 192, 128], Float32, CPU>)[symbol_name:model.layers.14.self_attn.q_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x000000000003c5 prog.free(%1654:tensor<[1, 12, 192, 128], Float32, CPU>) -> ()
        addr:0x000000000003c6 prog.kernel_launch(%1655:tensor<[1, 2, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1658:tensor<[1, 2, 192, 128], Float32, CPU>)[symbol_name:model.layers.14.self_attn.k_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x000000000003c7 prog.free(%1655:tensor<[1, 2, 192, 128], Float32, CPU>) -> ()
        addr:0x000000000003c8 prog.kernel_launch(%1658:tensor<[1, 2, 192, 128], Float32, CPU>, %1656:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1659:tensor<[1, 2, 192, 128], Float32, CPU>, %1660:tensor<[1, 2, 192, 128], Float32, CPU>)[symbol_name:model.layers.14.self_attn.kv_cache, op_type:KVCache, op_options:{"head_dim":128,"kv_head":2,"layer_idx":0,"q_head":12,"use_fa2":false}]
        addr:0x000000000003c9 prog.free(%1656:tensor<[1, 2, 192, 128], Float32, CPU>) -> ()
        addr:0x000000000003ca prog.free(%1658:tensor<[1, 2, 192, 128], Float32, CPU>) -> ()
        addr:0x000000000003cb prog.kernel_launch(%1657:tensor<[1, 12, 192, 128], Float32, CPU>, %1659:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1661:tensor<[1, 12, 192, 192], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":true}]
        addr:0x000000000003cc prog.free(%1659:tensor<[1, 2, 192, 128], Float32, CPU>) -> ()
        addr:0x000000000003cd prog.free(%1657:tensor<[1, 12, 192, 128], Float32, CPU>) -> ()
        addr:0x000000000003ce prog.kernel_launch(%1661:tensor<[1, 12, 192, 192], Float32, CPU>, %1662:tensor<[1], Float32, CPU>) -> (%1663:tensor<[1, 12, 192, 192], Float32, CPU>)[op_type:Mul, op_options:null]
        addr:0x000000000003cf prog.free(%1662:tensor<[1], Float32, CPU>) -> ()
        addr:0x000000000003d0 prog.free(%1661:tensor<[1, 12, 192, 192], Float32, CPU>) -> ()
        addr:0x000000000003d1 prog.kernel_launch(%1663:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%1664:tensor<[1, 12, 192, 192], Float32, CPU>)[symbol_name:model.layers.14.self_attn.mask, op_type:CausalMask, op_options:{"sliding_window":false,"window_size":0}]
        addr:0x000000000003d2 prog.free(%1663:tensor<[1, 12, 192, 192], Float32, CPU>) -> ()
        addr:0x000000000003d3 prog.kernel_launch(%1664:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%1665:tensor<[1, 12, 192, 192], Float32, CPU>)[symbol_name:model.layers.14.self_attn.softmax, op_type:Softmax, op_options:{"axis":-1}]
        addr:0x000000000003d4 prog.free(%1664:tensor<[1, 12, 192, 192], Float32, CPU>) -> ()
        addr:0x000000000003d5 prog.kernel_launch(%1665:tensor<[1, 12, 192, 192], Float32, CPU>, %1660:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1666:tensor<[1, 12, 192, 128], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":false}]
        addr:0x000000000003d6 prog.free(%1660:tensor<[1, 2, 192, 128], Float32, CPU>) -> ()
        addr:0x000000000003d7 prog.free(%1665:tensor<[1, 12, 192, 192], Float32, CPU>) -> ()
        addr:0x000000000003d8 prog.kernel_launch(%1666:tensor<[1, 12, 192, 128], Float32, CPU>) -> (%1667:tensor<[1, 192, 12, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x000000000003d9 prog.free(%1666:tensor<[1, 12, 192, 128], Float32, CPU>) -> ()
        addr:0x000000000003da prog.kernel_launch(%1667:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%1667:tensor<[1, 192, 1536], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,1536]}]
        addr:0x000000000003db prog.kernel_launch(%1667:tensor<[1, 192, 1536], Float32, CPU>) -> (%1668:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.14.self_attn.o_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x000000000003dc prog.free(%1667:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x000000000003dd prog.ret
        addr:0x000000000003de prog.label[symbol:model.layers.14.mlp.__entry]
        addr:0x000000000003df prog.kernel_launch(%1670:tensor<[1, 192, 1536], Float32, CPU>) -> (%1671:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.14.mlp.gate_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x000000000003e0 prog.kernel_launch(%1671:tensor<[1, 192, 8960], Float32, CPU>) -> (%1672:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.14.mlp.act, op_type:SiLU, op_options:null]
        addr:0x000000000003e1 prog.free(%1671:tensor<[1, 192, 8960], Float32, CPU>) -> ()
        addr:0x000000000003e2 prog.kernel_launch(%1670:tensor<[1, 192, 1536], Float32, CPU>) -> (%1673:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.14.mlp.up_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x000000000003e3 prog.free(%1670:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x000000000003e4 prog.kernel_launch(%1672:tensor<[1, 192, 8960], Float32, CPU>, %1673:tensor<[1, 192, 8960], Float32, CPU>) -> (%1674:tensor<[1, 192, 8960], Float32, CPU>)[op_type:Mul, op_options:null]
        addr:0x000000000003e5 prog.free(%1673:tensor<[1, 192, 8960], Float32, CPU>) -> ()
        addr:0x000000000003e6 prog.free(%1672:tensor<[1, 192, 8960], Float32, CPU>) -> ()
        addr:0x000000000003e7 prog.kernel_launch(%1674:tensor<[1, 192, 8960], Float32, CPU>) -> (%1675:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.14.mlp.down_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":8960,"out_channels":1536}]
        addr:0x000000000003e8 prog.free(%1674:tensor<[1, 192, 8960], Float32, CPU>) -> ()
        addr:0x000000000003e9 prog.ret
        addr:0x000000000003ea prog.label[symbol:model.layers.15.__entry]
        addr:0x000000000003eb prog.kernel_launch(%1676:tensor<[1, 192, 1536], Float32, CPU>) -> (%1677:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.15.input_layernorm, op_type:RMSNorm, op_options:{"add_unit_offset":false,"epsilon":9.999999974752427e-07}]
        addr:0x000000000003ec prog.jump model.layers.15.self_attn.__entry[offset:10]
        addr:0x000000000003ed prog.kernel_launch(%1695:tensor<[1, 192, 1536], Float32, CPU>, %1676:tensor<[1, 192, 1536], Float32, CPU>) -> (%1696:tensor<[1, 192, 1536], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x000000000003ee prog.free(%1676:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x000000000003ef prog.free(%1695:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x000000000003f0 prog.kernel_launch(%1696:tensor<[1, 192, 1536], Float32, CPU>) -> (%1697:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.15.post_attention_layernorm, op_type:RMSNorm, op_options:{"add_unit_offset":false,"epsilon":9.999999974752427e-07}]
        addr:0x000000000003f1 prog.jump model.layers.15.mlp.__entry[offset:45]
        addr:0x000000000003f2 prog.kernel_launch(%1702:tensor<[1, 192, 1536], Float32, CPU>, %1696:tensor<[1, 192, 1536], Float32, CPU>) -> (%1703:tensor<[1, 192, 1536], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x000000000003f3 prog.free(%1696:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x000000000003f4 prog.free(%1702:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x000000000003f5 prog.ret
        addr:0x000000000003f6 prog.label[symbol:model.layers.15.self_attn.__entry]
        addr:0x000000000003f7 prog.kernel_launch(%1677:tensor<[1, 192, 1536], Float32, CPU>) -> (%1678:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.15.self_attn.q_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x000000000003f8 prog.kernel_launch(%1677:tensor<[1, 192, 1536], Float32, CPU>) -> (%1679:tensor<[1, 192, 256], Float32, CPU>)[symbol_name:model.layers.15.self_attn.k_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x000000000003f9 prog.kernel_launch(%1677:tensor<[1, 192, 1536], Float32, CPU>) -> (%1680:tensor<[1, 192, 256], Float32, CPU>)[symbol_name:model.layers.15.self_attn.v_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x000000000003fa prog.free(%1677:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x000000000003fb prog.kernel_launch(%1678:tensor<[1, 192, 1536], Float32, CPU>) -> (%1678:tensor<[1, 192, 12, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,12,128]}]
        addr:0x000000000003fc prog.kernel_launch(%1679:tensor<[1, 192, 256], Float32, CPU>) -> (%1679:tensor<[1, 192, 2, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,2,128]}]
        addr:0x000000000003fd prog.kernel_launch(%1680:tensor<[1, 192, 256], Float32, CPU>) -> (%1680:tensor<[1, 192, 2, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,2,128]}]
        addr:0x000000000003fe prog.kernel_launch(%1678:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%1681:tensor<[1, 12, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x000000000003ff prog.free(%1678:tensor<[1, 192, 12, 128], Float32, CPU>) -> ()
        addr:0x00000000000400 prog.kernel_launch(%1679:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%1682:tensor<[1, 2, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x00000000000401 prog.free(%1679:tensor<[1, 192, 2, 128], Float32, CPU>) -> ()
        addr:0x00000000000402 prog.kernel_launch(%1680:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%1683:tensor<[1, 2, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x00000000000403 prog.free(%1680:tensor<[1, 192, 2, 128], Float32, CPU>) -> ()
        addr:0x00000000000404 prog.kernel_launch(%1681:tensor<[1, 12, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1684:tensor<[1, 12, 192, 128], Float32, CPU>)[symbol_name:model.layers.15.self_attn.q_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x00000000000405 prog.free(%1681:tensor<[1, 12, 192, 128], Float32, CPU>) -> ()
        addr:0x00000000000406 prog.kernel_launch(%1682:tensor<[1, 2, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1685:tensor<[1, 2, 192, 128], Float32, CPU>)[symbol_name:model.layers.15.self_attn.k_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x00000000000407 prog.free(%1682:tensor<[1, 2, 192, 128], Float32, CPU>) -> ()
        addr:0x00000000000408 prog.kernel_launch(%1685:tensor<[1, 2, 192, 128], Float32, CPU>, %1683:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1686:tensor<[1, 2, 192, 128], Float32, CPU>, %1687:tensor<[1, 2, 192, 128], Float32, CPU>)[symbol_name:model.layers.15.self_attn.kv_cache, op_type:KVCache, op_options:{"head_dim":128,"kv_head":2,"layer_idx":0,"q_head":12,"use_fa2":false}]
        addr:0x00000000000409 prog.free(%1683:tensor<[1, 2, 192, 128], Float32, CPU>) -> ()
        addr:0x0000000000040a prog.free(%1685:tensor<[1, 2, 192, 128], Float32, CPU>) -> ()
        addr:0x0000000000040b prog.kernel_launch(%1684:tensor<[1, 12, 192, 128], Float32, CPU>, %1686:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1688:tensor<[1, 12, 192, 192], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":true}]
        addr:0x0000000000040c prog.free(%1686:tensor<[1, 2, 192, 128], Float32, CPU>) -> ()
        addr:0x0000000000040d prog.free(%1684:tensor<[1, 12, 192, 128], Float32, CPU>) -> ()
        addr:0x0000000000040e prog.kernel_launch(%1688:tensor<[1, 12, 192, 192], Float32, CPU>, %1689:tensor<[1], Float32, CPU>) -> (%1690:tensor<[1, 12, 192, 192], Float32, CPU>)[op_type:Mul, op_options:null]
        addr:0x0000000000040f prog.free(%1689:tensor<[1], Float32, CPU>) -> ()
        addr:0x00000000000410 prog.free(%1688:tensor<[1, 12, 192, 192], Float32, CPU>) -> ()
        addr:0x00000000000411 prog.kernel_launch(%1690:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%1691:tensor<[1, 12, 192, 192], Float32, CPU>)[symbol_name:model.layers.15.self_attn.mask, op_type:CausalMask, op_options:{"sliding_window":false,"window_size":0}]
        addr:0x00000000000412 prog.free(%1690:tensor<[1, 12, 192, 192], Float32, CPU>) -> ()
        addr:0x00000000000413 prog.kernel_launch(%1691:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%1692:tensor<[1, 12, 192, 192], Float32, CPU>)[symbol_name:model.layers.15.self_attn.softmax, op_type:Softmax, op_options:{"axis":-1}]
        addr:0x00000000000414 prog.free(%1691:tensor<[1, 12, 192, 192], Float32, CPU>) -> ()
        addr:0x00000000000415 prog.kernel_launch(%1692:tensor<[1, 12, 192, 192], Float32, CPU>, %1687:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1693:tensor<[1, 12, 192, 128], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":false}]
        addr:0x00000000000416 prog.free(%1687:tensor<[1, 2, 192, 128], Float32, CPU>) -> ()
        addr:0x00000000000417 prog.free(%1692:tensor<[1, 12, 192, 192], Float32, CPU>) -> ()
        addr:0x00000000000418 prog.kernel_launch(%1693:tensor<[1, 12, 192, 128], Float32, CPU>) -> (%1694:tensor<[1, 192, 12, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x00000000000419 prog.free(%1693:tensor<[1, 12, 192, 128], Float32, CPU>) -> ()
        addr:0x0000000000041a prog.kernel_launch(%1694:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%1694:tensor<[1, 192, 1536], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,1536]}]
        addr:0x0000000000041b prog.kernel_launch(%1694:tensor<[1, 192, 1536], Float32, CPU>) -> (%1695:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.15.self_attn.o_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x0000000000041c prog.free(%1694:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x0000000000041d prog.ret
        addr:0x0000000000041e prog.label[symbol:model.layers.15.mlp.__entry]
        addr:0x0000000000041f prog.kernel_launch(%1697:tensor<[1, 192, 1536], Float32, CPU>) -> (%1698:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.15.mlp.gate_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x00000000000420 prog.kernel_launch(%1698:tensor<[1, 192, 8960], Float32, CPU>) -> (%1699:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.15.mlp.act, op_type:SiLU, op_options:null]
        addr:0x00000000000421 prog.free(%1698:tensor<[1, 192, 8960], Float32, CPU>) -> ()
        addr:0x00000000000422 prog.kernel_launch(%1697:tensor<[1, 192, 1536], Float32, CPU>) -> (%1700:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.15.mlp.up_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x00000000000423 prog.free(%1697:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x00000000000424 prog.kernel_launch(%1699:tensor<[1, 192, 8960], Float32, CPU>, %1700:tensor<[1, 192, 8960], Float32, CPU>) -> (%1701:tensor<[1, 192, 8960], Float32, CPU>)[op_type:Mul, op_options:null]
        addr:0x00000000000425 prog.free(%1700:tensor<[1, 192, 8960], Float32, CPU>) -> ()
        addr:0x00000000000426 prog.free(%1699:tensor<[1, 192, 8960], Float32, CPU>) -> ()
        addr:0x00000000000427 prog.kernel_launch(%1701:tensor<[1, 192, 8960], Float32, CPU>) -> (%1702:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.15.mlp.down_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":8960,"out_channels":1536}]
        addr:0x00000000000428 prog.free(%1701:tensor<[1, 192, 8960], Float32, CPU>) -> ()
        addr:0x00000000000429 prog.ret
        addr:0x0000000000042a prog.label[symbol:model.layers.16.__entry]
        addr:0x0000000000042b prog.kernel_launch(%1703:tensor<[1, 192, 1536], Float32, CPU>) -> (%1704:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.16.input_layernorm, op_type:RMSNorm, op_options:{"add_unit_offset":false,"epsilon":9.999999974752427e-07}]
        addr:0x0000000000042c prog.jump model.layers.16.self_attn.__entry[offset:10]
        addr:0x0000000000042d prog.kernel_launch(%1722:tensor<[1, 192, 1536], Float32, CPU>, %1703:tensor<[1, 192, 1536], Float32, CPU>) -> (%1723:tensor<[1, 192, 1536], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x0000000000042e prog.free(%1703:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x0000000000042f prog.free(%1722:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x00000000000430 prog.kernel_launch(%1723:tensor<[1, 192, 1536], Float32, CPU>) -> (%1724:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.16.post_attention_layernorm, op_type:RMSNorm, op_options:{"add_unit_offset":false,"epsilon":9.999999974752427e-07}]
        addr:0x00000000000431 prog.jump model.layers.16.mlp.__entry[offset:45]
        addr:0x00000000000432 prog.kernel_launch(%1729:tensor<[1, 192, 1536], Float32, CPU>, %1723:tensor<[1, 192, 1536], Float32, CPU>) -> (%1730:tensor<[1, 192, 1536], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x00000000000433 prog.free(%1723:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x00000000000434 prog.free(%1729:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x00000000000435 prog.ret
        addr:0x00000000000436 prog.label[symbol:model.layers.16.self_attn.__entry]
        addr:0x00000000000437 prog.kernel_launch(%1704:tensor<[1, 192, 1536], Float32, CPU>) -> (%1705:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.16.self_attn.q_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x00000000000438 prog.kernel_launch(%1704:tensor<[1, 192, 1536], Float32, CPU>) -> (%1706:tensor<[1, 192, 256], Float32, CPU>)[symbol_name:model.layers.16.self_attn.k_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x00000000000439 prog.kernel_launch(%1704:tensor<[1, 192, 1536], Float32, CPU>) -> (%1707:tensor<[1, 192, 256], Float32, CPU>)[symbol_name:model.layers.16.self_attn.v_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x0000000000043a prog.free(%1704:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x0000000000043b prog.kernel_launch(%1705:tensor<[1, 192, 1536], Float32, CPU>) -> (%1705:tensor<[1, 192, 12, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,12,128]}]
        addr:0x0000000000043c prog.kernel_launch(%1706:tensor<[1, 192, 256], Float32, CPU>) -> (%1706:tensor<[1, 192, 2, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,2,128]}]
        addr:0x0000000000043d prog.kernel_launch(%1707:tensor<[1, 192, 256], Float32, CPU>) -> (%1707:tensor<[1, 192, 2, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,2,128]}]
        addr:0x0000000000043e prog.kernel_launch(%1705:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%1708:tensor<[1, 12, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x0000000000043f prog.free(%1705:tensor<[1, 192, 12, 128], Float32, CPU>) -> ()
        addr:0x00000000000440 prog.kernel_launch(%1706:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%1709:tensor<[1, 2, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x00000000000441 prog.free(%1706:tensor<[1, 192, 2, 128], Float32, CPU>) -> ()
        addr:0x00000000000442 prog.kernel_launch(%1707:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%1710:tensor<[1, 2, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x00000000000443 prog.free(%1707:tensor<[1, 192, 2, 128], Float32, CPU>) -> ()
        addr:0x00000000000444 prog.kernel_launch(%1708:tensor<[1, 12, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1711:tensor<[1, 12, 192, 128], Float32, CPU>)[symbol_name:model.layers.16.self_attn.q_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x00000000000445 prog.free(%1708:tensor<[1, 12, 192, 128], Float32, CPU>) -> ()
        addr:0x00000000000446 prog.kernel_launch(%1709:tensor<[1, 2, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1712:tensor<[1, 2, 192, 128], Float32, CPU>)[symbol_name:model.layers.16.self_attn.k_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x00000000000447 prog.free(%1709:tensor<[1, 2, 192, 128], Float32, CPU>) -> ()
        addr:0x00000000000448 prog.kernel_launch(%1712:tensor<[1, 2, 192, 128], Float32, CPU>, %1710:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1713:tensor<[1, 2, 192, 128], Float32, CPU>, %1714:tensor<[1, 2, 192, 128], Float32, CPU>)[symbol_name:model.layers.16.self_attn.kv_cache, op_type:KVCache, op_options:{"head_dim":128,"kv_head":2,"layer_idx":0,"q_head":12,"use_fa2":false}]
        addr:0x00000000000449 prog.free(%1710:tensor<[1, 2, 192, 128], Float32, CPU>) -> ()
        addr:0x0000000000044a prog.free(%1712:tensor<[1, 2, 192, 128], Float32, CPU>) -> ()
        addr:0x0000000000044b prog.kernel_launch(%1711:tensor<[1, 12, 192, 128], Float32, CPU>, %1713:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1715:tensor<[1, 12, 192, 192], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":true}]
        addr:0x0000000000044c prog.free(%1713:tensor<[1, 2, 192, 128], Float32, CPU>) -> ()
        addr:0x0000000000044d prog.free(%1711:tensor<[1, 12, 192, 128], Float32, CPU>) -> ()
        addr:0x0000000000044e prog.kernel_launch(%1715:tensor<[1, 12, 192, 192], Float32, CPU>, %1716:tensor<[1], Float32, CPU>) -> (%1717:tensor<[1, 12, 192, 192], Float32, CPU>)[op_type:Mul, op_options:null]
        addr:0x0000000000044f prog.free(%1716:tensor<[1], Float32, CPU>) -> ()
        addr:0x00000000000450 prog.free(%1715:tensor<[1, 12, 192, 192], Float32, CPU>) -> ()
        addr:0x00000000000451 prog.kernel_launch(%1717:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%1718:tensor<[1, 12, 192, 192], Float32, CPU>)[symbol_name:model.layers.16.self_attn.mask, op_type:CausalMask, op_options:{"sliding_window":false,"window_size":0}]
        addr:0x00000000000452 prog.free(%1717:tensor<[1, 12, 192, 192], Float32, CPU>) -> ()
        addr:0x00000000000453 prog.kernel_launch(%1718:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%1719:tensor<[1, 12, 192, 192], Float32, CPU>)[symbol_name:model.layers.16.self_attn.softmax, op_type:Softmax, op_options:{"axis":-1}]
        addr:0x00000000000454 prog.free(%1718:tensor<[1, 12, 192, 192], Float32, CPU>) -> ()
        addr:0x00000000000455 prog.kernel_launch(%1719:tensor<[1, 12, 192, 192], Float32, CPU>, %1714:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1720:tensor<[1, 12, 192, 128], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":false}]
        addr:0x00000000000456 prog.free(%1714:tensor<[1, 2, 192, 128], Float32, CPU>) -> ()
        addr:0x00000000000457 prog.free(%1719:tensor<[1, 12, 192, 192], Float32, CPU>) -> ()
        addr:0x00000000000458 prog.kernel_launch(%1720:tensor<[1, 12, 192, 128], Float32, CPU>) -> (%1721:tensor<[1, 192, 12, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x00000000000459 prog.free(%1720:tensor<[1, 12, 192, 128], Float32, CPU>) -> ()
        addr:0x0000000000045a prog.kernel_launch(%1721:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%1721:tensor<[1, 192, 1536], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,1536]}]
        addr:0x0000000000045b prog.kernel_launch(%1721:tensor<[1, 192, 1536], Float32, CPU>) -> (%1722:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.16.self_attn.o_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x0000000000045c prog.free(%1721:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x0000000000045d prog.ret
        addr:0x0000000000045e prog.label[symbol:model.layers.16.mlp.__entry]
        addr:0x0000000000045f prog.kernel_launch(%1724:tensor<[1, 192, 1536], Float32, CPU>) -> (%1725:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.16.mlp.gate_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x00000000000460 prog.kernel_launch(%1725:tensor<[1, 192, 8960], Float32, CPU>) -> (%1726:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.16.mlp.act, op_type:SiLU, op_options:null]
        addr:0x00000000000461 prog.free(%1725:tensor<[1, 192, 8960], Float32, CPU>) -> ()
        addr:0x00000000000462 prog.kernel_launch(%1724:tensor<[1, 192, 1536], Float32, CPU>) -> (%1727:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.16.mlp.up_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x00000000000463 prog.free(%1724:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x00000000000464 prog.kernel_launch(%1726:tensor<[1, 192, 8960], Float32, CPU>, %1727:tensor<[1, 192, 8960], Float32, CPU>) -> (%1728:tensor<[1, 192, 8960], Float32, CPU>)[op_type:Mul, op_options:null]
        addr:0x00000000000465 prog.free(%1727:tensor<[1, 192, 8960], Float32, CPU>) -> ()
        addr:0x00000000000466 prog.free(%1726:tensor<[1, 192, 8960], Float32, CPU>) -> ()
        addr:0x00000000000467 prog.kernel_launch(%1728:tensor<[1, 192, 8960], Float32, CPU>) -> (%1729:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.16.mlp.down_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":8960,"out_channels":1536}]
        addr:0x00000000000468 prog.free(%1728:tensor<[1, 192, 8960], Float32, CPU>) -> ()
        addr:0x00000000000469 prog.ret
        addr:0x0000000000046a prog.label[symbol:model.layers.17.__entry]
        addr:0x0000000000046b prog.kernel_launch(%1730:tensor<[1, 192, 1536], Float32, CPU>) -> (%1731:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.17.input_layernorm, op_type:RMSNorm, op_options:{"add_unit_offset":false,"epsilon":9.999999974752427e-07}]
        addr:0x0000000000046c prog.jump model.layers.17.self_attn.__entry[offset:10]
        addr:0x0000000000046d prog.kernel_launch(%1749:tensor<[1, 192, 1536], Float32, CPU>, %1730:tensor<[1, 192, 1536], Float32, CPU>) -> (%1750:tensor<[1, 192, 1536], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x0000000000046e prog.free(%1730:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x0000000000046f prog.free(%1749:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x00000000000470 prog.kernel_launch(%1750:tensor<[1, 192, 1536], Float32, CPU>) -> (%1751:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.17.post_attention_layernorm, op_type:RMSNorm, op_options:{"add_unit_offset":false,"epsilon":9.999999974752427e-07}]
        addr:0x00000000000471 prog.jump model.layers.17.mlp.__entry[offset:45]
        addr:0x00000000000472 prog.kernel_launch(%1756:tensor<[1, 192, 1536], Float32, CPU>, %1750:tensor<[1, 192, 1536], Float32, CPU>) -> (%1757:tensor<[1, 192, 1536], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x00000000000473 prog.free(%1750:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x00000000000474 prog.free(%1756:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x00000000000475 prog.ret
        addr:0x00000000000476 prog.label[symbol:model.layers.17.self_attn.__entry]
        addr:0x00000000000477 prog.kernel_launch(%1731:tensor<[1, 192, 1536], Float32, CPU>) -> (%1732:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.17.self_attn.q_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x00000000000478 prog.kernel_launch(%1731:tensor<[1, 192, 1536], Float32, CPU>) -> (%1733:tensor<[1, 192, 256], Float32, CPU>)[symbol_name:model.layers.17.self_attn.k_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x00000000000479 prog.kernel_launch(%1731:tensor<[1, 192, 1536], Float32, CPU>) -> (%1734:tensor<[1, 192, 256], Float32, CPU>)[symbol_name:model.layers.17.self_attn.v_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x0000000000047a prog.free(%1731:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x0000000000047b prog.kernel_launch(%1732:tensor<[1, 192, 1536], Float32, CPU>) -> (%1732:tensor<[1, 192, 12, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,12,128]}]
        addr:0x0000000000047c prog.kernel_launch(%1733:tensor<[1, 192, 256], Float32, CPU>) -> (%1733:tensor<[1, 192, 2, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,2,128]}]
        addr:0x0000000000047d prog.kernel_launch(%1734:tensor<[1, 192, 256], Float32, CPU>) -> (%1734:tensor<[1, 192, 2, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,2,128]}]
        addr:0x0000000000047e prog.kernel_launch(%1732:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%1735:tensor<[1, 12, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x0000000000047f prog.free(%1732:tensor<[1, 192, 12, 128], Float32, CPU>) -> ()
        addr:0x00000000000480 prog.kernel_launch(%1733:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%1736:tensor<[1, 2, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x00000000000481 prog.free(%1733:tensor<[1, 192, 2, 128], Float32, CPU>) -> ()
        addr:0x00000000000482 prog.kernel_launch(%1734:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%1737:tensor<[1, 2, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x00000000000483 prog.free(%1734:tensor<[1, 192, 2, 128], Float32, CPU>) -> ()
        addr:0x00000000000484 prog.kernel_launch(%1735:tensor<[1, 12, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1738:tensor<[1, 12, 192, 128], Float32, CPU>)[symbol_name:model.layers.17.self_attn.q_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x00000000000485 prog.free(%1735:tensor<[1, 12, 192, 128], Float32, CPU>) -> ()
        addr:0x00000000000486 prog.kernel_launch(%1736:tensor<[1, 2, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1739:tensor<[1, 2, 192, 128], Float32, CPU>)[symbol_name:model.layers.17.self_attn.k_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x00000000000487 prog.free(%1736:tensor<[1, 2, 192, 128], Float32, CPU>) -> ()
        addr:0x00000000000488 prog.kernel_launch(%1739:tensor<[1, 2, 192, 128], Float32, CPU>, %1737:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1740:tensor<[1, 2, 192, 128], Float32, CPU>, %1741:tensor<[1, 2, 192, 128], Float32, CPU>)[symbol_name:model.layers.17.self_attn.kv_cache, op_type:KVCache, op_options:{"head_dim":128,"kv_head":2,"layer_idx":0,"q_head":12,"use_fa2":false}]
        addr:0x00000000000489 prog.free(%1737:tensor<[1, 2, 192, 128], Float32, CPU>) -> ()
        addr:0x0000000000048a prog.free(%1739:tensor<[1, 2, 192, 128], Float32, CPU>) -> ()
        addr:0x0000000000048b prog.kernel_launch(%1738:tensor<[1, 12, 192, 128], Float32, CPU>, %1740:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1742:tensor<[1, 12, 192, 192], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":true}]
        addr:0x0000000000048c prog.free(%1740:tensor<[1, 2, 192, 128], Float32, CPU>) -> ()
        addr:0x0000000000048d prog.free(%1738:tensor<[1, 12, 192, 128], Float32, CPU>) -> ()
        addr:0x0000000000048e prog.kernel_launch(%1742:tensor<[1, 12, 192, 192], Float32, CPU>, %1743:tensor<[1], Float32, CPU>) -> (%1744:tensor<[1, 12, 192, 192], Float32, CPU>)[op_type:Mul, op_options:null]
        addr:0x0000000000048f prog.free(%1743:tensor<[1], Float32, CPU>) -> ()
        addr:0x00000000000490 prog.free(%1742:tensor<[1, 12, 192, 192], Float32, CPU>) -> ()
        addr:0x00000000000491 prog.kernel_launch(%1744:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%1745:tensor<[1, 12, 192, 192], Float32, CPU>)[symbol_name:model.layers.17.self_attn.mask, op_type:CausalMask, op_options:{"sliding_window":false,"window_size":0}]
        addr:0x00000000000492 prog.free(%1744:tensor<[1, 12, 192, 192], Float32, CPU>) -> ()
        addr:0x00000000000493 prog.kernel_launch(%1745:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%1746:tensor<[1, 12, 192, 192], Float32, CPU>)[symbol_name:model.layers.17.self_attn.softmax, op_type:Softmax, op_options:{"axis":-1}]
        addr:0x00000000000494 prog.free(%1745:tensor<[1, 12, 192, 192], Float32, CPU>) -> ()
        addr:0x00000000000495 prog.kernel_launch(%1746:tensor<[1, 12, 192, 192], Float32, CPU>, %1741:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1747:tensor<[1, 12, 192, 128], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":false}]
        addr:0x00000000000496 prog.free(%1741:tensor<[1, 2, 192, 128], Float32, CPU>) -> ()
        addr:0x00000000000497 prog.free(%1746:tensor<[1, 12, 192, 192], Float32, CPU>) -> ()
        addr:0x00000000000498 prog.kernel_launch(%1747:tensor<[1, 12, 192, 128], Float32, CPU>) -> (%1748:tensor<[1, 192, 12, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x00000000000499 prog.free(%1747:tensor<[1, 12, 192, 128], Float32, CPU>) -> ()
        addr:0x0000000000049a prog.kernel_launch(%1748:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%1748:tensor<[1, 192, 1536], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,1536]}]
        addr:0x0000000000049b prog.kernel_launch(%1748:tensor<[1, 192, 1536], Float32, CPU>) -> (%1749:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.17.self_attn.o_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x0000000000049c prog.free(%1748:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x0000000000049d prog.ret
        addr:0x0000000000049e prog.label[symbol:model.layers.17.mlp.__entry]
        addr:0x0000000000049f prog.kernel_launch(%1751:tensor<[1, 192, 1536], Float32, CPU>) -> (%1752:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.17.mlp.gate_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x000000000004a0 prog.kernel_launch(%1752:tensor<[1, 192, 8960], Float32, CPU>) -> (%1753:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.17.mlp.act, op_type:SiLU, op_options:null]
        addr:0x000000000004a1 prog.free(%1752:tensor<[1, 192, 8960], Float32, CPU>) -> ()
        addr:0x000000000004a2 prog.kernel_launch(%1751:tensor<[1, 192, 1536], Float32, CPU>) -> (%1754:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.17.mlp.up_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x000000000004a3 prog.free(%1751:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x000000000004a4 prog.kernel_launch(%1753:tensor<[1, 192, 8960], Float32, CPU>, %1754:tensor<[1, 192, 8960], Float32, CPU>) -> (%1755:tensor<[1, 192, 8960], Float32, CPU>)[op_type:Mul, op_options:null]
        addr:0x000000000004a5 prog.free(%1754:tensor<[1, 192, 8960], Float32, CPU>) -> ()
        addr:0x000000000004a6 prog.free(%1753:tensor<[1, 192, 8960], Float32, CPU>) -> ()
        addr:0x000000000004a7 prog.kernel_launch(%1755:tensor<[1, 192, 8960], Float32, CPU>) -> (%1756:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.17.mlp.down_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":8960,"out_channels":1536}]
        addr:0x000000000004a8 prog.free(%1755:tensor<[1, 192, 8960], Float32, CPU>) -> ()
        addr:0x000000000004a9 prog.ret
        addr:0x000000000004aa prog.label[symbol:model.layers.18.__entry]
        addr:0x000000000004ab prog.kernel_launch(%1757:tensor<[1, 192, 1536], Float32, CPU>) -> (%1758:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.18.input_layernorm, op_type:RMSNorm, op_options:{"add_unit_offset":false,"epsilon":9.999999974752427e-07}]
        addr:0x000000000004ac prog.jump model.layers.18.self_attn.__entry[offset:10]
        addr:0x000000000004ad prog.kernel_launch(%1776:tensor<[1, 192, 1536], Float32, CPU>, %1757:tensor<[1, 192, 1536], Float32, CPU>) -> (%1777:tensor<[1, 192, 1536], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x000000000004ae prog.free(%1757:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x000000000004af prog.free(%1776:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x000000000004b0 prog.kernel_launch(%1777:tensor<[1, 192, 1536], Float32, CPU>) -> (%1778:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.18.post_attention_layernorm, op_type:RMSNorm, op_options:{"add_unit_offset":false,"epsilon":9.999999974752427e-07}]
        addr:0x000000000004b1 prog.jump model.layers.18.mlp.__entry[offset:45]
        addr:0x000000000004b2 prog.kernel_launch(%1783:tensor<[1, 192, 1536], Float32, CPU>, %1777:tensor<[1, 192, 1536], Float32, CPU>) -> (%1784:tensor<[1, 192, 1536], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x000000000004b3 prog.free(%1777:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x000000000004b4 prog.free(%1783:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x000000000004b5 prog.ret
        addr:0x000000000004b6 prog.label[symbol:model.layers.18.self_attn.__entry]
        addr:0x000000000004b7 prog.kernel_launch(%1758:tensor<[1, 192, 1536], Float32, CPU>) -> (%1759:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.18.self_attn.q_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x000000000004b8 prog.kernel_launch(%1758:tensor<[1, 192, 1536], Float32, CPU>) -> (%1760:tensor<[1, 192, 256], Float32, CPU>)[symbol_name:model.layers.18.self_attn.k_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x000000000004b9 prog.kernel_launch(%1758:tensor<[1, 192, 1536], Float32, CPU>) -> (%1761:tensor<[1, 192, 256], Float32, CPU>)[symbol_name:model.layers.18.self_attn.v_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x000000000004ba prog.free(%1758:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x000000000004bb prog.kernel_launch(%1759:tensor<[1, 192, 1536], Float32, CPU>) -> (%1759:tensor<[1, 192, 12, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,12,128]}]
        addr:0x000000000004bc prog.kernel_launch(%1760:tensor<[1, 192, 256], Float32, CPU>) -> (%1760:tensor<[1, 192, 2, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,2,128]}]
        addr:0x000000000004bd prog.kernel_launch(%1761:tensor<[1, 192, 256], Float32, CPU>) -> (%1761:tensor<[1, 192, 2, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,2,128]}]
        addr:0x000000000004be prog.kernel_launch(%1759:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%1762:tensor<[1, 12, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x000000000004bf prog.free(%1759:tensor<[1, 192, 12, 128], Float32, CPU>) -> ()
        addr:0x000000000004c0 prog.kernel_launch(%1760:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%1763:tensor<[1, 2, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x000000000004c1 prog.free(%1760:tensor<[1, 192, 2, 128], Float32, CPU>) -> ()
        addr:0x000000000004c2 prog.kernel_launch(%1761:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%1764:tensor<[1, 2, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x000000000004c3 prog.free(%1761:tensor<[1, 192, 2, 128], Float32, CPU>) -> ()
        addr:0x000000000004c4 prog.kernel_launch(%1762:tensor<[1, 12, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1765:tensor<[1, 12, 192, 128], Float32, CPU>)[symbol_name:model.layers.18.self_attn.q_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x000000000004c5 prog.free(%1762:tensor<[1, 12, 192, 128], Float32, CPU>) -> ()
        addr:0x000000000004c6 prog.kernel_launch(%1763:tensor<[1, 2, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1766:tensor<[1, 2, 192, 128], Float32, CPU>)[symbol_name:model.layers.18.self_attn.k_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x000000000004c7 prog.free(%1763:tensor<[1, 2, 192, 128], Float32, CPU>) -> ()
        addr:0x000000000004c8 prog.kernel_launch(%1766:tensor<[1, 2, 192, 128], Float32, CPU>, %1764:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1767:tensor<[1, 2, 192, 128], Float32, CPU>, %1768:tensor<[1, 2, 192, 128], Float32, CPU>)[symbol_name:model.layers.18.self_attn.kv_cache, op_type:KVCache, op_options:{"head_dim":128,"kv_head":2,"layer_idx":0,"q_head":12,"use_fa2":false}]
        addr:0x000000000004c9 prog.free(%1764:tensor<[1, 2, 192, 128], Float32, CPU>) -> ()
        addr:0x000000000004ca prog.free(%1766:tensor<[1, 2, 192, 128], Float32, CPU>) -> ()
        addr:0x000000000004cb prog.kernel_launch(%1765:tensor<[1, 12, 192, 128], Float32, CPU>, %1767:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1769:tensor<[1, 12, 192, 192], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":true}]
        addr:0x000000000004cc prog.free(%1767:tensor<[1, 2, 192, 128], Float32, CPU>) -> ()
        addr:0x000000000004cd prog.free(%1765:tensor<[1, 12, 192, 128], Float32, CPU>) -> ()
        addr:0x000000000004ce prog.kernel_launch(%1769:tensor<[1, 12, 192, 192], Float32, CPU>, %1770:tensor<[1], Float32, CPU>) -> (%1771:tensor<[1, 12, 192, 192], Float32, CPU>)[op_type:Mul, op_options:null]
        addr:0x000000000004cf prog.free(%1770:tensor<[1], Float32, CPU>) -> ()
        addr:0x000000000004d0 prog.free(%1769:tensor<[1, 12, 192, 192], Float32, CPU>) -> ()
        addr:0x000000000004d1 prog.kernel_launch(%1771:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%1772:tensor<[1, 12, 192, 192], Float32, CPU>)[symbol_name:model.layers.18.self_attn.mask, op_type:CausalMask, op_options:{"sliding_window":false,"window_size":0}]
        addr:0x000000000004d2 prog.free(%1771:tensor<[1, 12, 192, 192], Float32, CPU>) -> ()
        addr:0x000000000004d3 prog.kernel_launch(%1772:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%1773:tensor<[1, 12, 192, 192], Float32, CPU>)[symbol_name:model.layers.18.self_attn.softmax, op_type:Softmax, op_options:{"axis":-1}]
        addr:0x000000000004d4 prog.free(%1772:tensor<[1, 12, 192, 192], Float32, CPU>) -> ()
        addr:0x000000000004d5 prog.kernel_launch(%1773:tensor<[1, 12, 192, 192], Float32, CPU>, %1768:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1774:tensor<[1, 12, 192, 128], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":false}]
        addr:0x000000000004d6 prog.free(%1768:tensor<[1, 2, 192, 128], Float32, CPU>) -> ()
        addr:0x000000000004d7 prog.free(%1773:tensor<[1, 12, 192, 192], Float32, CPU>) -> ()
        addr:0x000000000004d8 prog.kernel_launch(%1774:tensor<[1, 12, 192, 128], Float32, CPU>) -> (%1775:tensor<[1, 192, 12, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x000000000004d9 prog.free(%1774:tensor<[1, 12, 192, 128], Float32, CPU>) -> ()
        addr:0x000000000004da prog.kernel_launch(%1775:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%1775:tensor<[1, 192, 1536], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,1536]}]
        addr:0x000000000004db prog.kernel_launch(%1775:tensor<[1, 192, 1536], Float32, CPU>) -> (%1776:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.18.self_attn.o_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x000000000004dc prog.free(%1775:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x000000000004dd prog.ret
        addr:0x000000000004de prog.label[symbol:model.layers.18.mlp.__entry]
        addr:0x000000000004df prog.kernel_launch(%1778:tensor<[1, 192, 1536], Float32, CPU>) -> (%1779:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.18.mlp.gate_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x000000000004e0 prog.kernel_launch(%1779:tensor<[1, 192, 8960], Float32, CPU>) -> (%1780:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.18.mlp.act, op_type:SiLU, op_options:null]
        addr:0x000000000004e1 prog.free(%1779:tensor<[1, 192, 8960], Float32, CPU>) -> ()
        addr:0x000000000004e2 prog.kernel_launch(%1778:tensor<[1, 192, 1536], Float32, CPU>) -> (%1781:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.18.mlp.up_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x000000000004e3 prog.free(%1778:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x000000000004e4 prog.kernel_launch(%1780:tensor<[1, 192, 8960], Float32, CPU>, %1781:tensor<[1, 192, 8960], Float32, CPU>) -> (%1782:tensor<[1, 192, 8960], Float32, CPU>)[op_type:Mul, op_options:null]
        addr:0x000000000004e5 prog.free(%1781:tensor<[1, 192, 8960], Float32, CPU>) -> ()
        addr:0x000000000004e6 prog.free(%1780:tensor<[1, 192, 8960], Float32, CPU>) -> ()
        addr:0x000000000004e7 prog.kernel_launch(%1782:tensor<[1, 192, 8960], Float32, CPU>) -> (%1783:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.18.mlp.down_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":8960,"out_channels":1536}]
        addr:0x000000000004e8 prog.free(%1782:tensor<[1, 192, 8960], Float32, CPU>) -> ()
        addr:0x000000000004e9 prog.ret
        addr:0x000000000004ea prog.label[symbol:model.layers.19.__entry]
        addr:0x000000000004eb prog.kernel_launch(%1784:tensor<[1, 192, 1536], Float32, CPU>) -> (%1785:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.19.input_layernorm, op_type:RMSNorm, op_options:{"add_unit_offset":false,"epsilon":9.999999974752427e-07}]
        addr:0x000000000004ec prog.jump model.layers.19.self_attn.__entry[offset:10]
        addr:0x000000000004ed prog.kernel_launch(%1803:tensor<[1, 192, 1536], Float32, CPU>, %1784:tensor<[1, 192, 1536], Float32, CPU>) -> (%1804:tensor<[1, 192, 1536], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x000000000004ee prog.free(%1784:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x000000000004ef prog.free(%1803:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x000000000004f0 prog.kernel_launch(%1804:tensor<[1, 192, 1536], Float32, CPU>) -> (%1805:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.19.post_attention_layernorm, op_type:RMSNorm, op_options:{"add_unit_offset":false,"epsilon":9.999999974752427e-07}]
        addr:0x000000000004f1 prog.jump model.layers.19.mlp.__entry[offset:45]
        addr:0x000000000004f2 prog.kernel_launch(%1810:tensor<[1, 192, 1536], Float32, CPU>, %1804:tensor<[1, 192, 1536], Float32, CPU>) -> (%1811:tensor<[1, 192, 1536], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x000000000004f3 prog.free(%1804:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x000000000004f4 prog.free(%1810:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x000000000004f5 prog.ret
        addr:0x000000000004f6 prog.label[symbol:model.layers.19.self_attn.__entry]
        addr:0x000000000004f7 prog.kernel_launch(%1785:tensor<[1, 192, 1536], Float32, CPU>) -> (%1786:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.19.self_attn.q_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x000000000004f8 prog.kernel_launch(%1785:tensor<[1, 192, 1536], Float32, CPU>) -> (%1787:tensor<[1, 192, 256], Float32, CPU>)[symbol_name:model.layers.19.self_attn.k_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x000000000004f9 prog.kernel_launch(%1785:tensor<[1, 192, 1536], Float32, CPU>) -> (%1788:tensor<[1, 192, 256], Float32, CPU>)[symbol_name:model.layers.19.self_attn.v_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x000000000004fa prog.free(%1785:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x000000000004fb prog.kernel_launch(%1786:tensor<[1, 192, 1536], Float32, CPU>) -> (%1786:tensor<[1, 192, 12, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,12,128]}]
        addr:0x000000000004fc prog.kernel_launch(%1787:tensor<[1, 192, 256], Float32, CPU>) -> (%1787:tensor<[1, 192, 2, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,2,128]}]
        addr:0x000000000004fd prog.kernel_launch(%1788:tensor<[1, 192, 256], Float32, CPU>) -> (%1788:tensor<[1, 192, 2, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,2,128]}]
        addr:0x000000000004fe prog.kernel_launch(%1786:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%1789:tensor<[1, 12, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x000000000004ff prog.free(%1786:tensor<[1, 192, 12, 128], Float32, CPU>) -> ()
        addr:0x00000000000500 prog.kernel_launch(%1787:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%1790:tensor<[1, 2, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x00000000000501 prog.free(%1787:tensor<[1, 192, 2, 128], Float32, CPU>) -> ()
        addr:0x00000000000502 prog.kernel_launch(%1788:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%1791:tensor<[1, 2, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x00000000000503 prog.free(%1788:tensor<[1, 192, 2, 128], Float32, CPU>) -> ()
        addr:0x00000000000504 prog.kernel_launch(%1789:tensor<[1, 12, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1792:tensor<[1, 12, 192, 128], Float32, CPU>)[symbol_name:model.layers.19.self_attn.q_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x00000000000505 prog.free(%1789:tensor<[1, 12, 192, 128], Float32, CPU>) -> ()
        addr:0x00000000000506 prog.kernel_launch(%1790:tensor<[1, 2, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1793:tensor<[1, 2, 192, 128], Float32, CPU>)[symbol_name:model.layers.19.self_attn.k_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x00000000000507 prog.free(%1790:tensor<[1, 2, 192, 128], Float32, CPU>) -> ()
        addr:0x00000000000508 prog.kernel_launch(%1793:tensor<[1, 2, 192, 128], Float32, CPU>, %1791:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1794:tensor<[1, 2, 192, 128], Float32, CPU>, %1795:tensor<[1, 2, 192, 128], Float32, CPU>)[symbol_name:model.layers.19.self_attn.kv_cache, op_type:KVCache, op_options:{"head_dim":128,"kv_head":2,"layer_idx":0,"q_head":12,"use_fa2":false}]
        addr:0x00000000000509 prog.free(%1791:tensor<[1, 2, 192, 128], Float32, CPU>) -> ()
        addr:0x0000000000050a prog.free(%1793:tensor<[1, 2, 192, 128], Float32, CPU>) -> ()
        addr:0x0000000000050b prog.kernel_launch(%1792:tensor<[1, 12, 192, 128], Float32, CPU>, %1794:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1796:tensor<[1, 12, 192, 192], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":true}]
        addr:0x0000000000050c prog.free(%1794:tensor<[1, 2, 192, 128], Float32, CPU>) -> ()
        addr:0x0000000000050d prog.free(%1792:tensor<[1, 12, 192, 128], Float32, CPU>) -> ()
        addr:0x0000000000050e prog.kernel_launch(%1796:tensor<[1, 12, 192, 192], Float32, CPU>, %1797:tensor<[1], Float32, CPU>) -> (%1798:tensor<[1, 12, 192, 192], Float32, CPU>)[op_type:Mul, op_options:null]
        addr:0x0000000000050f prog.free(%1797:tensor<[1], Float32, CPU>) -> ()
        addr:0x00000000000510 prog.free(%1796:tensor<[1, 12, 192, 192], Float32, CPU>) -> ()
        addr:0x00000000000511 prog.kernel_launch(%1798:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%1799:tensor<[1, 12, 192, 192], Float32, CPU>)[symbol_name:model.layers.19.self_attn.mask, op_type:CausalMask, op_options:{"sliding_window":false,"window_size":0}]
        addr:0x00000000000512 prog.free(%1798:tensor<[1, 12, 192, 192], Float32, CPU>) -> ()
        addr:0x00000000000513 prog.kernel_launch(%1799:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%1800:tensor<[1, 12, 192, 192], Float32, CPU>)[symbol_name:model.layers.19.self_attn.softmax, op_type:Softmax, op_options:{"axis":-1}]
        addr:0x00000000000514 prog.free(%1799:tensor<[1, 12, 192, 192], Float32, CPU>) -> ()
        addr:0x00000000000515 prog.kernel_launch(%1800:tensor<[1, 12, 192, 192], Float32, CPU>, %1795:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1801:tensor<[1, 12, 192, 128], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":false}]
        addr:0x00000000000516 prog.free(%1795:tensor<[1, 2, 192, 128], Float32, CPU>) -> ()
        addr:0x00000000000517 prog.free(%1800:tensor<[1, 12, 192, 192], Float32, CPU>) -> ()
        addr:0x00000000000518 prog.kernel_launch(%1801:tensor<[1, 12, 192, 128], Float32, CPU>) -> (%1802:tensor<[1, 192, 12, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x00000000000519 prog.free(%1801:tensor<[1, 12, 192, 128], Float32, CPU>) -> ()
        addr:0x0000000000051a prog.kernel_launch(%1802:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%1802:tensor<[1, 192, 1536], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,1536]}]
        addr:0x0000000000051b prog.kernel_launch(%1802:tensor<[1, 192, 1536], Float32, CPU>) -> (%1803:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.19.self_attn.o_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x0000000000051c prog.free(%1802:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x0000000000051d prog.ret
        addr:0x0000000000051e prog.label[symbol:model.layers.19.mlp.__entry]
        addr:0x0000000000051f prog.kernel_launch(%1805:tensor<[1, 192, 1536], Float32, CPU>) -> (%1806:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.19.mlp.gate_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x00000000000520 prog.kernel_launch(%1806:tensor<[1, 192, 8960], Float32, CPU>) -> (%1807:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.19.mlp.act, op_type:SiLU, op_options:null]
        addr:0x00000000000521 prog.free(%1806:tensor<[1, 192, 8960], Float32, CPU>) -> ()
        addr:0x00000000000522 prog.kernel_launch(%1805:tensor<[1, 192, 1536], Float32, CPU>) -> (%1808:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.19.mlp.up_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x00000000000523 prog.free(%1805:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x00000000000524 prog.kernel_launch(%1807:tensor<[1, 192, 8960], Float32, CPU>, %1808:tensor<[1, 192, 8960], Float32, CPU>) -> (%1809:tensor<[1, 192, 8960], Float32, CPU>)[op_type:Mul, op_options:null]
        addr:0x00000000000525 prog.free(%1808:tensor<[1, 192, 8960], Float32, CPU>) -> ()
        addr:0x00000000000526 prog.free(%1807:tensor<[1, 192, 8960], Float32, CPU>) -> ()
        addr:0x00000000000527 prog.kernel_launch(%1809:tensor<[1, 192, 8960], Float32, CPU>) -> (%1810:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.19.mlp.down_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":8960,"out_channels":1536}]
        addr:0x00000000000528 prog.free(%1809:tensor<[1, 192, 8960], Float32, CPU>) -> ()
        addr:0x00000000000529 prog.ret
        addr:0x0000000000052a prog.label[symbol:model.layers.20.__entry]
        addr:0x0000000000052b prog.kernel_launch(%1811:tensor<[1, 192, 1536], Float32, CPU>) -> (%1812:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.20.input_layernorm, op_type:RMSNorm, op_options:{"add_unit_offset":false,"epsilon":9.999999974752427e-07}]
        addr:0x0000000000052c prog.jump model.layers.20.self_attn.__entry[offset:10]
        addr:0x0000000000052d prog.kernel_launch(%1830:tensor<[1, 192, 1536], Float32, CPU>, %1811:tensor<[1, 192, 1536], Float32, CPU>) -> (%1831:tensor<[1, 192, 1536], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x0000000000052e prog.free(%1811:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x0000000000052f prog.free(%1830:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x00000000000530 prog.kernel_launch(%1831:tensor<[1, 192, 1536], Float32, CPU>) -> (%1832:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.20.post_attention_layernorm, op_type:RMSNorm, op_options:{"add_unit_offset":false,"epsilon":9.999999974752427e-07}]
        addr:0x00000000000531 prog.jump model.layers.20.mlp.__entry[offset:45]
        addr:0x00000000000532 prog.kernel_launch(%1837:tensor<[1, 192, 1536], Float32, CPU>, %1831:tensor<[1, 192, 1536], Float32, CPU>) -> (%1838:tensor<[1, 192, 1536], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x00000000000533 prog.free(%1831:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x00000000000534 prog.free(%1837:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x00000000000535 prog.ret
        addr:0x00000000000536 prog.label[symbol:model.layers.20.self_attn.__entry]
        addr:0x00000000000537 prog.kernel_launch(%1812:tensor<[1, 192, 1536], Float32, CPU>) -> (%1813:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.20.self_attn.q_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x00000000000538 prog.kernel_launch(%1812:tensor<[1, 192, 1536], Float32, CPU>) -> (%1814:tensor<[1, 192, 256], Float32, CPU>)[symbol_name:model.layers.20.self_attn.k_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x00000000000539 prog.kernel_launch(%1812:tensor<[1, 192, 1536], Float32, CPU>) -> (%1815:tensor<[1, 192, 256], Float32, CPU>)[symbol_name:model.layers.20.self_attn.v_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x0000000000053a prog.free(%1812:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x0000000000053b prog.kernel_launch(%1813:tensor<[1, 192, 1536], Float32, CPU>) -> (%1813:tensor<[1, 192, 12, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,12,128]}]
        addr:0x0000000000053c prog.kernel_launch(%1814:tensor<[1, 192, 256], Float32, CPU>) -> (%1814:tensor<[1, 192, 2, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,2,128]}]
        addr:0x0000000000053d prog.kernel_launch(%1815:tensor<[1, 192, 256], Float32, CPU>) -> (%1815:tensor<[1, 192, 2, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,2,128]}]
        addr:0x0000000000053e prog.kernel_launch(%1813:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%1816:tensor<[1, 12, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x0000000000053f prog.free(%1813:tensor<[1, 192, 12, 128], Float32, CPU>) -> ()
        addr:0x00000000000540 prog.kernel_launch(%1814:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%1817:tensor<[1, 2, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x00000000000541 prog.free(%1814:tensor<[1, 192, 2, 128], Float32, CPU>) -> ()
        addr:0x00000000000542 prog.kernel_launch(%1815:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%1818:tensor<[1, 2, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x00000000000543 prog.free(%1815:tensor<[1, 192, 2, 128], Float32, CPU>) -> ()
        addr:0x00000000000544 prog.kernel_launch(%1816:tensor<[1, 12, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1819:tensor<[1, 12, 192, 128], Float32, CPU>)[symbol_name:model.layers.20.self_attn.q_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x00000000000545 prog.free(%1816:tensor<[1, 12, 192, 128], Float32, CPU>) -> ()
        addr:0x00000000000546 prog.kernel_launch(%1817:tensor<[1, 2, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1820:tensor<[1, 2, 192, 128], Float32, CPU>)[symbol_name:model.layers.20.self_attn.k_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x00000000000547 prog.free(%1817:tensor<[1, 2, 192, 128], Float32, CPU>) -> ()
        addr:0x00000000000548 prog.kernel_launch(%1820:tensor<[1, 2, 192, 128], Float32, CPU>, %1818:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1821:tensor<[1, 2, 192, 128], Float32, CPU>, %1822:tensor<[1, 2, 192, 128], Float32, CPU>)[symbol_name:model.layers.20.self_attn.kv_cache, op_type:KVCache, op_options:{"head_dim":128,"kv_head":2,"layer_idx":0,"q_head":12,"use_fa2":false}]
        addr:0x00000000000549 prog.free(%1818:tensor<[1, 2, 192, 128], Float32, CPU>) -> ()
        addr:0x0000000000054a prog.free(%1820:tensor<[1, 2, 192, 128], Float32, CPU>) -> ()
        addr:0x0000000000054b prog.kernel_launch(%1819:tensor<[1, 12, 192, 128], Float32, CPU>, %1821:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1823:tensor<[1, 12, 192, 192], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":true}]
        addr:0x0000000000054c prog.free(%1821:tensor<[1, 2, 192, 128], Float32, CPU>) -> ()
        addr:0x0000000000054d prog.free(%1819:tensor<[1, 12, 192, 128], Float32, CPU>) -> ()
        addr:0x0000000000054e prog.kernel_launch(%1823:tensor<[1, 12, 192, 192], Float32, CPU>, %1824:tensor<[1], Float32, CPU>) -> (%1825:tensor<[1, 12, 192, 192], Float32, CPU>)[op_type:Mul, op_options:null]
        addr:0x0000000000054f prog.free(%1824:tensor<[1], Float32, CPU>) -> ()
        addr:0x00000000000550 prog.free(%1823:tensor<[1, 12, 192, 192], Float32, CPU>) -> ()
        addr:0x00000000000551 prog.kernel_launch(%1825:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%1826:tensor<[1, 12, 192, 192], Float32, CPU>)[symbol_name:model.layers.20.self_attn.mask, op_type:CausalMask, op_options:{"sliding_window":false,"window_size":0}]
        addr:0x00000000000552 prog.free(%1825:tensor<[1, 12, 192, 192], Float32, CPU>) -> ()
        addr:0x00000000000553 prog.kernel_launch(%1826:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%1827:tensor<[1, 12, 192, 192], Float32, CPU>)[symbol_name:model.layers.20.self_attn.softmax, op_type:Softmax, op_options:{"axis":-1}]
        addr:0x00000000000554 prog.free(%1826:tensor<[1, 12, 192, 192], Float32, CPU>) -> ()
        addr:0x00000000000555 prog.kernel_launch(%1827:tensor<[1, 12, 192, 192], Float32, CPU>, %1822:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1828:tensor<[1, 12, 192, 128], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":false}]
        addr:0x00000000000556 prog.free(%1822:tensor<[1, 2, 192, 128], Float32, CPU>) -> ()
        addr:0x00000000000557 prog.free(%1827:tensor<[1, 12, 192, 192], Float32, CPU>) -> ()
        addr:0x00000000000558 prog.kernel_launch(%1828:tensor<[1, 12, 192, 128], Float32, CPU>) -> (%1829:tensor<[1, 192, 12, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x00000000000559 prog.free(%1828:tensor<[1, 12, 192, 128], Float32, CPU>) -> ()
        addr:0x0000000000055a prog.kernel_launch(%1829:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%1829:tensor<[1, 192, 1536], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,1536]}]
        addr:0x0000000000055b prog.kernel_launch(%1829:tensor<[1, 192, 1536], Float32, CPU>) -> (%1830:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.20.self_attn.o_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x0000000000055c prog.free(%1829:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x0000000000055d prog.ret
        addr:0x0000000000055e prog.label[symbol:model.layers.20.mlp.__entry]
        addr:0x0000000000055f prog.kernel_launch(%1832:tensor<[1, 192, 1536], Float32, CPU>) -> (%1833:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.20.mlp.gate_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x00000000000560 prog.kernel_launch(%1833:tensor<[1, 192, 8960], Float32, CPU>) -> (%1834:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.20.mlp.act, op_type:SiLU, op_options:null]
        addr:0x00000000000561 prog.free(%1833:tensor<[1, 192, 8960], Float32, CPU>) -> ()
        addr:0x00000000000562 prog.kernel_launch(%1832:tensor<[1, 192, 1536], Float32, CPU>) -> (%1835:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.20.mlp.up_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x00000000000563 prog.free(%1832:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x00000000000564 prog.kernel_launch(%1834:tensor<[1, 192, 8960], Float32, CPU>, %1835:tensor<[1, 192, 8960], Float32, CPU>) -> (%1836:tensor<[1, 192, 8960], Float32, CPU>)[op_type:Mul, op_options:null]
        addr:0x00000000000565 prog.free(%1835:tensor<[1, 192, 8960], Float32, CPU>) -> ()
        addr:0x00000000000566 prog.free(%1834:tensor<[1, 192, 8960], Float32, CPU>) -> ()
        addr:0x00000000000567 prog.kernel_launch(%1836:tensor<[1, 192, 8960], Float32, CPU>) -> (%1837:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.20.mlp.down_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":8960,"out_channels":1536}]
        addr:0x00000000000568 prog.free(%1836:tensor<[1, 192, 8960], Float32, CPU>) -> ()
        addr:0x00000000000569 prog.ret
        addr:0x0000000000056a prog.label[symbol:model.layers.21.__entry]
        addr:0x0000000000056b prog.kernel_launch(%1838:tensor<[1, 192, 1536], Float32, CPU>) -> (%1839:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.21.input_layernorm, op_type:RMSNorm, op_options:{"add_unit_offset":false,"epsilon":9.999999974752427e-07}]
        addr:0x0000000000056c prog.jump model.layers.21.self_attn.__entry[offset:10]
        addr:0x0000000000056d prog.kernel_launch(%1857:tensor<[1, 192, 1536], Float32, CPU>, %1838:tensor<[1, 192, 1536], Float32, CPU>) -> (%1858:tensor<[1, 192, 1536], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x0000000000056e prog.free(%1838:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x0000000000056f prog.free(%1857:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x00000000000570 prog.kernel_launch(%1858:tensor<[1, 192, 1536], Float32, CPU>) -> (%1859:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.21.post_attention_layernorm, op_type:RMSNorm, op_options:{"add_unit_offset":false,"epsilon":9.999999974752427e-07}]
        addr:0x00000000000571 prog.jump model.layers.21.mlp.__entry[offset:45]
        addr:0x00000000000572 prog.kernel_launch(%1864:tensor<[1, 192, 1536], Float32, CPU>, %1858:tensor<[1, 192, 1536], Float32, CPU>) -> (%1865:tensor<[1, 192, 1536], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x00000000000573 prog.free(%1858:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x00000000000574 prog.free(%1864:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x00000000000575 prog.ret
        addr:0x00000000000576 prog.label[symbol:model.layers.21.self_attn.__entry]
        addr:0x00000000000577 prog.kernel_launch(%1839:tensor<[1, 192, 1536], Float32, CPU>) -> (%1840:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.21.self_attn.q_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x00000000000578 prog.kernel_launch(%1839:tensor<[1, 192, 1536], Float32, CPU>) -> (%1841:tensor<[1, 192, 256], Float32, CPU>)[symbol_name:model.layers.21.self_attn.k_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x00000000000579 prog.kernel_launch(%1839:tensor<[1, 192, 1536], Float32, CPU>) -> (%1842:tensor<[1, 192, 256], Float32, CPU>)[symbol_name:model.layers.21.self_attn.v_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x0000000000057a prog.free(%1839:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x0000000000057b prog.kernel_launch(%1840:tensor<[1, 192, 1536], Float32, CPU>) -> (%1840:tensor<[1, 192, 12, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,12,128]}]
        addr:0x0000000000057c prog.kernel_launch(%1841:tensor<[1, 192, 256], Float32, CPU>) -> (%1841:tensor<[1, 192, 2, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,2,128]}]
        addr:0x0000000000057d prog.kernel_launch(%1842:tensor<[1, 192, 256], Float32, CPU>) -> (%1842:tensor<[1, 192, 2, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,2,128]}]
        addr:0x0000000000057e prog.kernel_launch(%1840:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%1843:tensor<[1, 12, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x0000000000057f prog.free(%1840:tensor<[1, 192, 12, 128], Float32, CPU>) -> ()
        addr:0x00000000000580 prog.kernel_launch(%1841:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%1844:tensor<[1, 2, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x00000000000581 prog.free(%1841:tensor<[1, 192, 2, 128], Float32, CPU>) -> ()
        addr:0x00000000000582 prog.kernel_launch(%1842:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%1845:tensor<[1, 2, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x00000000000583 prog.free(%1842:tensor<[1, 192, 2, 128], Float32, CPU>) -> ()
        addr:0x00000000000584 prog.kernel_launch(%1843:tensor<[1, 12, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1846:tensor<[1, 12, 192, 128], Float32, CPU>)[symbol_name:model.layers.21.self_attn.q_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x00000000000585 prog.free(%1843:tensor<[1, 12, 192, 128], Float32, CPU>) -> ()
        addr:0x00000000000586 prog.kernel_launch(%1844:tensor<[1, 2, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1847:tensor<[1, 2, 192, 128], Float32, CPU>)[symbol_name:model.layers.21.self_attn.k_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x00000000000587 prog.free(%1844:tensor<[1, 2, 192, 128], Float32, CPU>) -> ()
        addr:0x00000000000588 prog.kernel_launch(%1847:tensor<[1, 2, 192, 128], Float32, CPU>, %1845:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1848:tensor<[1, 2, 192, 128], Float32, CPU>, %1849:tensor<[1, 2, 192, 128], Float32, CPU>)[symbol_name:model.layers.21.self_attn.kv_cache, op_type:KVCache, op_options:{"head_dim":128,"kv_head":2,"layer_idx":0,"q_head":12,"use_fa2":false}]
        addr:0x00000000000589 prog.free(%1845:tensor<[1, 2, 192, 128], Float32, CPU>) -> ()
        addr:0x0000000000058a prog.free(%1847:tensor<[1, 2, 192, 128], Float32, CPU>) -> ()
        addr:0x0000000000058b prog.kernel_launch(%1846:tensor<[1, 12, 192, 128], Float32, CPU>, %1848:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1850:tensor<[1, 12, 192, 192], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":true}]
        addr:0x0000000000058c prog.free(%1848:tensor<[1, 2, 192, 128], Float32, CPU>) -> ()
        addr:0x0000000000058d prog.free(%1846:tensor<[1, 12, 192, 128], Float32, CPU>) -> ()
        addr:0x0000000000058e prog.kernel_launch(%1850:tensor<[1, 12, 192, 192], Float32, CPU>, %1851:tensor<[1], Float32, CPU>) -> (%1852:tensor<[1, 12, 192, 192], Float32, CPU>)[op_type:Mul, op_options:null]
        addr:0x0000000000058f prog.free(%1851:tensor<[1], Float32, CPU>) -> ()
        addr:0x00000000000590 prog.free(%1850:tensor<[1, 12, 192, 192], Float32, CPU>) -> ()
        addr:0x00000000000591 prog.kernel_launch(%1852:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%1853:tensor<[1, 12, 192, 192], Float32, CPU>)[symbol_name:model.layers.21.self_attn.mask, op_type:CausalMask, op_options:{"sliding_window":false,"window_size":0}]
        addr:0x00000000000592 prog.free(%1852:tensor<[1, 12, 192, 192], Float32, CPU>) -> ()
        addr:0x00000000000593 prog.kernel_launch(%1853:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%1854:tensor<[1, 12, 192, 192], Float32, CPU>)[symbol_name:model.layers.21.self_attn.softmax, op_type:Softmax, op_options:{"axis":-1}]
        addr:0x00000000000594 prog.free(%1853:tensor<[1, 12, 192, 192], Float32, CPU>) -> ()
        addr:0x00000000000595 prog.kernel_launch(%1854:tensor<[1, 12, 192, 192], Float32, CPU>, %1849:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1855:tensor<[1, 12, 192, 128], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":false}]
        addr:0x00000000000596 prog.free(%1849:tensor<[1, 2, 192, 128], Float32, CPU>) -> ()
        addr:0x00000000000597 prog.free(%1854:tensor<[1, 12, 192, 192], Float32, CPU>) -> ()
        addr:0x00000000000598 prog.kernel_launch(%1855:tensor<[1, 12, 192, 128], Float32, CPU>) -> (%1856:tensor<[1, 192, 12, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x00000000000599 prog.free(%1855:tensor<[1, 12, 192, 128], Float32, CPU>) -> ()
        addr:0x0000000000059a prog.kernel_launch(%1856:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%1856:tensor<[1, 192, 1536], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,1536]}]
        addr:0x0000000000059b prog.kernel_launch(%1856:tensor<[1, 192, 1536], Float32, CPU>) -> (%1857:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.21.self_attn.o_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x0000000000059c prog.free(%1856:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x0000000000059d prog.ret
        addr:0x0000000000059e prog.label[symbol:model.layers.21.mlp.__entry]
        addr:0x0000000000059f prog.kernel_launch(%1859:tensor<[1, 192, 1536], Float32, CPU>) -> (%1860:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.21.mlp.gate_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x000000000005a0 prog.kernel_launch(%1860:tensor<[1, 192, 8960], Float32, CPU>) -> (%1861:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.21.mlp.act, op_type:SiLU, op_options:null]
        addr:0x000000000005a1 prog.free(%1860:tensor<[1, 192, 8960], Float32, CPU>) -> ()
        addr:0x000000000005a2 prog.kernel_launch(%1859:tensor<[1, 192, 1536], Float32, CPU>) -> (%1862:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.21.mlp.up_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x000000000005a3 prog.free(%1859:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x000000000005a4 prog.kernel_launch(%1861:tensor<[1, 192, 8960], Float32, CPU>, %1862:tensor<[1, 192, 8960], Float32, CPU>) -> (%1863:tensor<[1, 192, 8960], Float32, CPU>)[op_type:Mul, op_options:null]
        addr:0x000000000005a5 prog.free(%1862:tensor<[1, 192, 8960], Float32, CPU>) -> ()
        addr:0x000000000005a6 prog.free(%1861:tensor<[1, 192, 8960], Float32, CPU>) -> ()
        addr:0x000000000005a7 prog.kernel_launch(%1863:tensor<[1, 192, 8960], Float32, CPU>) -> (%1864:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.21.mlp.down_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":8960,"out_channels":1536}]
        addr:0x000000000005a8 prog.free(%1863:tensor<[1, 192, 8960], Float32, CPU>) -> ()
        addr:0x000000000005a9 prog.ret
        addr:0x000000000005aa prog.label[symbol:model.layers.22.__entry]
        addr:0x000000000005ab prog.kernel_launch(%1865:tensor<[1, 192, 1536], Float32, CPU>) -> (%1866:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.22.input_layernorm, op_type:RMSNorm, op_options:{"add_unit_offset":false,"epsilon":9.999999974752427e-07}]
        addr:0x000000000005ac prog.jump model.layers.22.self_attn.__entry[offset:10]
        addr:0x000000000005ad prog.kernel_launch(%1884:tensor<[1, 192, 1536], Float32, CPU>, %1865:tensor<[1, 192, 1536], Float32, CPU>) -> (%1885:tensor<[1, 192, 1536], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x000000000005ae prog.free(%1865:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x000000000005af prog.free(%1884:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x000000000005b0 prog.kernel_launch(%1885:tensor<[1, 192, 1536], Float32, CPU>) -> (%1886:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.22.post_attention_layernorm, op_type:RMSNorm, op_options:{"add_unit_offset":false,"epsilon":9.999999974752427e-07}]
        addr:0x000000000005b1 prog.jump model.layers.22.mlp.__entry[offset:45]
        addr:0x000000000005b2 prog.kernel_launch(%1891:tensor<[1, 192, 1536], Float32, CPU>, %1885:tensor<[1, 192, 1536], Float32, CPU>) -> (%1892:tensor<[1, 192, 1536], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x000000000005b3 prog.free(%1885:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x000000000005b4 prog.free(%1891:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x000000000005b5 prog.ret
        addr:0x000000000005b6 prog.label[symbol:model.layers.22.self_attn.__entry]
        addr:0x000000000005b7 prog.kernel_launch(%1866:tensor<[1, 192, 1536], Float32, CPU>) -> (%1867:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.22.self_attn.q_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x000000000005b8 prog.kernel_launch(%1866:tensor<[1, 192, 1536], Float32, CPU>) -> (%1868:tensor<[1, 192, 256], Float32, CPU>)[symbol_name:model.layers.22.self_attn.k_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x000000000005b9 prog.kernel_launch(%1866:tensor<[1, 192, 1536], Float32, CPU>) -> (%1869:tensor<[1, 192, 256], Float32, CPU>)[symbol_name:model.layers.22.self_attn.v_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x000000000005ba prog.free(%1866:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x000000000005bb prog.kernel_launch(%1867:tensor<[1, 192, 1536], Float32, CPU>) -> (%1867:tensor<[1, 192, 12, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,12,128]}]
        addr:0x000000000005bc prog.kernel_launch(%1868:tensor<[1, 192, 256], Float32, CPU>) -> (%1868:tensor<[1, 192, 2, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,2,128]}]
        addr:0x000000000005bd prog.kernel_launch(%1869:tensor<[1, 192, 256], Float32, CPU>) -> (%1869:tensor<[1, 192, 2, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,2,128]}]
        addr:0x000000000005be prog.kernel_launch(%1867:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%1870:tensor<[1, 12, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x000000000005bf prog.free(%1867:tensor<[1, 192, 12, 128], Float32, CPU>) -> ()
        addr:0x000000000005c0 prog.kernel_launch(%1868:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%1871:tensor<[1, 2, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x000000000005c1 prog.free(%1868:tensor<[1, 192, 2, 128], Float32, CPU>) -> ()
        addr:0x000000000005c2 prog.kernel_launch(%1869:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%1872:tensor<[1, 2, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x000000000005c3 prog.free(%1869:tensor<[1, 192, 2, 128], Float32, CPU>) -> ()
        addr:0x000000000005c4 prog.kernel_launch(%1870:tensor<[1, 12, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1873:tensor<[1, 12, 192, 128], Float32, CPU>)[symbol_name:model.layers.22.self_attn.q_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x000000000005c5 prog.free(%1870:tensor<[1, 12, 192, 128], Float32, CPU>) -> ()
        addr:0x000000000005c6 prog.kernel_launch(%1871:tensor<[1, 2, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1874:tensor<[1, 2, 192, 128], Float32, CPU>)[symbol_name:model.layers.22.self_attn.k_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x000000000005c7 prog.free(%1871:tensor<[1, 2, 192, 128], Float32, CPU>) -> ()
        addr:0x000000000005c8 prog.kernel_launch(%1874:tensor<[1, 2, 192, 128], Float32, CPU>, %1872:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1875:tensor<[1, 2, 192, 128], Float32, CPU>, %1876:tensor<[1, 2, 192, 128], Float32, CPU>)[symbol_name:model.layers.22.self_attn.kv_cache, op_type:KVCache, op_options:{"head_dim":128,"kv_head":2,"layer_idx":0,"q_head":12,"use_fa2":false}]
        addr:0x000000000005c9 prog.free(%1872:tensor<[1, 2, 192, 128], Float32, CPU>) -> ()
        addr:0x000000000005ca prog.free(%1874:tensor<[1, 2, 192, 128], Float32, CPU>) -> ()
        addr:0x000000000005cb prog.kernel_launch(%1873:tensor<[1, 12, 192, 128], Float32, CPU>, %1875:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1877:tensor<[1, 12, 192, 192], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":true}]
        addr:0x000000000005cc prog.free(%1875:tensor<[1, 2, 192, 128], Float32, CPU>) -> ()
        addr:0x000000000005cd prog.free(%1873:tensor<[1, 12, 192, 128], Float32, CPU>) -> ()
        addr:0x000000000005ce prog.kernel_launch(%1877:tensor<[1, 12, 192, 192], Float32, CPU>, %1878:tensor<[1], Float32, CPU>) -> (%1879:tensor<[1, 12, 192, 192], Float32, CPU>)[op_type:Mul, op_options:null]
        addr:0x000000000005cf prog.free(%1878:tensor<[1], Float32, CPU>) -> ()
        addr:0x000000000005d0 prog.free(%1877:tensor<[1, 12, 192, 192], Float32, CPU>) -> ()
        addr:0x000000000005d1 prog.kernel_launch(%1879:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%1880:tensor<[1, 12, 192, 192], Float32, CPU>)[symbol_name:model.layers.22.self_attn.mask, op_type:CausalMask, op_options:{"sliding_window":false,"window_size":0}]
        addr:0x000000000005d2 prog.free(%1879:tensor<[1, 12, 192, 192], Float32, CPU>) -> ()
        addr:0x000000000005d3 prog.kernel_launch(%1880:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%1881:tensor<[1, 12, 192, 192], Float32, CPU>)[symbol_name:model.layers.22.self_attn.softmax, op_type:Softmax, op_options:{"axis":-1}]
        addr:0x000000000005d4 prog.free(%1880:tensor<[1, 12, 192, 192], Float32, CPU>) -> ()
        addr:0x000000000005d5 prog.kernel_launch(%1881:tensor<[1, 12, 192, 192], Float32, CPU>, %1876:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1882:tensor<[1, 12, 192, 128], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":false}]
        addr:0x000000000005d6 prog.free(%1876:tensor<[1, 2, 192, 128], Float32, CPU>) -> ()
        addr:0x000000000005d7 prog.free(%1881:tensor<[1, 12, 192, 192], Float32, CPU>) -> ()
        addr:0x000000000005d8 prog.kernel_launch(%1882:tensor<[1, 12, 192, 128], Float32, CPU>) -> (%1883:tensor<[1, 192, 12, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x000000000005d9 prog.free(%1882:tensor<[1, 12, 192, 128], Float32, CPU>) -> ()
        addr:0x000000000005da prog.kernel_launch(%1883:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%1883:tensor<[1, 192, 1536], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,1536]}]
        addr:0x000000000005db prog.kernel_launch(%1883:tensor<[1, 192, 1536], Float32, CPU>) -> (%1884:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.22.self_attn.o_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x000000000005dc prog.free(%1883:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x000000000005dd prog.ret
        addr:0x000000000005de prog.label[symbol:model.layers.22.mlp.__entry]
        addr:0x000000000005df prog.kernel_launch(%1886:tensor<[1, 192, 1536], Float32, CPU>) -> (%1887:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.22.mlp.gate_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x000000000005e0 prog.kernel_launch(%1887:tensor<[1, 192, 8960], Float32, CPU>) -> (%1888:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.22.mlp.act, op_type:SiLU, op_options:null]
        addr:0x000000000005e1 prog.free(%1887:tensor<[1, 192, 8960], Float32, CPU>) -> ()
        addr:0x000000000005e2 prog.kernel_launch(%1886:tensor<[1, 192, 1536], Float32, CPU>) -> (%1889:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.22.mlp.up_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x000000000005e3 prog.free(%1886:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x000000000005e4 prog.kernel_launch(%1888:tensor<[1, 192, 8960], Float32, CPU>, %1889:tensor<[1, 192, 8960], Float32, CPU>) -> (%1890:tensor<[1, 192, 8960], Float32, CPU>)[op_type:Mul, op_options:null]
        addr:0x000000000005e5 prog.free(%1889:tensor<[1, 192, 8960], Float32, CPU>) -> ()
        addr:0x000000000005e6 prog.free(%1888:tensor<[1, 192, 8960], Float32, CPU>) -> ()
        addr:0x000000000005e7 prog.kernel_launch(%1890:tensor<[1, 192, 8960], Float32, CPU>) -> (%1891:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.22.mlp.down_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":8960,"out_channels":1536}]
        addr:0x000000000005e8 prog.free(%1890:tensor<[1, 192, 8960], Float32, CPU>) -> ()
        addr:0x000000000005e9 prog.ret
        addr:0x000000000005ea prog.label[symbol:model.layers.23.__entry]
        addr:0x000000000005eb prog.kernel_launch(%1892:tensor<[1, 192, 1536], Float32, CPU>) -> (%1893:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.23.input_layernorm, op_type:RMSNorm, op_options:{"add_unit_offset":false,"epsilon":9.999999974752427e-07}]
        addr:0x000000000005ec prog.jump model.layers.23.self_attn.__entry[offset:10]
        addr:0x000000000005ed prog.kernel_launch(%1911:tensor<[1, 192, 1536], Float32, CPU>, %1892:tensor<[1, 192, 1536], Float32, CPU>) -> (%1912:tensor<[1, 192, 1536], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x000000000005ee prog.free(%1892:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x000000000005ef prog.free(%1911:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x000000000005f0 prog.kernel_launch(%1912:tensor<[1, 192, 1536], Float32, CPU>) -> (%1913:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.23.post_attention_layernorm, op_type:RMSNorm, op_options:{"add_unit_offset":false,"epsilon":9.999999974752427e-07}]
        addr:0x000000000005f1 prog.jump model.layers.23.mlp.__entry[offset:45]
        addr:0x000000000005f2 prog.kernel_launch(%1918:tensor<[1, 192, 1536], Float32, CPU>, %1912:tensor<[1, 192, 1536], Float32, CPU>) -> (%1919:tensor<[1, 192, 1536], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x000000000005f3 prog.free(%1912:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x000000000005f4 prog.free(%1918:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x000000000005f5 prog.ret
        addr:0x000000000005f6 prog.label[symbol:model.layers.23.self_attn.__entry]
        addr:0x000000000005f7 prog.kernel_launch(%1893:tensor<[1, 192, 1536], Float32, CPU>) -> (%1894:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.23.self_attn.q_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x000000000005f8 prog.kernel_launch(%1893:tensor<[1, 192, 1536], Float32, CPU>) -> (%1895:tensor<[1, 192, 256], Float32, CPU>)[symbol_name:model.layers.23.self_attn.k_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x000000000005f9 prog.kernel_launch(%1893:tensor<[1, 192, 1536], Float32, CPU>) -> (%1896:tensor<[1, 192, 256], Float32, CPU>)[symbol_name:model.layers.23.self_attn.v_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x000000000005fa prog.free(%1893:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x000000000005fb prog.kernel_launch(%1894:tensor<[1, 192, 1536], Float32, CPU>) -> (%1894:tensor<[1, 192, 12, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,12,128]}]
        addr:0x000000000005fc prog.kernel_launch(%1895:tensor<[1, 192, 256], Float32, CPU>) -> (%1895:tensor<[1, 192, 2, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,2,128]}]
        addr:0x000000000005fd prog.kernel_launch(%1896:tensor<[1, 192, 256], Float32, CPU>) -> (%1896:tensor<[1, 192, 2, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,2,128]}]
        addr:0x000000000005fe prog.kernel_launch(%1894:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%1897:tensor<[1, 12, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x000000000005ff prog.free(%1894:tensor<[1, 192, 12, 128], Float32, CPU>) -> ()
        addr:0x00000000000600 prog.kernel_launch(%1895:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%1898:tensor<[1, 2, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x00000000000601 prog.free(%1895:tensor<[1, 192, 2, 128], Float32, CPU>) -> ()
        addr:0x00000000000602 prog.kernel_launch(%1896:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%1899:tensor<[1, 2, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x00000000000603 prog.free(%1896:tensor<[1, 192, 2, 128], Float32, CPU>) -> ()
        addr:0x00000000000604 prog.kernel_launch(%1897:tensor<[1, 12, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1900:tensor<[1, 12, 192, 128], Float32, CPU>)[symbol_name:model.layers.23.self_attn.q_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x00000000000605 prog.free(%1897:tensor<[1, 12, 192, 128], Float32, CPU>) -> ()
        addr:0x00000000000606 prog.kernel_launch(%1898:tensor<[1, 2, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1901:tensor<[1, 2, 192, 128], Float32, CPU>)[symbol_name:model.layers.23.self_attn.k_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x00000000000607 prog.free(%1898:tensor<[1, 2, 192, 128], Float32, CPU>) -> ()
        addr:0x00000000000608 prog.kernel_launch(%1901:tensor<[1, 2, 192, 128], Float32, CPU>, %1899:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1902:tensor<[1, 2, 192, 128], Float32, CPU>, %1903:tensor<[1, 2, 192, 128], Float32, CPU>)[symbol_name:model.layers.23.self_attn.kv_cache, op_type:KVCache, op_options:{"head_dim":128,"kv_head":2,"layer_idx":0,"q_head":12,"use_fa2":false}]
        addr:0x00000000000609 prog.free(%1899:tensor<[1, 2, 192, 128], Float32, CPU>) -> ()
        addr:0x0000000000060a prog.free(%1901:tensor<[1, 2, 192, 128], Float32, CPU>) -> ()
        addr:0x0000000000060b prog.kernel_launch(%1900:tensor<[1, 12, 192, 128], Float32, CPU>, %1902:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1904:tensor<[1, 12, 192, 192], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":true}]
        addr:0x0000000000060c prog.free(%1902:tensor<[1, 2, 192, 128], Float32, CPU>) -> ()
        addr:0x0000000000060d prog.free(%1900:tensor<[1, 12, 192, 128], Float32, CPU>) -> ()
        addr:0x0000000000060e prog.kernel_launch(%1904:tensor<[1, 12, 192, 192], Float32, CPU>, %1905:tensor<[1], Float32, CPU>) -> (%1906:tensor<[1, 12, 192, 192], Float32, CPU>)[op_type:Mul, op_options:null]
        addr:0x0000000000060f prog.free(%1905:tensor<[1], Float32, CPU>) -> ()
        addr:0x00000000000610 prog.free(%1904:tensor<[1, 12, 192, 192], Float32, CPU>) -> ()
        addr:0x00000000000611 prog.kernel_launch(%1906:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%1907:tensor<[1, 12, 192, 192], Float32, CPU>)[symbol_name:model.layers.23.self_attn.mask, op_type:CausalMask, op_options:{"sliding_window":false,"window_size":0}]
        addr:0x00000000000612 prog.free(%1906:tensor<[1, 12, 192, 192], Float32, CPU>) -> ()
        addr:0x00000000000613 prog.kernel_launch(%1907:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%1908:tensor<[1, 12, 192, 192], Float32, CPU>)[symbol_name:model.layers.23.self_attn.softmax, op_type:Softmax, op_options:{"axis":-1}]
        addr:0x00000000000614 prog.free(%1907:tensor<[1, 12, 192, 192], Float32, CPU>) -> ()
        addr:0x00000000000615 prog.kernel_launch(%1908:tensor<[1, 12, 192, 192], Float32, CPU>, %1903:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1909:tensor<[1, 12, 192, 128], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":false}]
        addr:0x00000000000616 prog.free(%1903:tensor<[1, 2, 192, 128], Float32, CPU>) -> ()
        addr:0x00000000000617 prog.free(%1908:tensor<[1, 12, 192, 192], Float32, CPU>) -> ()
        addr:0x00000000000618 prog.kernel_launch(%1909:tensor<[1, 12, 192, 128], Float32, CPU>) -> (%1910:tensor<[1, 192, 12, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x00000000000619 prog.free(%1909:tensor<[1, 12, 192, 128], Float32, CPU>) -> ()
        addr:0x0000000000061a prog.kernel_launch(%1910:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%1910:tensor<[1, 192, 1536], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,1536]}]
        addr:0x0000000000061b prog.kernel_launch(%1910:tensor<[1, 192, 1536], Float32, CPU>) -> (%1911:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.23.self_attn.o_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x0000000000061c prog.free(%1910:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x0000000000061d prog.ret
        addr:0x0000000000061e prog.label[symbol:model.layers.23.mlp.__entry]
        addr:0x0000000000061f prog.kernel_launch(%1913:tensor<[1, 192, 1536], Float32, CPU>) -> (%1914:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.23.mlp.gate_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x00000000000620 prog.kernel_launch(%1914:tensor<[1, 192, 8960], Float32, CPU>) -> (%1915:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.23.mlp.act, op_type:SiLU, op_options:null]
        addr:0x00000000000621 prog.free(%1914:tensor<[1, 192, 8960], Float32, CPU>) -> ()
        addr:0x00000000000622 prog.kernel_launch(%1913:tensor<[1, 192, 1536], Float32, CPU>) -> (%1916:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.23.mlp.up_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x00000000000623 prog.free(%1913:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x00000000000624 prog.kernel_launch(%1915:tensor<[1, 192, 8960], Float32, CPU>, %1916:tensor<[1, 192, 8960], Float32, CPU>) -> (%1917:tensor<[1, 192, 8960], Float32, CPU>)[op_type:Mul, op_options:null]
        addr:0x00000000000625 prog.free(%1916:tensor<[1, 192, 8960], Float32, CPU>) -> ()
        addr:0x00000000000626 prog.free(%1915:tensor<[1, 192, 8960], Float32, CPU>) -> ()
        addr:0x00000000000627 prog.kernel_launch(%1917:tensor<[1, 192, 8960], Float32, CPU>) -> (%1918:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.23.mlp.down_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":8960,"out_channels":1536}]
        addr:0x00000000000628 prog.free(%1917:tensor<[1, 192, 8960], Float32, CPU>) -> ()
        addr:0x00000000000629 prog.ret
        addr:0x0000000000062a prog.label[symbol:model.layers.24.__entry]
        addr:0x0000000000062b prog.kernel_launch(%1919:tensor<[1, 192, 1536], Float32, CPU>) -> (%1920:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.24.input_layernorm, op_type:RMSNorm, op_options:{"add_unit_offset":false,"epsilon":9.999999974752427e-07}]
        addr:0x0000000000062c prog.jump model.layers.24.self_attn.__entry[offset:10]
        addr:0x0000000000062d prog.kernel_launch(%1938:tensor<[1, 192, 1536], Float32, CPU>, %1919:tensor<[1, 192, 1536], Float32, CPU>) -> (%1939:tensor<[1, 192, 1536], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x0000000000062e prog.free(%1919:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x0000000000062f prog.free(%1938:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x00000000000630 prog.kernel_launch(%1939:tensor<[1, 192, 1536], Float32, CPU>) -> (%1940:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.24.post_attention_layernorm, op_type:RMSNorm, op_options:{"add_unit_offset":false,"epsilon":9.999999974752427e-07}]
        addr:0x00000000000631 prog.jump model.layers.24.mlp.__entry[offset:45]
        addr:0x00000000000632 prog.kernel_launch(%1945:tensor<[1, 192, 1536], Float32, CPU>, %1939:tensor<[1, 192, 1536], Float32, CPU>) -> (%1946:tensor<[1, 192, 1536], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x00000000000633 prog.free(%1939:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x00000000000634 prog.free(%1945:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x00000000000635 prog.ret
        addr:0x00000000000636 prog.label[symbol:model.layers.24.self_attn.__entry]
        addr:0x00000000000637 prog.kernel_launch(%1920:tensor<[1, 192, 1536], Float32, CPU>) -> (%1921:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.24.self_attn.q_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x00000000000638 prog.kernel_launch(%1920:tensor<[1, 192, 1536], Float32, CPU>) -> (%1922:tensor<[1, 192, 256], Float32, CPU>)[symbol_name:model.layers.24.self_attn.k_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x00000000000639 prog.kernel_launch(%1920:tensor<[1, 192, 1536], Float32, CPU>) -> (%1923:tensor<[1, 192, 256], Float32, CPU>)[symbol_name:model.layers.24.self_attn.v_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x0000000000063a prog.free(%1920:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x0000000000063b prog.kernel_launch(%1921:tensor<[1, 192, 1536], Float32, CPU>) -> (%1921:tensor<[1, 192, 12, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,12,128]}]
        addr:0x0000000000063c prog.kernel_launch(%1922:tensor<[1, 192, 256], Float32, CPU>) -> (%1922:tensor<[1, 192, 2, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,2,128]}]
        addr:0x0000000000063d prog.kernel_launch(%1923:tensor<[1, 192, 256], Float32, CPU>) -> (%1923:tensor<[1, 192, 2, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,2,128]}]
        addr:0x0000000000063e prog.kernel_launch(%1921:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%1924:tensor<[1, 12, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x0000000000063f prog.free(%1921:tensor<[1, 192, 12, 128], Float32, CPU>) -> ()
        addr:0x00000000000640 prog.kernel_launch(%1922:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%1925:tensor<[1, 2, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x00000000000641 prog.free(%1922:tensor<[1, 192, 2, 128], Float32, CPU>) -> ()
        addr:0x00000000000642 prog.kernel_launch(%1923:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%1926:tensor<[1, 2, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x00000000000643 prog.free(%1923:tensor<[1, 192, 2, 128], Float32, CPU>) -> ()
        addr:0x00000000000644 prog.kernel_launch(%1924:tensor<[1, 12, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1927:tensor<[1, 12, 192, 128], Float32, CPU>)[symbol_name:model.layers.24.self_attn.q_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x00000000000645 prog.free(%1924:tensor<[1, 12, 192, 128], Float32, CPU>) -> ()
        addr:0x00000000000646 prog.kernel_launch(%1925:tensor<[1, 2, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1928:tensor<[1, 2, 192, 128], Float32, CPU>)[symbol_name:model.layers.24.self_attn.k_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x00000000000647 prog.free(%1925:tensor<[1, 2, 192, 128], Float32, CPU>) -> ()
        addr:0x00000000000648 prog.kernel_launch(%1928:tensor<[1, 2, 192, 128], Float32, CPU>, %1926:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1929:tensor<[1, 2, 192, 128], Float32, CPU>, %1930:tensor<[1, 2, 192, 128], Float32, CPU>)[symbol_name:model.layers.24.self_attn.kv_cache, op_type:KVCache, op_options:{"head_dim":128,"kv_head":2,"layer_idx":0,"q_head":12,"use_fa2":false}]
        addr:0x00000000000649 prog.free(%1926:tensor<[1, 2, 192, 128], Float32, CPU>) -> ()
        addr:0x0000000000064a prog.free(%1928:tensor<[1, 2, 192, 128], Float32, CPU>) -> ()
        addr:0x0000000000064b prog.kernel_launch(%1927:tensor<[1, 12, 192, 128], Float32, CPU>, %1929:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1931:tensor<[1, 12, 192, 192], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":true}]
        addr:0x0000000000064c prog.free(%1929:tensor<[1, 2, 192, 128], Float32, CPU>) -> ()
        addr:0x0000000000064d prog.free(%1927:tensor<[1, 12, 192, 128], Float32, CPU>) -> ()
        addr:0x0000000000064e prog.kernel_launch(%1931:tensor<[1, 12, 192, 192], Float32, CPU>, %1932:tensor<[1], Float32, CPU>) -> (%1933:tensor<[1, 12, 192, 192], Float32, CPU>)[op_type:Mul, op_options:null]
        addr:0x0000000000064f prog.free(%1932:tensor<[1], Float32, CPU>) -> ()
        addr:0x00000000000650 prog.free(%1931:tensor<[1, 12, 192, 192], Float32, CPU>) -> ()
        addr:0x00000000000651 prog.kernel_launch(%1933:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%1934:tensor<[1, 12, 192, 192], Float32, CPU>)[symbol_name:model.layers.24.self_attn.mask, op_type:CausalMask, op_options:{"sliding_window":false,"window_size":0}]
        addr:0x00000000000652 prog.free(%1933:tensor<[1, 12, 192, 192], Float32, CPU>) -> ()
        addr:0x00000000000653 prog.kernel_launch(%1934:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%1935:tensor<[1, 12, 192, 192], Float32, CPU>)[symbol_name:model.layers.24.self_attn.softmax, op_type:Softmax, op_options:{"axis":-1}]
        addr:0x00000000000654 prog.free(%1934:tensor<[1, 12, 192, 192], Float32, CPU>) -> ()
        addr:0x00000000000655 prog.kernel_launch(%1935:tensor<[1, 12, 192, 192], Float32, CPU>, %1930:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1936:tensor<[1, 12, 192, 128], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":false}]
        addr:0x00000000000656 prog.free(%1930:tensor<[1, 2, 192, 128], Float32, CPU>) -> ()
        addr:0x00000000000657 prog.free(%1935:tensor<[1, 12, 192, 192], Float32, CPU>) -> ()
        addr:0x00000000000658 prog.kernel_launch(%1936:tensor<[1, 12, 192, 128], Float32, CPU>) -> (%1937:tensor<[1, 192, 12, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x00000000000659 prog.free(%1936:tensor<[1, 12, 192, 128], Float32, CPU>) -> ()
        addr:0x0000000000065a prog.kernel_launch(%1937:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%1937:tensor<[1, 192, 1536], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,1536]}]
        addr:0x0000000000065b prog.kernel_launch(%1937:tensor<[1, 192, 1536], Float32, CPU>) -> (%1938:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.24.self_attn.o_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x0000000000065c prog.free(%1937:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x0000000000065d prog.ret
        addr:0x0000000000065e prog.label[symbol:model.layers.24.mlp.__entry]
        addr:0x0000000000065f prog.kernel_launch(%1940:tensor<[1, 192, 1536], Float32, CPU>) -> (%1941:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.24.mlp.gate_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x00000000000660 prog.kernel_launch(%1941:tensor<[1, 192, 8960], Float32, CPU>) -> (%1942:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.24.mlp.act, op_type:SiLU, op_options:null]
        addr:0x00000000000661 prog.free(%1941:tensor<[1, 192, 8960], Float32, CPU>) -> ()
        addr:0x00000000000662 prog.kernel_launch(%1940:tensor<[1, 192, 1536], Float32, CPU>) -> (%1943:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.24.mlp.up_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x00000000000663 prog.free(%1940:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x00000000000664 prog.kernel_launch(%1942:tensor<[1, 192, 8960], Float32, CPU>, %1943:tensor<[1, 192, 8960], Float32, CPU>) -> (%1944:tensor<[1, 192, 8960], Float32, CPU>)[op_type:Mul, op_options:null]
        addr:0x00000000000665 prog.free(%1943:tensor<[1, 192, 8960], Float32, CPU>) -> ()
        addr:0x00000000000666 prog.free(%1942:tensor<[1, 192, 8960], Float32, CPU>) -> ()
        addr:0x00000000000667 prog.kernel_launch(%1944:tensor<[1, 192, 8960], Float32, CPU>) -> (%1945:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.24.mlp.down_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":8960,"out_channels":1536}]
        addr:0x00000000000668 prog.free(%1944:tensor<[1, 192, 8960], Float32, CPU>) -> ()
        addr:0x00000000000669 prog.ret
        addr:0x0000000000066a prog.label[symbol:model.layers.25.__entry]
        addr:0x0000000000066b prog.kernel_launch(%1946:tensor<[1, 192, 1536], Float32, CPU>) -> (%1947:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.25.input_layernorm, op_type:RMSNorm, op_options:{"add_unit_offset":false,"epsilon":9.999999974752427e-07}]
        addr:0x0000000000066c prog.jump model.layers.25.self_attn.__entry[offset:10]
        addr:0x0000000000066d prog.kernel_launch(%1965:tensor<[1, 192, 1536], Float32, CPU>, %1946:tensor<[1, 192, 1536], Float32, CPU>) -> (%1966:tensor<[1, 192, 1536], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x0000000000066e prog.free(%1946:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x0000000000066f prog.free(%1965:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x00000000000670 prog.kernel_launch(%1966:tensor<[1, 192, 1536], Float32, CPU>) -> (%1967:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.25.post_attention_layernorm, op_type:RMSNorm, op_options:{"add_unit_offset":false,"epsilon":9.999999974752427e-07}]
        addr:0x00000000000671 prog.jump model.layers.25.mlp.__entry[offset:45]
        addr:0x00000000000672 prog.kernel_launch(%1972:tensor<[1, 192, 1536], Float32, CPU>, %1966:tensor<[1, 192, 1536], Float32, CPU>) -> (%1973:tensor<[1, 192, 1536], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x00000000000673 prog.free(%1966:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x00000000000674 prog.free(%1972:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x00000000000675 prog.ret
        addr:0x00000000000676 prog.label[symbol:model.layers.25.self_attn.__entry]
        addr:0x00000000000677 prog.kernel_launch(%1947:tensor<[1, 192, 1536], Float32, CPU>) -> (%1948:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.25.self_attn.q_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x00000000000678 prog.kernel_launch(%1947:tensor<[1, 192, 1536], Float32, CPU>) -> (%1949:tensor<[1, 192, 256], Float32, CPU>)[symbol_name:model.layers.25.self_attn.k_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x00000000000679 prog.kernel_launch(%1947:tensor<[1, 192, 1536], Float32, CPU>) -> (%1950:tensor<[1, 192, 256], Float32, CPU>)[symbol_name:model.layers.25.self_attn.v_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x0000000000067a prog.free(%1947:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x0000000000067b prog.kernel_launch(%1948:tensor<[1, 192, 1536], Float32, CPU>) -> (%1948:tensor<[1, 192, 12, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,12,128]}]
        addr:0x0000000000067c prog.kernel_launch(%1949:tensor<[1, 192, 256], Float32, CPU>) -> (%1949:tensor<[1, 192, 2, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,2,128]}]
        addr:0x0000000000067d prog.kernel_launch(%1950:tensor<[1, 192, 256], Float32, CPU>) -> (%1950:tensor<[1, 192, 2, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,2,128]}]
        addr:0x0000000000067e prog.kernel_launch(%1948:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%1951:tensor<[1, 12, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x0000000000067f prog.free(%1948:tensor<[1, 192, 12, 128], Float32, CPU>) -> ()
        addr:0x00000000000680 prog.kernel_launch(%1949:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%1952:tensor<[1, 2, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x00000000000681 prog.free(%1949:tensor<[1, 192, 2, 128], Float32, CPU>) -> ()
        addr:0x00000000000682 prog.kernel_launch(%1950:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%1953:tensor<[1, 2, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x00000000000683 prog.free(%1950:tensor<[1, 192, 2, 128], Float32, CPU>) -> ()
        addr:0x00000000000684 prog.kernel_launch(%1951:tensor<[1, 12, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1954:tensor<[1, 12, 192, 128], Float32, CPU>)[symbol_name:model.layers.25.self_attn.q_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x00000000000685 prog.free(%1951:tensor<[1, 12, 192, 128], Float32, CPU>) -> ()
        addr:0x00000000000686 prog.kernel_launch(%1952:tensor<[1, 2, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1955:tensor<[1, 2, 192, 128], Float32, CPU>)[symbol_name:model.layers.25.self_attn.k_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x00000000000687 prog.free(%1952:tensor<[1, 2, 192, 128], Float32, CPU>) -> ()
        addr:0x00000000000688 prog.kernel_launch(%1955:tensor<[1, 2, 192, 128], Float32, CPU>, %1953:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1956:tensor<[1, 2, 192, 128], Float32, CPU>, %1957:tensor<[1, 2, 192, 128], Float32, CPU>)[symbol_name:model.layers.25.self_attn.kv_cache, op_type:KVCache, op_options:{"head_dim":128,"kv_head":2,"layer_idx":0,"q_head":12,"use_fa2":false}]
        addr:0x00000000000689 prog.free(%1953:tensor<[1, 2, 192, 128], Float32, CPU>) -> ()
        addr:0x0000000000068a prog.free(%1955:tensor<[1, 2, 192, 128], Float32, CPU>) -> ()
        addr:0x0000000000068b prog.kernel_launch(%1954:tensor<[1, 12, 192, 128], Float32, CPU>, %1956:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1958:tensor<[1, 12, 192, 192], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":true}]
        addr:0x0000000000068c prog.free(%1956:tensor<[1, 2, 192, 128], Float32, CPU>) -> ()
        addr:0x0000000000068d prog.free(%1954:tensor<[1, 12, 192, 128], Float32, CPU>) -> ()
        addr:0x0000000000068e prog.kernel_launch(%1958:tensor<[1, 12, 192, 192], Float32, CPU>, %1959:tensor<[1], Float32, CPU>) -> (%1960:tensor<[1, 12, 192, 192], Float32, CPU>)[op_type:Mul, op_options:null]
        addr:0x0000000000068f prog.free(%1959:tensor<[1], Float32, CPU>) -> ()
        addr:0x00000000000690 prog.free(%1958:tensor<[1, 12, 192, 192], Float32, CPU>) -> ()
        addr:0x00000000000691 prog.kernel_launch(%1960:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%1961:tensor<[1, 12, 192, 192], Float32, CPU>)[symbol_name:model.layers.25.self_attn.mask, op_type:CausalMask, op_options:{"sliding_window":false,"window_size":0}]
        addr:0x00000000000692 prog.free(%1960:tensor<[1, 12, 192, 192], Float32, CPU>) -> ()
        addr:0x00000000000693 prog.kernel_launch(%1961:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%1962:tensor<[1, 12, 192, 192], Float32, CPU>)[symbol_name:model.layers.25.self_attn.softmax, op_type:Softmax, op_options:{"axis":-1}]
        addr:0x00000000000694 prog.free(%1961:tensor<[1, 12, 192, 192], Float32, CPU>) -> ()
        addr:0x00000000000695 prog.kernel_launch(%1962:tensor<[1, 12, 192, 192], Float32, CPU>, %1957:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1963:tensor<[1, 12, 192, 128], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":false}]
        addr:0x00000000000696 prog.free(%1957:tensor<[1, 2, 192, 128], Float32, CPU>) -> ()
        addr:0x00000000000697 prog.free(%1962:tensor<[1, 12, 192, 192], Float32, CPU>) -> ()
        addr:0x00000000000698 prog.kernel_launch(%1963:tensor<[1, 12, 192, 128], Float32, CPU>) -> (%1964:tensor<[1, 192, 12, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x00000000000699 prog.free(%1963:tensor<[1, 12, 192, 128], Float32, CPU>) -> ()
        addr:0x0000000000069a prog.kernel_launch(%1964:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%1964:tensor<[1, 192, 1536], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,1536]}]
        addr:0x0000000000069b prog.kernel_launch(%1964:tensor<[1, 192, 1536], Float32, CPU>) -> (%1965:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.25.self_attn.o_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x0000000000069c prog.free(%1964:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x0000000000069d prog.ret
        addr:0x0000000000069e prog.label[symbol:model.layers.25.mlp.__entry]
        addr:0x0000000000069f prog.kernel_launch(%1967:tensor<[1, 192, 1536], Float32, CPU>) -> (%1968:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.25.mlp.gate_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x000000000006a0 prog.kernel_launch(%1968:tensor<[1, 192, 8960], Float32, CPU>) -> (%1969:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.25.mlp.act, op_type:SiLU, op_options:null]
        addr:0x000000000006a1 prog.free(%1968:tensor<[1, 192, 8960], Float32, CPU>) -> ()
        addr:0x000000000006a2 prog.kernel_launch(%1967:tensor<[1, 192, 1536], Float32, CPU>) -> (%1970:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.25.mlp.up_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x000000000006a3 prog.free(%1967:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x000000000006a4 prog.kernel_launch(%1969:tensor<[1, 192, 8960], Float32, CPU>, %1970:tensor<[1, 192, 8960], Float32, CPU>) -> (%1971:tensor<[1, 192, 8960], Float32, CPU>)[op_type:Mul, op_options:null]
        addr:0x000000000006a5 prog.free(%1970:tensor<[1, 192, 8960], Float32, CPU>) -> ()
        addr:0x000000000006a6 prog.free(%1969:tensor<[1, 192, 8960], Float32, CPU>) -> ()
        addr:0x000000000006a7 prog.kernel_launch(%1971:tensor<[1, 192, 8960], Float32, CPU>) -> (%1972:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.25.mlp.down_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":8960,"out_channels":1536}]
        addr:0x000000000006a8 prog.free(%1971:tensor<[1, 192, 8960], Float32, CPU>) -> ()
        addr:0x000000000006a9 prog.ret
        addr:0x000000000006aa prog.label[symbol:model.layers.26.__entry]
        addr:0x000000000006ab prog.kernel_launch(%1973:tensor<[1, 192, 1536], Float32, CPU>) -> (%1974:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.26.input_layernorm, op_type:RMSNorm, op_options:{"add_unit_offset":false,"epsilon":9.999999974752427e-07}]
        addr:0x000000000006ac prog.jump model.layers.26.self_attn.__entry[offset:10]
        addr:0x000000000006ad prog.kernel_launch(%1992:tensor<[1, 192, 1536], Float32, CPU>, %1973:tensor<[1, 192, 1536], Float32, CPU>) -> (%1993:tensor<[1, 192, 1536], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x000000000006ae prog.free(%1973:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x000000000006af prog.free(%1992:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x000000000006b0 prog.kernel_launch(%1993:tensor<[1, 192, 1536], Float32, CPU>) -> (%1994:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.26.post_attention_layernorm, op_type:RMSNorm, op_options:{"add_unit_offset":false,"epsilon":9.999999974752427e-07}]
        addr:0x000000000006b1 prog.jump model.layers.26.mlp.__entry[offset:45]
        addr:0x000000000006b2 prog.kernel_launch(%1999:tensor<[1, 192, 1536], Float32, CPU>, %1993:tensor<[1, 192, 1536], Float32, CPU>) -> (%2000:tensor<[1, 192, 1536], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x000000000006b3 prog.free(%1993:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x000000000006b4 prog.free(%1999:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x000000000006b5 prog.ret
        addr:0x000000000006b6 prog.label[symbol:model.layers.26.self_attn.__entry]
        addr:0x000000000006b7 prog.kernel_launch(%1974:tensor<[1, 192, 1536], Float32, CPU>) -> (%1975:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.26.self_attn.q_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x000000000006b8 prog.kernel_launch(%1974:tensor<[1, 192, 1536], Float32, CPU>) -> (%1976:tensor<[1, 192, 256], Float32, CPU>)[symbol_name:model.layers.26.self_attn.k_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x000000000006b9 prog.kernel_launch(%1974:tensor<[1, 192, 1536], Float32, CPU>) -> (%1977:tensor<[1, 192, 256], Float32, CPU>)[symbol_name:model.layers.26.self_attn.v_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x000000000006ba prog.free(%1974:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x000000000006bb prog.kernel_launch(%1975:tensor<[1, 192, 1536], Float32, CPU>) -> (%1975:tensor<[1, 192, 12, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,12,128]}]
        addr:0x000000000006bc prog.kernel_launch(%1976:tensor<[1, 192, 256], Float32, CPU>) -> (%1976:tensor<[1, 192, 2, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,2,128]}]
        addr:0x000000000006bd prog.kernel_launch(%1977:tensor<[1, 192, 256], Float32, CPU>) -> (%1977:tensor<[1, 192, 2, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,2,128]}]
        addr:0x000000000006be prog.kernel_launch(%1975:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%1978:tensor<[1, 12, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x000000000006bf prog.free(%1975:tensor<[1, 192, 12, 128], Float32, CPU>) -> ()
        addr:0x000000000006c0 prog.kernel_launch(%1976:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%1979:tensor<[1, 2, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x000000000006c1 prog.free(%1976:tensor<[1, 192, 2, 128], Float32, CPU>) -> ()
        addr:0x000000000006c2 prog.kernel_launch(%1977:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%1980:tensor<[1, 2, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x000000000006c3 prog.free(%1977:tensor<[1, 192, 2, 128], Float32, CPU>) -> ()
        addr:0x000000000006c4 prog.kernel_launch(%1978:tensor<[1, 12, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1981:tensor<[1, 12, 192, 128], Float32, CPU>)[symbol_name:model.layers.26.self_attn.q_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x000000000006c5 prog.free(%1978:tensor<[1, 12, 192, 128], Float32, CPU>) -> ()
        addr:0x000000000006c6 prog.kernel_launch(%1979:tensor<[1, 2, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1982:tensor<[1, 2, 192, 128], Float32, CPU>)[symbol_name:model.layers.26.self_attn.k_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x000000000006c7 prog.free(%1979:tensor<[1, 2, 192, 128], Float32, CPU>) -> ()
        addr:0x000000000006c8 prog.kernel_launch(%1982:tensor<[1, 2, 192, 128], Float32, CPU>, %1980:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1983:tensor<[1, 2, 192, 128], Float32, CPU>, %1984:tensor<[1, 2, 192, 128], Float32, CPU>)[symbol_name:model.layers.26.self_attn.kv_cache, op_type:KVCache, op_options:{"head_dim":128,"kv_head":2,"layer_idx":0,"q_head":12,"use_fa2":false}]
        addr:0x000000000006c9 prog.free(%1980:tensor<[1, 2, 192, 128], Float32, CPU>) -> ()
        addr:0x000000000006ca prog.free(%1982:tensor<[1, 2, 192, 128], Float32, CPU>) -> ()
        addr:0x000000000006cb prog.kernel_launch(%1981:tensor<[1, 12, 192, 128], Float32, CPU>, %1983:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1985:tensor<[1, 12, 192, 192], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":true}]
        addr:0x000000000006cc prog.free(%1983:tensor<[1, 2, 192, 128], Float32, CPU>) -> ()
        addr:0x000000000006cd prog.free(%1981:tensor<[1, 12, 192, 128], Float32, CPU>) -> ()
        addr:0x000000000006ce prog.kernel_launch(%1985:tensor<[1, 12, 192, 192], Float32, CPU>, %1986:tensor<[1], Float32, CPU>) -> (%1987:tensor<[1, 12, 192, 192], Float32, CPU>)[op_type:Mul, op_options:null]
        addr:0x000000000006cf prog.free(%1986:tensor<[1], Float32, CPU>) -> ()
        addr:0x000000000006d0 prog.free(%1985:tensor<[1, 12, 192, 192], Float32, CPU>) -> ()
        addr:0x000000000006d1 prog.kernel_launch(%1987:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%1988:tensor<[1, 12, 192, 192], Float32, CPU>)[symbol_name:model.layers.26.self_attn.mask, op_type:CausalMask, op_options:{"sliding_window":false,"window_size":0}]
        addr:0x000000000006d2 prog.free(%1987:tensor<[1, 12, 192, 192], Float32, CPU>) -> ()
        addr:0x000000000006d3 prog.kernel_launch(%1988:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%1989:tensor<[1, 12, 192, 192], Float32, CPU>)[symbol_name:model.layers.26.self_attn.softmax, op_type:Softmax, op_options:{"axis":-1}]
        addr:0x000000000006d4 prog.free(%1988:tensor<[1, 12, 192, 192], Float32, CPU>) -> ()
        addr:0x000000000006d5 prog.kernel_launch(%1989:tensor<[1, 12, 192, 192], Float32, CPU>, %1984:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1990:tensor<[1, 12, 192, 128], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":false}]
        addr:0x000000000006d6 prog.free(%1984:tensor<[1, 2, 192, 128], Float32, CPU>) -> ()
        addr:0x000000000006d7 prog.free(%1989:tensor<[1, 12, 192, 192], Float32, CPU>) -> ()
        addr:0x000000000006d8 prog.kernel_launch(%1990:tensor<[1, 12, 192, 128], Float32, CPU>) -> (%1991:tensor<[1, 192, 12, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x000000000006d9 prog.free(%1990:tensor<[1, 12, 192, 128], Float32, CPU>) -> ()
        addr:0x000000000006da prog.kernel_launch(%1991:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%1991:tensor<[1, 192, 1536], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,1536]}]
        addr:0x000000000006db prog.kernel_launch(%1991:tensor<[1, 192, 1536], Float32, CPU>) -> (%1992:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.26.self_attn.o_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x000000000006dc prog.free(%1991:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x000000000006dd prog.ret
        addr:0x000000000006de prog.label[symbol:model.layers.26.mlp.__entry]
        addr:0x000000000006df prog.kernel_launch(%1994:tensor<[1, 192, 1536], Float32, CPU>) -> (%1995:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.26.mlp.gate_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x000000000006e0 prog.kernel_launch(%1995:tensor<[1, 192, 8960], Float32, CPU>) -> (%1996:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.26.mlp.act, op_type:SiLU, op_options:null]
        addr:0x000000000006e1 prog.free(%1995:tensor<[1, 192, 8960], Float32, CPU>) -> ()
        addr:0x000000000006e2 prog.kernel_launch(%1994:tensor<[1, 192, 1536], Float32, CPU>) -> (%1997:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.26.mlp.up_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x000000000006e3 prog.free(%1994:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x000000000006e4 prog.kernel_launch(%1996:tensor<[1, 192, 8960], Float32, CPU>, %1997:tensor<[1, 192, 8960], Float32, CPU>) -> (%1998:tensor<[1, 192, 8960], Float32, CPU>)[op_type:Mul, op_options:null]
        addr:0x000000000006e5 prog.free(%1997:tensor<[1, 192, 8960], Float32, CPU>) -> ()
        addr:0x000000000006e6 prog.free(%1996:tensor<[1, 192, 8960], Float32, CPU>) -> ()
        addr:0x000000000006e7 prog.kernel_launch(%1998:tensor<[1, 192, 8960], Float32, CPU>) -> (%1999:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.26.mlp.down_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":8960,"out_channels":1536}]
        addr:0x000000000006e8 prog.free(%1998:tensor<[1, 192, 8960], Float32, CPU>) -> ()
        addr:0x000000000006e9 prog.ret
        addr:0x000000000006ea prog.label[symbol:model.layers.27.__entry]
        addr:0x000000000006eb prog.kernel_launch(%2000:tensor<[1, 192, 1536], Float32, CPU>) -> (%2001:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.27.input_layernorm, op_type:RMSNorm, op_options:{"add_unit_offset":false,"epsilon":9.999999974752427e-07}]
        addr:0x000000000006ec prog.jump model.layers.27.self_attn.__entry[offset:10]
        addr:0x000000000006ed prog.kernel_launch(%2019:tensor<[1, 192, 1536], Float32, CPU>, %2000:tensor<[1, 192, 1536], Float32, CPU>) -> (%2020:tensor<[1, 192, 1536], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x000000000006ee prog.free(%2000:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x000000000006ef prog.free(%2019:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x000000000006f0 prog.kernel_launch(%2020:tensor<[1, 192, 1536], Float32, CPU>) -> (%2021:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.27.post_attention_layernorm, op_type:RMSNorm, op_options:{"add_unit_offset":false,"epsilon":9.999999974752427e-07}]
        addr:0x000000000006f1 prog.jump model.layers.27.mlp.__entry[offset:47]
        addr:0x000000000006f2 prog.kernel_launch(%2026:tensor<[1, 192, 1536], Float32, CPU>, %2020:tensor<[1, 192, 1536], Float32, CPU>) -> (%2027:tensor<[1, 192, 1536], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x000000000006f3 prog.free(%2020:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x000000000006f4 prog.free(%2026:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x000000000006f5 prog.ret
        addr:0x000000000006f6 prog.label[symbol:model.layers.27.self_attn.__entry]
        addr:0x000000000006f7 prog.kernel_launch(%2001:tensor<[1, 192, 1536], Float32, CPU>) -> (%2002:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.27.self_attn.q_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x000000000006f8 prog.kernel_launch(%2001:tensor<[1, 192, 1536], Float32, CPU>) -> (%2003:tensor<[1, 192, 256], Float32, CPU>)[symbol_name:model.layers.27.self_attn.k_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x000000000006f9 prog.kernel_launch(%2001:tensor<[1, 192, 1536], Float32, CPU>) -> (%2004:tensor<[1, 192, 256], Float32, CPU>)[symbol_name:model.layers.27.self_attn.v_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x000000000006fa prog.free(%2001:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x000000000006fb prog.kernel_launch(%2002:tensor<[1, 192, 1536], Float32, CPU>) -> (%2002:tensor<[1, 192, 12, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,12,128]}]
        addr:0x000000000006fc prog.kernel_launch(%2003:tensor<[1, 192, 256], Float32, CPU>) -> (%2003:tensor<[1, 192, 2, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,2,128]}]
        addr:0x000000000006fd prog.kernel_launch(%2004:tensor<[1, 192, 256], Float32, CPU>) -> (%2004:tensor<[1, 192, 2, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,2,128]}]
        addr:0x000000000006fe prog.kernel_launch(%2002:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%2005:tensor<[1, 12, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x000000000006ff prog.free(%2002:tensor<[1, 192, 12, 128], Float32, CPU>) -> ()
        addr:0x00000000000700 prog.kernel_launch(%2003:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%2006:tensor<[1, 2, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x00000000000701 prog.free(%2003:tensor<[1, 192, 2, 128], Float32, CPU>) -> ()
        addr:0x00000000000702 prog.kernel_launch(%2004:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%2007:tensor<[1, 2, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x00000000000703 prog.free(%2004:tensor<[1, 192, 2, 128], Float32, CPU>) -> ()
        addr:0x00000000000704 prog.kernel_launch(%2005:tensor<[1, 12, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%2008:tensor<[1, 12, 192, 128], Float32, CPU>)[symbol_name:model.layers.27.self_attn.q_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x00000000000705 prog.free(%2005:tensor<[1, 12, 192, 128], Float32, CPU>) -> ()
        addr:0x00000000000706 prog.kernel_launch(%2006:tensor<[1, 2, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%2009:tensor<[1, 2, 192, 128], Float32, CPU>)[symbol_name:model.layers.27.self_attn.k_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x00000000000707 prog.free(%1271:tensor<[192, 128], Float32, CPU>) -> ()
        addr:0x00000000000708 prog.free(%1270:tensor<[192, 128], Float32, CPU>) -> ()
        addr:0x00000000000709 prog.free(%2006:tensor<[1, 2, 192, 128], Float32, CPU>) -> ()
        addr:0x0000000000070a prog.kernel_launch(%2009:tensor<[1, 2, 192, 128], Float32, CPU>, %2007:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%2010:tensor<[1, 2, 192, 128], Float32, CPU>, %2011:tensor<[1, 2, 192, 128], Float32, CPU>)[symbol_name:model.layers.27.self_attn.kv_cache, op_type:KVCache, op_options:{"head_dim":128,"kv_head":2,"layer_idx":0,"q_head":12,"use_fa2":false}]
        addr:0x0000000000070b prog.free(%2007:tensor<[1, 2, 192, 128], Float32, CPU>) -> ()
        addr:0x0000000000070c prog.free(%2009:tensor<[1, 2, 192, 128], Float32, CPU>) -> ()
        addr:0x0000000000070d prog.kernel_launch(%2008:tensor<[1, 12, 192, 128], Float32, CPU>, %2010:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%2012:tensor<[1, 12, 192, 192], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":true}]
        addr:0x0000000000070e prog.free(%2010:tensor<[1, 2, 192, 128], Float32, CPU>) -> ()
        addr:0x0000000000070f prog.free(%2008:tensor<[1, 12, 192, 128], Float32, CPU>) -> ()
        addr:0x00000000000710 prog.kernel_launch(%2012:tensor<[1, 12, 192, 192], Float32, CPU>, %2013:tensor<[1], Float32, CPU>) -> (%2014:tensor<[1, 12, 192, 192], Float32, CPU>)[op_type:Mul, op_options:null]
        addr:0x00000000000711 prog.free(%2013:tensor<[1], Float32, CPU>) -> ()
        addr:0x00000000000712 prog.free(%2012:tensor<[1, 12, 192, 192], Float32, CPU>) -> ()
        addr:0x00000000000713 prog.kernel_launch(%2014:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%2015:tensor<[1, 12, 192, 192], Float32, CPU>)[symbol_name:model.layers.27.self_attn.mask, op_type:CausalMask, op_options:{"sliding_window":false,"window_size":0}]
        addr:0x00000000000714 prog.free(%2014:tensor<[1, 12, 192, 192], Float32, CPU>) -> ()
        addr:0x00000000000715 prog.kernel_launch(%2015:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%2016:tensor<[1, 12, 192, 192], Float32, CPU>)[symbol_name:model.layers.27.self_attn.softmax, op_type:Softmax, op_options:{"axis":-1}]
        addr:0x00000000000716 prog.free(%2015:tensor<[1, 12, 192, 192], Float32, CPU>) -> ()
        addr:0x00000000000717 prog.kernel_launch(%2016:tensor<[1, 12, 192, 192], Float32, CPU>, %2011:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%2017:tensor<[1, 12, 192, 128], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":false}]
        addr:0x00000000000718 prog.free(%2011:tensor<[1, 2, 192, 128], Float32, CPU>) -> ()
        addr:0x00000000000719 prog.free(%2016:tensor<[1, 12, 192, 192], Float32, CPU>) -> ()
        addr:0x0000000000071a prog.kernel_launch(%2017:tensor<[1, 12, 192, 128], Float32, CPU>) -> (%2018:tensor<[1, 192, 12, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x0000000000071b prog.free(%2017:tensor<[1, 12, 192, 128], Float32, CPU>) -> ()
        addr:0x0000000000071c prog.kernel_launch(%2018:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%2018:tensor<[1, 192, 1536], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,1536]}]
        addr:0x0000000000071d prog.kernel_launch(%2018:tensor<[1, 192, 1536], Float32, CPU>) -> (%2019:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.27.self_attn.o_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x0000000000071e prog.free(%2018:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x0000000000071f prog.ret
        addr:0x00000000000720 prog.label[symbol:model.layers.27.mlp.__entry]
        addr:0x00000000000721 prog.kernel_launch(%2021:tensor<[1, 192, 1536], Float32, CPU>) -> (%2022:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.27.mlp.gate_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x00000000000722 prog.kernel_launch(%2022:tensor<[1, 192, 8960], Float32, CPU>) -> (%2023:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.27.mlp.act, op_type:SiLU, op_options:null]
        addr:0x00000000000723 prog.free(%2022:tensor<[1, 192, 8960], Float32, CPU>) -> ()
        addr:0x00000000000724 prog.kernel_launch(%2021:tensor<[1, 192, 1536], Float32, CPU>) -> (%2024:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.27.mlp.up_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x00000000000725 prog.free(%2021:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        addr:0x00000000000726 prog.kernel_launch(%2023:tensor<[1, 192, 8960], Float32, CPU>, %2024:tensor<[1, 192, 8960], Float32, CPU>) -> (%2025:tensor<[1, 192, 8960], Float32, CPU>)[op_type:Mul, op_options:null]
        addr:0x00000000000727 prog.free(%2024:tensor<[1, 192, 8960], Float32, CPU>) -> ()
        addr:0x00000000000728 prog.free(%2023:tensor<[1, 192, 8960], Float32, CPU>) -> ()
        addr:0x00000000000729 prog.kernel_launch(%2025:tensor<[1, 192, 8960], Float32, CPU>) -> (%2026:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.27.mlp.down_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":8960,"out_channels":1536}]
        addr:0x0000000000072a prog.free(%2025:tensor<[1, 192, 8960], Float32, CPU>) -> ()
        addr:0x0000000000072b prog.ret
    }
    prog.fragment @__MLLM_JIT_PACKAGE_SYMBOL_TABLE_SEGMENT <CPU> <table> {
        addr:0x00000000000000 prog.value_symbol() -> (%138:tensor<[1339392], UInt8, CPU>[@model.layers.0.self_attn.q_proj.weight])[symbol:model.layers.0.self_attn.q_proj.weight]
        addr:0x00000000000001 prog.value_symbol() -> (%574:tensor<[223232], UInt8, CPU>[@model.layers.0.self_attn.k_proj.weight])[symbol:model.layers.0.self_attn.k_proj.weight]
        addr:0x00000000000002 prog.value_symbol() -> (%570:tensor<[223232], UInt8, CPU>[@model.layers.0.self_attn.v_proj.weight])[symbol:model.layers.0.self_attn.v_proj.weight]
        addr:0x00000000000003 prog.value_symbol() -> (%536:tensor<[1339392], UInt8, CPU>[@model.layers.0.self_attn.o_proj.weight])[symbol:model.layers.0.self_attn.o_proj.weight]
        addr:0x00000000000004 prog.value_symbol() -> (%566:tensor<[7813120], UInt8, CPU>[@model.layers.0.mlp.gate_proj.weight])[symbol:model.layers.0.mlp.gate_proj.weight]
        addr:0x00000000000005 prog.value_symbol() -> (%170:tensor<[7813120], UInt8, CPU>[@model.layers.0.mlp.up_proj.weight])[symbol:model.layers.0.mlp.up_proj.weight]
        addr:0x00000000000006 prog.value_symbol() -> (%187:tensor<[7753728], UInt8, CPU>[@model.layers.0.mlp.down_proj.weight])[symbol:model.layers.0.mlp.down_proj.weight]
        addr:0x00000000000007 prog.value_symbol() -> (%137:tensor<[1339392], UInt8, CPU>[@model.layers.1.self_attn.q_proj.weight])[symbol:model.layers.1.self_attn.q_proj.weight]
        addr:0x00000000000008 prog.value_symbol() -> (%164:tensor<[223232], UInt8, CPU>[@model.layers.1.self_attn.k_proj.weight])[symbol:model.layers.1.self_attn.k_proj.weight]
        addr:0x00000000000009 prog.value_symbol() -> (%299:tensor<[223232], UInt8, CPU>[@model.layers.1.self_attn.v_proj.weight])[symbol:model.layers.1.self_attn.v_proj.weight]
        addr:0x0000000000000a prog.value_symbol() -> (%144:tensor<[1339392], UInt8, CPU>[@model.layers.1.self_attn.o_proj.weight])[symbol:model.layers.1.self_attn.o_proj.weight]
        addr:0x0000000000000b prog.value_symbol() -> (%506:tensor<[7813120], UInt8, CPU>[@model.layers.1.mlp.gate_proj.weight])[symbol:model.layers.1.mlp.gate_proj.weight]
        addr:0x0000000000000c prog.value_symbol() -> (%399:tensor<[7813120], UInt8, CPU>[@model.layers.1.mlp.up_proj.weight])[symbol:model.layers.1.mlp.up_proj.weight]
        addr:0x0000000000000d prog.value_symbol() -> (%534:tensor<[7753728], UInt8, CPU>[@model.layers.1.mlp.down_proj.weight])[symbol:model.layers.1.mlp.down_proj.weight]
        addr:0x0000000000000e prog.value_symbol() -> (%449:tensor<[1339392], UInt8, CPU>[@model.layers.2.self_attn.q_proj.weight])[symbol:model.layers.2.self_attn.q_proj.weight]
        addr:0x0000000000000f prog.value_symbol() -> (%161:tensor<[223232], UInt8, CPU>[@model.layers.2.self_attn.k_proj.weight])[symbol:model.layers.2.self_attn.k_proj.weight]
        addr:0x00000000000010 prog.value_symbol() -> (%366:tensor<[223232], UInt8, CPU>[@model.layers.2.self_attn.v_proj.weight])[symbol:model.layers.2.self_attn.v_proj.weight]
        addr:0x00000000000011 prog.value_symbol() -> (%147:tensor<[1339392], UInt8, CPU>[@model.layers.2.self_attn.o_proj.weight])[symbol:model.layers.2.self_attn.o_proj.weight]
        addr:0x00000000000012 prog.value_symbol() -> (%440:tensor<[7813120], UInt8, CPU>[@model.layers.2.mlp.gate_proj.weight])[symbol:model.layers.2.mlp.gate_proj.weight]
        addr:0x00000000000013 prog.value_symbol() -> (%436:tensor<[7813120], UInt8, CPU>[@model.layers.2.mlp.up_proj.weight])[symbol:model.layers.2.mlp.up_proj.weight]
        addr:0x00000000000014 prog.value_symbol() -> (%194:tensor<[7753728], UInt8, CPU>[@model.layers.2.mlp.down_proj.weight])[symbol:model.layers.2.mlp.down_proj.weight]
        addr:0x00000000000015 prog.value_symbol() -> (%142:tensor<[1339392], UInt8, CPU>[@model.layers.3.self_attn.q_proj.weight])[symbol:model.layers.3.self_attn.q_proj.weight]
        addr:0x00000000000016 prog.value_symbol() -> (%356:tensor<[223232], UInt8, CPU>[@model.layers.3.self_attn.k_proj.weight])[symbol:model.layers.3.self_attn.k_proj.weight]
        addr:0x00000000000017 prog.value_symbol() -> (%355:tensor<[223232], UInt8, CPU>[@model.layers.3.self_attn.v_proj.weight])[symbol:model.layers.3.self_attn.v_proj.weight]
        addr:0x00000000000018 prog.value_symbol() -> (%148:tensor<[1339392], UInt8, CPU>[@model.layers.3.self_attn.o_proj.weight])[symbol:model.layers.3.self_attn.o_proj.weight]
        addr:0x00000000000019 prog.value_symbol() -> (%378:tensor<[7813120], UInt8, CPU>[@model.layers.3.mlp.gate_proj.weight])[symbol:model.layers.3.mlp.gate_proj.weight]
        addr:0x0000000000001a prog.value_symbol() -> (%173:tensor<[7813120], UInt8, CPU>[@model.layers.3.mlp.up_proj.weight])[symbol:model.layers.3.mlp.up_proj.weight]
        addr:0x0000000000001b prog.value_symbol() -> (%200:tensor<[7753728], UInt8, CPU>[@model.layers.3.mlp.down_proj.weight])[symbol:model.layers.3.mlp.down_proj.weight]
        addr:0x0000000000001c prog.value_symbol() -> (%530:tensor<[1339392], UInt8, CPU>[@model.layers.4.self_attn.q_proj.weight])[symbol:model.layers.4.self_attn.q_proj.weight]
        addr:0x0000000000001d prog.value_symbol() -> (%159:tensor<[223232], UInt8, CPU>[@model.layers.4.self_attn.k_proj.weight])[symbol:model.layers.4.self_attn.k_proj.weight]
        addr:0x0000000000001e prog.value_symbol() -> (%558:tensor<[223232], UInt8, CPU>[@model.layers.4.self_attn.v_proj.weight])[symbol:model.layers.4.self_attn.v_proj.weight]
        addr:0x0000000000001f prog.value_symbol() -> (%428:tensor<[1339392], UInt8, CPU>[@model.layers.4.self_attn.o_proj.weight])[symbol:model.layers.4.self_attn.o_proj.weight]
        addr:0x00000000000020 prog.value_symbol() -> (%354:tensor<[7813120], UInt8, CPU>[@model.layers.4.mlp.gate_proj.weight])[symbol:model.layers.4.mlp.gate_proj.weight]
        addr:0x00000000000021 prog.value_symbol() -> (%174:tensor<[7813120], UInt8, CPU>[@model.layers.4.mlp.up_proj.weight])[symbol:model.layers.4.mlp.up_proj.weight]
        addr:0x00000000000022 prog.value_symbol() -> (%201:tensor<[7753728], UInt8, CPU>[@model.layers.4.mlp.down_proj.weight])[symbol:model.layers.4.mlp.down_proj.weight]
        addr:0x00000000000023 prog.value_symbol() -> (%455:tensor<[1339392], UInt8, CPU>[@model.layers.5.self_attn.q_proj.weight])[symbol:model.layers.5.self_attn.q_proj.weight]
        addr:0x00000000000024 prog.value_symbol() -> (%160:tensor<[223232], UInt8, CPU>[@model.layers.5.self_attn.k_proj.weight])[symbol:model.layers.5.self_attn.k_proj.weight]
        addr:0x00000000000025 prog.value_symbol() -> (%131:tensor<[223232], UInt8, CPU>[@model.layers.5.self_attn.v_proj.weight])[symbol:model.layers.5.self_attn.v_proj.weight]
        addr:0x00000000000026 prog.value_symbol() -> (%398:tensor<[1339392], UInt8, CPU>[@model.layers.5.self_attn.o_proj.weight])[symbol:model.layers.5.self_attn.o_proj.weight]
        addr:0x00000000000027 prog.value_symbol() -> (%502:tensor<[7813120], UInt8, CPU>[@model.layers.5.mlp.gate_proj.weight])[symbol:model.layers.5.mlp.gate_proj.weight]
        addr:0x00000000000028 prog.value_symbol() -> (%168:tensor<[7813120], UInt8, CPU>[@model.layers.5.mlp.up_proj.weight])[symbol:model.layers.5.mlp.up_proj.weight]
        addr:0x00000000000029 prog.value_symbol() -> (%528:tensor<[7753728], UInt8, CPU>[@model.layers.5.mlp.down_proj.weight])[symbol:model.layers.5.mlp.down_proj.weight]
        addr:0x0000000000002a prog.value_symbol() -> (%344:tensor<[1339392], UInt8, CPU>[@model.layers.6.self_attn.q_proj.weight])[symbol:model.layers.6.self_attn.q_proj.weight]
        addr:0x0000000000002b prog.value_symbol() -> (%564:tensor<[223232], UInt8, CPU>[@model.layers.6.self_attn.k_proj.weight])[symbol:model.layers.6.self_attn.k_proj.weight]
        addr:0x0000000000002c prog.value_symbol() -> (%342:tensor<[223232], UInt8, CPU>[@model.layers.6.self_attn.v_proj.weight])[symbol:model.layers.6.self_attn.v_proj.weight]
        addr:0x0000000000002d prog.value_symbol() -> (%153:tensor<[1339392], UInt8, CPU>[@model.layers.6.self_attn.o_proj.weight])[symbol:model.layers.6.self_attn.o_proj.weight]
        addr:0x0000000000002e prog.value_symbol() -> (%185:tensor<[7813120], UInt8, CPU>[@model.layers.6.mlp.gate_proj.weight])[symbol:model.layers.6.mlp.gate_proj.weight]
        addr:0x0000000000002f prog.value_symbol() -> (%332:tensor<[7813120], UInt8, CPU>[@model.layers.6.mlp.up_proj.weight])[symbol:model.layers.6.mlp.up_proj.weight]
        addr:0x00000000000030 prog.value_symbol() -> (%309:tensor<[7753728], UInt8, CPU>[@model.layers.6.mlp.down_proj.weight])[symbol:model.layers.6.mlp.down_proj.weight]
        addr:0x00000000000031 prog.value_symbol() -> (%438:tensor<[1339392], UInt8, CPU>[@model.layers.7.self_attn.q_proj.weight])[symbol:model.layers.7.self_attn.q_proj.weight]
        addr:0x00000000000032 prog.value_symbol() -> (%363:tensor<[223232], UInt8, CPU>[@model.layers.7.self_attn.k_proj.weight])[symbol:model.layers.7.self_attn.k_proj.weight]
        addr:0x00000000000033 prog.value_symbol() -> (%127:tensor<[223232], UInt8, CPU>[@model.layers.7.self_attn.v_proj.weight])[symbol:model.layers.7.self_attn.v_proj.weight]
        addr:0x00000000000034 prog.value_symbol() -> (%145:tensor<[1339392], UInt8, CPU>[@model.layers.7.self_attn.o_proj.weight])[symbol:model.layers.7.self_attn.o_proj.weight]
        addr:0x00000000000035 prog.value_symbol() -> (%184:tensor<[7813120], UInt8, CPU>[@model.layers.7.mlp.gate_proj.weight])[symbol:model.layers.7.mlp.gate_proj.weight]
        addr:0x00000000000036 prog.value_symbol() -> (%165:tensor<[7813120], UInt8, CPU>[@model.layers.7.mlp.up_proj.weight])[symbol:model.layers.7.mlp.up_proj.weight]
        addr:0x00000000000037 prog.value_symbol() -> (%379:tensor<[7753728], UInt8, CPU>[@model.layers.7.mlp.down_proj.weight])[symbol:model.layers.7.mlp.down_proj.weight]
        addr:0x00000000000038 prog.value_symbol() -> (%334:tensor<[1339392], UInt8, CPU>[@model.layers.8.self_attn.q_proj.weight])[symbol:model.layers.8.self_attn.q_proj.weight]
        addr:0x00000000000039 prog.value_symbol() -> (%333:tensor<[223232], UInt8, CPU>[@model.layers.8.self_attn.k_proj.weight])[symbol:model.layers.8.self_attn.k_proj.weight]
        addr:0x0000000000003a prog.value_symbol() -> (%330:tensor<[223232], UInt8, CPU>[@model.layers.8.self_attn.v_proj.weight])[symbol:model.layers.8.self_attn.v_proj.weight]
        addr:0x0000000000003b prog.value_symbol() -> (%154:tensor<[1339392], UInt8, CPU>[@model.layers.8.self_attn.o_proj.weight])[symbol:model.layers.8.self_attn.o_proj.weight]
        addr:0x0000000000003c prog.value_symbol() -> (%335:tensor<[7813120], UInt8, CPU>[@model.layers.8.mlp.gate_proj.weight])[symbol:model.layers.8.mlp.gate_proj.weight]
        addr:0x0000000000003d prog.value_symbol() -> (%167:tensor<[7813120], UInt8, CPU>[@model.layers.8.mlp.up_proj.weight])[symbol:model.layers.8.mlp.up_proj.weight]
        addr:0x0000000000003e prog.value_symbol() -> (%199:tensor<[7753728], UInt8, CPU>[@model.layers.8.mlp.down_proj.weight])[symbol:model.layers.8.mlp.down_proj.weight]
        addr:0x0000000000003f prog.value_symbol() -> (%139:tensor<[1339392], UInt8, CPU>[@model.layers.9.self_attn.q_proj.weight])[symbol:model.layers.9.self_attn.q_proj.weight]
        addr:0x00000000000040 prog.value_symbol() -> (%324:tensor<[223232], UInt8, CPU>[@model.layers.9.self_attn.k_proj.weight])[symbol:model.layers.9.self_attn.k_proj.weight]
        addr:0x00000000000041 prog.value_symbol() -> (%129:tensor<[223232], UInt8, CPU>[@model.layers.9.self_attn.v_proj.weight])[symbol:model.layers.9.self_attn.v_proj.weight]
        addr:0x00000000000042 prog.value_symbol() -> (%149:tensor<[1339392], UInt8, CPU>[@model.layers.9.self_attn.o_proj.weight])[symbol:model.layers.9.self_attn.o_proj.weight]
        addr:0x00000000000043 prog.value_symbol() -> (%183:tensor<[7813120], UInt8, CPU>[@model.layers.9.mlp.gate_proj.weight])[symbol:model.layers.9.mlp.gate_proj.weight]
        addr:0x00000000000044 prog.value_symbol() -> (%406:tensor<[7813120], UInt8, CPU>[@model.layers.9.mlp.up_proj.weight])[symbol:model.layers.9.mlp.up_proj.weight]
        addr:0x00000000000045 prog.value_symbol() -> (%328:tensor<[7753728], UInt8, CPU>[@model.layers.9.mlp.down_proj.weight])[symbol:model.layers.9.mlp.down_proj.weight]
        addr:0x00000000000046 prog.value_symbol() -> (%545:tensor<[1339392], UInt8, CPU>[@model.layers.10.self_attn.q_proj.weight])[symbol:model.layers.10.self_attn.q_proj.weight]
        addr:0x00000000000047 prog.value_symbol() -> (%158:tensor<[223232], UInt8, CPU>[@model.layers.10.self_attn.k_proj.weight])[symbol:model.layers.10.self_attn.k_proj.weight]
        addr:0x00000000000048 prog.value_symbol() -> (%128:tensor<[223232], UInt8, CPU>[@model.layers.10.self_attn.v_proj.weight])[symbol:model.layers.10.self_attn.v_proj.weight]
        addr:0x00000000000049 prog.value_symbol() -> (%546:tensor<[1339392], UInt8, CPU>[@model.layers.10.self_attn.o_proj.weight])[symbol:model.layers.10.self_attn.o_proj.weight]
        addr:0x0000000000004a prog.value_symbol() -> (%175:tensor<[7813120], UInt8, CPU>[@model.layers.10.mlp.gate_proj.weight])[symbol:model.layers.10.mlp.gate_proj.weight]
        addr:0x0000000000004b prog.value_symbol() -> (%555:tensor<[7813120], UInt8, CPU>[@model.layers.10.mlp.up_proj.weight])[symbol:model.layers.10.mlp.up_proj.weight]
        addr:0x0000000000004c prog.value_symbol() -> (%188:tensor<[7753728], UInt8, CPU>[@model.layers.10.mlp.down_proj.weight])[symbol:model.layers.10.mlp.down_proj.weight]
        addr:0x0000000000004d prog.value_symbol() -> (%133:tensor<[1339392], UInt8, CPU>[@model.layers.11.self_attn.q_proj.weight])[symbol:model.layers.11.self_attn.q_proj.weight]
        addr:0x0000000000004e prog.value_symbol() -> (%498:tensor<[223232], UInt8, CPU>[@model.layers.11.self_attn.k_proj.weight])[symbol:model.layers.11.self_attn.k_proj.weight]
        addr:0x0000000000004f prog.value_symbol() -> (%550:tensor<[223232], UInt8, CPU>[@model.layers.11.self_attn.v_proj.weight])[symbol:model.layers.11.self_attn.v_proj.weight]
        addr:0x00000000000050 prog.value_symbol() -> (%143:tensor<[1339392], UInt8, CPU>[@model.layers.11.self_attn.o_proj.weight])[symbol:model.layers.11.self_attn.o_proj.weight]
        addr:0x00000000000051 prog.value_symbol() -> (%531:tensor<[7813120], UInt8, CPU>[@model.layers.11.mlp.gate_proj.weight])[symbol:model.layers.11.mlp.gate_proj.weight]
        addr:0x00000000000052 prog.value_symbol() -> (%169:tensor<[7813120], UInt8, CPU>[@model.layers.11.mlp.up_proj.weight])[symbol:model.layers.11.mlp.up_proj.weight]
        addr:0x00000000000053 prog.value_symbol() -> (%561:tensor<[7753728], UInt8, CPU>[@model.layers.11.mlp.down_proj.weight])[symbol:model.layers.11.mlp.down_proj.weight]
        addr:0x00000000000054 prog.value_symbol() -> (%132:tensor<[1339392], UInt8, CPU>[@model.layers.12.self_attn.q_proj.weight])[symbol:model.layers.12.self_attn.q_proj.weight]
        addr:0x00000000000055 prog.value_symbol() -> (%524:tensor<[223232], UInt8, CPU>[@model.layers.12.self_attn.k_proj.weight])[symbol:model.layers.12.self_attn.k_proj.weight]
        addr:0x00000000000056 prog.value_symbol() -> (%130:tensor<[223232], UInt8, CPU>[@model.layers.12.self_attn.v_proj.weight])[symbol:model.layers.12.self_attn.v_proj.weight]
        addr:0x00000000000057 prog.value_symbol() -> (%156:tensor<[1339392], UInt8, CPU>[@model.layers.12.self_attn.o_proj.weight])[symbol:model.layers.12.self_attn.o_proj.weight]
        addr:0x00000000000058 prog.value_symbol() -> (%182:tensor<[7813120], UInt8, CPU>[@model.layers.12.mlp.gate_proj.weight])[symbol:model.layers.12.mlp.gate_proj.weight]
        addr:0x00000000000059 prog.value_symbol() -> (%510:tensor<[7813120], UInt8, CPU>[@model.layers.12.mlp.up_proj.weight])[symbol:model.layers.12.mlp.up_proj.weight]
        addr:0x0000000000005a prog.value_symbol() -> (%198:tensor<[7753728], UInt8, CPU>[@model.layers.12.mlp.down_proj.weight])[symbol:model.layers.12.mlp.down_proj.weight]
        addr:0x0000000000005b prog.value_symbol() -> (%297:tensor<[1339392], UInt8, CPU>[@model.layers.13.self_attn.q_proj.weight])[symbol:model.layers.13.self_attn.q_proj.weight]
        addr:0x0000000000005c prog.value_symbol() -> (%157:tensor<[223232], UInt8, CPU>[@model.layers.13.self_attn.k_proj.weight])[symbol:model.layers.13.self_attn.k_proj.weight]
        addr:0x0000000000005d prog.value_symbol() -> (%124:tensor<[223232], UInt8, CPU>[@model.layers.13.self_attn.v_proj.weight])[symbol:model.layers.13.self_attn.v_proj.weight]
        addr:0x0000000000005e prog.value_symbol() -> (%487:tensor<[1339392], UInt8, CPU>[@model.layers.13.self_attn.o_proj.weight])[symbol:model.layers.13.self_attn.o_proj.weight]
        addr:0x0000000000005f prog.value_symbol() -> (%179:tensor<[7813120], UInt8, CPU>[@model.layers.13.mlp.gate_proj.weight])[symbol:model.layers.13.mlp.gate_proj.weight]
        addr:0x00000000000060 prog.value_symbol() -> (%423:tensor<[7813120], UInt8, CPU>[@model.layers.13.mlp.up_proj.weight])[symbol:model.layers.13.mlp.up_proj.weight]
        addr:0x00000000000061 prog.value_symbol() -> (%197:tensor<[7753728], UInt8, CPU>[@model.layers.13.mlp.down_proj.weight])[symbol:model.layers.13.mlp.down_proj.weight]
        addr:0x00000000000062 prog.value_symbol() -> (%512:tensor<[1339392], UInt8, CPU>[@model.layers.14.self_attn.q_proj.weight])[symbol:model.layers.14.self_attn.q_proj.weight]
        addr:0x00000000000063 prog.value_symbol() -> (%495:tensor<[223232], UInt8, CPU>[@model.layers.14.self_attn.k_proj.weight])[symbol:model.layers.14.self_attn.k_proj.weight]
        addr:0x00000000000064 prog.value_symbol() -> (%434:tensor<[223232], UInt8, CPU>[@model.layers.14.self_attn.v_proj.weight])[symbol:model.layers.14.self_attn.v_proj.weight]
        addr:0x00000000000065 prog.value_symbol() -> (%152:tensor<[1339392], UInt8, CPU>[@model.layers.14.self_attn.o_proj.weight])[symbol:model.layers.14.self_attn.o_proj.weight]
        addr:0x00000000000066 prog.value_symbol() -> (%437:tensor<[7813120], UInt8, CPU>[@model.layers.14.mlp.gate_proj.weight])[symbol:model.layers.14.mlp.gate_proj.weight]
        addr:0x00000000000067 prog.value_symbol() -> (%359:tensor<[7813120], UInt8, CPU>[@model.layers.14.mlp.up_proj.weight])[symbol:model.layers.14.mlp.up_proj.weight]
        addr:0x00000000000068 prog.value_symbol() -> (%192:tensor<[7753728], UInt8, CPU>[@model.layers.14.mlp.down_proj.weight])[symbol:model.layers.14.mlp.down_proj.weight]
        addr:0x00000000000069 prog.value_symbol() -> (%484:tensor<[1339392], UInt8, CPU>[@model.layers.15.self_attn.q_proj.weight])[symbol:model.layers.15.self_attn.q_proj.weight]
        addr:0x0000000000006a prog.value_symbol() -> (%389:tensor<[223232], UInt8, CPU>[@model.layers.15.self_attn.k_proj.weight])[symbol:model.layers.15.self_attn.k_proj.weight]
        addr:0x0000000000006b prog.value_symbol() -> (%481:tensor<[223232], UInt8, CPU>[@model.layers.15.self_attn.v_proj.weight])[symbol:model.layers.15.self_attn.v_proj.weight]
        addr:0x0000000000006c prog.value_symbol() -> (%146:tensor<[1339392], UInt8, CPU>[@model.layers.15.self_attn.o_proj.weight])[symbol:model.layers.15.self_attn.o_proj.weight]
        addr:0x0000000000006d prog.value_symbol() -> (%343:tensor<[7813120], UInt8, CPU>[@model.layers.15.mlp.gate_proj.weight])[symbol:model.layers.15.mlp.gate_proj.weight]
        addr:0x0000000000006e prog.value_symbol() -> (%414:tensor<[7813120], UInt8, CPU>[@model.layers.15.mlp.up_proj.weight])[symbol:model.layers.15.mlp.up_proj.weight]
        addr:0x0000000000006f prog.value_symbol() -> (%417:tensor<[7753728], UInt8, CPU>[@model.layers.15.mlp.down_proj.weight])[symbol:model.layers.15.mlp.down_proj.weight]
        addr:0x00000000000070 prog.value_symbol() -> (%134:tensor<[1339392], UInt8, CPU>[@model.layers.16.self_attn.q_proj.weight])[symbol:model.layers.16.self_attn.q_proj.weight]
        addr:0x00000000000071 prog.value_symbol() -> (%554:tensor<[223232], UInt8, CPU>[@model.layers.16.self_attn.k_proj.weight])[symbol:model.layers.16.self_attn.k_proj.weight]
        addr:0x00000000000072 prog.value_symbol() -> (%474:tensor<[223232], UInt8, CPU>[@model.layers.16.self_attn.v_proj.weight])[symbol:model.layers.16.self_attn.v_proj.weight]
        addr:0x00000000000073 prog.value_symbol() -> (%405:tensor<[1339392], UInt8, CPU>[@model.layers.16.self_attn.o_proj.weight])[symbol:model.layers.16.self_attn.o_proj.weight]
        addr:0x00000000000074 prog.value_symbol() -> (%463:tensor<[7813120], UInt8, CPU>[@model.layers.16.mlp.gate_proj.weight])[symbol:model.layers.16.mlp.gate_proj.weight]
        addr:0x00000000000075 prog.value_symbol() -> (%171:tensor<[7813120], UInt8, CPU>[@model.layers.16.mlp.up_proj.weight])[symbol:model.layers.16.mlp.up_proj.weight]
        addr:0x00000000000076 prog.value_symbol() -> (%432:tensor<[7753728], UInt8, CPU>[@model.layers.16.mlp.down_proj.weight])[symbol:model.layers.16.mlp.down_proj.weight]
        addr:0x00000000000077 prog.value_symbol() -> (%135:tensor<[1339392], UInt8, CPU>[@model.layers.17.self_attn.q_proj.weight])[symbol:model.layers.17.self_attn.q_proj.weight]
        addr:0x00000000000078 prog.value_symbol() -> (%345:tensor<[223232], UInt8, CPU>[@model.layers.17.self_attn.k_proj.weight])[symbol:model.layers.17.self_attn.k_proj.weight]
        addr:0x00000000000079 prog.value_symbol() -> (%412:tensor<[223232], UInt8, CPU>[@model.layers.17.self_attn.v_proj.weight])[symbol:model.layers.17.self_attn.v_proj.weight]
        addr:0x0000000000007a prog.value_symbol() -> (%446:tensor<[1339392], UInt8, CPU>[@model.layers.17.self_attn.o_proj.weight])[symbol:model.layers.17.self_attn.o_proj.weight]
        addr:0x0000000000007b prog.value_symbol() -> (%439:tensor<[7813120], UInt8, CPU>[@model.layers.17.mlp.gate_proj.weight])[symbol:model.layers.17.mlp.gate_proj.weight]
        addr:0x0000000000007c prog.value_symbol() -> (%469:tensor<[7813120], UInt8, CPU>[@model.layers.17.mlp.up_proj.weight])[symbol:model.layers.17.mlp.up_proj.weight]
        addr:0x0000000000007d prog.value_symbol() -> (%471:tensor<[7753728], UInt8, CPU>[@model.layers.17.mlp.down_proj.weight])[symbol:model.layers.17.mlp.down_proj.weight]
        addr:0x0000000000007e prog.value_symbol() -> (%454:tensor<[1339392], UInt8, CPU>[@model.layers.18.self_attn.q_proj.weight])[symbol:model.layers.18.self_attn.q_proj.weight]
        addr:0x0000000000007f prog.value_symbol() -> (%339:tensor<[223232], UInt8, CPU>[@model.layers.18.self_attn.k_proj.weight])[symbol:model.layers.18.self_attn.k_proj.weight]
        addr:0x00000000000080 prog.value_symbol() -> (%461:tensor<[223232], UInt8, CPU>[@model.layers.18.self_attn.v_proj.weight])[symbol:model.layers.18.self_attn.v_proj.weight]
        addr:0x00000000000081 prog.value_symbol() -> (%319:tensor<[1339392], UInt8, CPU>[@model.layers.18.self_attn.o_proj.weight])[symbol:model.layers.18.self_attn.o_proj.weight]
        addr:0x00000000000082 prog.value_symbol() -> (%457:tensor<[7813120], UInt8, CPU>[@model.layers.18.mlp.gate_proj.weight])[symbol:model.layers.18.mlp.gate_proj.weight]
        addr:0x00000000000083 prog.value_symbol() -> (%456:tensor<[7813120], UInt8, CPU>[@model.layers.18.mlp.up_proj.weight])[symbol:model.layers.18.mlp.up_proj.weight]
        addr:0x00000000000084 prog.value_symbol() -> (%191:tensor<[7753728], UInt8, CPU>[@model.layers.18.mlp.down_proj.weight])[symbol:model.layers.18.mlp.down_proj.weight]
        addr:0x00000000000085 prog.value_symbol() -> (%136:tensor<[1339392], UInt8, CPU>[@model.layers.19.self_attn.q_proj.weight])[symbol:model.layers.19.self_attn.q_proj.weight]
        addr:0x00000000000086 prog.value_symbol() -> (%445:tensor<[223232], UInt8, CPU>[@model.layers.19.self_attn.k_proj.weight])[symbol:model.layers.19.self_attn.k_proj.weight]
        addr:0x00000000000087 prog.value_symbol() -> (%126:tensor<[223232], UInt8, CPU>[@model.layers.19.self_attn.v_proj.weight])[symbol:model.layers.19.self_attn.v_proj.weight]
        addr:0x00000000000088 prog.value_symbol() -> (%444:tensor<[1339392], UInt8, CPU>[@model.layers.19.self_attn.o_proj.weight])[symbol:model.layers.19.self_attn.o_proj.weight]
        addr:0x00000000000089 prog.value_symbol() -> (%176:tensor<[7813120], UInt8, CPU>[@model.layers.19.mlp.gate_proj.weight])[symbol:model.layers.19.mlp.gate_proj.weight]
        addr:0x0000000000008a prog.value_symbol() -> (%451:tensor<[7813120], UInt8, CPU>[@model.layers.19.mlp.up_proj.weight])[symbol:model.layers.19.mlp.up_proj.weight]
        addr:0x0000000000008b prog.value_symbol() -> (%189:tensor<[7753728], UInt8, CPU>[@model.layers.19.mlp.down_proj.weight])[symbol:model.layers.19.mlp.down_proj.weight]
        addr:0x0000000000008c prog.value_symbol() -> (%140:tensor<[1339392], UInt8, CPU>[@model.layers.20.self_attn.q_proj.weight])[symbol:model.layers.20.self_attn.q_proj.weight]
        addr:0x0000000000008d prog.value_symbol() -> (%491:tensor<[223232], UInt8, CPU>[@model.layers.20.self_attn.k_proj.weight])[symbol:model.layers.20.self_attn.k_proj.weight]
        addr:0x0000000000008e prog.value_symbol() -> (%553:tensor<[223232], UInt8, CPU>[@model.layers.20.self_attn.v_proj.weight])[symbol:model.layers.20.self_attn.v_proj.weight]
        addr:0x0000000000008f prog.value_symbol() -> (%569:tensor<[1339392], UInt8, CPU>[@model.layers.20.self_attn.o_proj.weight])[symbol:model.layers.20.self_attn.o_proj.weight]
        addr:0x00000000000090 prog.value_symbol() -> (%181:tensor<[7813120], UInt8, CPU>[@model.layers.20.mlp.gate_proj.weight])[symbol:model.layers.20.mlp.gate_proj.weight]
        addr:0x00000000000091 prog.value_symbol() -> (%548:tensor<[7813120], UInt8, CPU>[@model.layers.20.mlp.up_proj.weight])[symbol:model.layers.20.mlp.up_proj.weight]
        addr:0x00000000000092 prog.value_symbol() -> (%413:tensor<[7753728], UInt8, CPU>[@model.layers.20.mlp.down_proj.weight])[symbol:model.layers.20.mlp.down_proj.weight]
        addr:0x00000000000093 prog.value_symbol() -> (%541:tensor<[1339392], UInt8, CPU>[@model.layers.21.self_attn.q_proj.weight])[symbol:model.layers.21.self_attn.q_proj.weight]
        addr:0x00000000000094 prog.value_symbol() -> (%162:tensor<[223232], UInt8, CPU>[@model.layers.21.self_attn.k_proj.weight])[symbol:model.layers.21.self_attn.k_proj.weight]
        addr:0x00000000000095 prog.value_symbol() -> (%125:tensor<[223232], UInt8, CPU>[@model.layers.21.self_attn.v_proj.weight])[symbol:model.layers.21.self_attn.v_proj.weight]
        addr:0x00000000000096 prog.value_symbol() -> (%419:tensor<[1339392], UInt8, CPU>[@model.layers.21.self_attn.o_proj.weight])[symbol:model.layers.21.self_attn.o_proj.weight]
        addr:0x00000000000097 prog.value_symbol() -> (%186:tensor<[7813120], UInt8, CPU>[@model.layers.21.mlp.gate_proj.weight])[symbol:model.layers.21.mlp.gate_proj.weight]
        addr:0x00000000000098 prog.value_symbol() -> (%368:tensor<[7813120], UInt8, CPU>[@model.layers.21.mlp.up_proj.weight])[symbol:model.layers.21.mlp.up_proj.weight]
        addr:0x00000000000099 prog.value_symbol() -> (%478:tensor<[7753728], UInt8, CPU>[@model.layers.21.mlp.down_proj.weight])[symbol:model.layers.21.mlp.down_proj.weight]
        addr:0x0000000000009a prog.value_symbol() -> (%494:tensor<[1339392], UInt8, CPU>[@model.layers.22.self_attn.q_proj.weight])[symbol:model.layers.22.self_attn.q_proj.weight]
        addr:0x0000000000009b prog.value_symbol() -> (%420:tensor<[223232], UInt8, CPU>[@model.layers.22.self_attn.k_proj.weight])[symbol:model.layers.22.self_attn.k_proj.weight]
        addr:0x0000000000009c prog.value_symbol() -> (%404:tensor<[223232], UInt8, CPU>[@model.layers.22.self_attn.v_proj.weight])[symbol:model.layers.22.self_attn.v_proj.weight]
        addr:0x0000000000009d prog.value_symbol() -> (%475:tensor<[1339392], UInt8, CPU>[@model.layers.22.self_attn.o_proj.weight])[symbol:model.layers.22.self_attn.o_proj.weight]
        addr:0x0000000000009e prog.value_symbol() -> (%178:tensor<[7813120], UInt8, CPU>[@model.layers.22.mlp.gate_proj.weight])[symbol:model.layers.22.mlp.gate_proj.weight]
        addr:0x0000000000009f prog.value_symbol() -> (%410:tensor<[7813120], UInt8, CPU>[@model.layers.22.mlp.up_proj.weight])[symbol:model.layers.22.mlp.up_proj.weight]
        addr:0x000000000000a0 prog.value_symbol() -> (%415:tensor<[7753728], UInt8, CPU>[@model.layers.22.mlp.down_proj.weight])[symbol:model.layers.22.mlp.down_proj.weight]
        addr:0x000000000000a1 prog.value_symbol() -> (%393:tensor<[1339392], UInt8, CPU>[@model.layers.23.self_attn.q_proj.weight])[symbol:model.layers.23.self_attn.q_proj.weight]
        addr:0x000000000000a2 prog.value_symbol() -> (%163:tensor<[223232], UInt8, CPU>[@model.layers.23.self_attn.k_proj.weight])[symbol:model.layers.23.self_attn.k_proj.weight]
        addr:0x000000000000a3 prog.value_symbol() -> (%362:tensor<[223232], UInt8, CPU>[@model.layers.23.self_attn.v_proj.weight])[symbol:model.layers.23.self_attn.v_proj.weight]
        addr:0x000000000000a4 prog.value_symbol() -> (%460:tensor<[1339392], UInt8, CPU>[@model.layers.23.self_attn.o_proj.weight])[symbol:model.layers.23.self_attn.o_proj.weight]
        addr:0x000000000000a5 prog.value_symbol() -> (%357:tensor<[7813120], UInt8, CPU>[@model.layers.23.mlp.gate_proj.weight])[symbol:model.layers.23.mlp.gate_proj.weight]
        addr:0x000000000000a6 prog.value_symbol() -> (%166:tensor<[7813120], UInt8, CPU>[@model.layers.23.mlp.up_proj.weight])[symbol:model.layers.23.mlp.up_proj.weight]
        addr:0x000000000000a7 prog.value_symbol() -> (%193:tensor<[7753728], UInt8, CPU>[@model.layers.23.mlp.down_proj.weight])[symbol:model.layers.23.mlp.down_proj.weight]
        addr:0x000000000000a8 prog.value_symbol() -> (%141:tensor<[1339392], UInt8, CPU>[@model.layers.24.self_attn.q_proj.weight])[symbol:model.layers.24.self_attn.q_proj.weight]
        addr:0x000000000000a9 prog.value_symbol() -> (%452:tensor<[223232], UInt8, CPU>[@model.layers.24.self_attn.k_proj.weight])[symbol:model.layers.24.self_attn.k_proj.weight]
        addr:0x000000000000aa prog.value_symbol() -> (%477:tensor<[223232], UInt8, CPU>[@model.layers.24.self_attn.v_proj.weight])[symbol:model.layers.24.self_attn.v_proj.weight]
        addr:0x000000000000ab prog.value_symbol() -> (%485:tensor<[1339392], UInt8, CPU>[@model.layers.24.self_attn.o_proj.weight])[symbol:model.layers.24.self_attn.o_proj.weight]
        addr:0x000000000000ac prog.value_symbol() -> (%180:tensor<[7813120], UInt8, CPU>[@model.layers.24.mlp.gate_proj.weight])[symbol:model.layers.24.mlp.gate_proj.weight]
        addr:0x000000000000ad prog.value_symbol() -> (%411:tensor<[7813120], UInt8, CPU>[@model.layers.24.mlp.up_proj.weight])[symbol:model.layers.24.mlp.up_proj.weight]
        addr:0x000000000000ae prog.value_symbol() -> (%196:tensor<[7753728], UInt8, CPU>[@model.layers.24.mlp.down_proj.weight])[symbol:model.layers.24.mlp.down_proj.weight]
        addr:0x000000000000af prog.value_symbol() -> (%325:tensor<[1339392], UInt8, CPU>[@model.layers.25.self_attn.q_proj.weight])[symbol:model.layers.25.self_attn.q_proj.weight]
        addr:0x000000000000b0 prog.value_symbol() -> (%385:tensor<[223232], UInt8, CPU>[@model.layers.25.self_attn.k_proj.weight])[symbol:model.layers.25.self_attn.k_proj.weight]
        addr:0x000000000000b1 prog.value_symbol() -> (%381:tensor<[223232], UInt8, CPU>[@model.layers.25.self_attn.v_proj.weight])[symbol:model.layers.25.self_attn.v_proj.weight]
        addr:0x000000000000b2 prog.value_symbol() -> (%150:tensor<[1339392], UInt8, CPU>[@model.layers.25.self_attn.o_proj.weight])[symbol:model.layers.25.self_attn.o_proj.weight]
        addr:0x000000000000b3 prog.value_symbol() -> (%382:tensor<[7813120], UInt8, CPU>[@model.layers.25.mlp.gate_proj.weight])[symbol:model.layers.25.mlp.gate_proj.weight]
        addr:0x000000000000b4 prog.value_symbol() -> (%337:tensor<[7813120], UInt8, CPU>[@model.layers.25.mlp.up_proj.weight])[symbol:model.layers.25.mlp.up_proj.weight]
        addr:0x000000000000b5 prog.value_symbol() -> (%387:tensor<[7753728], UInt8, CPU>[@model.layers.25.mlp.down_proj.weight])[symbol:model.layers.25.mlp.down_proj.weight]
        addr:0x000000000000b6 prog.value_symbol() -> (%327:tensor<[1339392], UInt8, CPU>[@model.layers.26.self_attn.q_proj.weight])[symbol:model.layers.26.self_attn.q_proj.weight]
        addr:0x000000000000b7 prog.value_symbol() -> (%321:tensor<[223232], UInt8, CPU>[@model.layers.26.self_attn.k_proj.weight])[symbol:model.layers.26.self_attn.k_proj.weight]
        addr:0x000000000000b8 prog.value_symbol() -> (%391:tensor<[223232], UInt8, CPU>[@model.layers.26.self_attn.v_proj.weight])[symbol:model.layers.26.self_attn.v_proj.weight]
        addr:0x000000000000b9 prog.value_symbol() -> (%155:tensor<[1339392], UInt8, CPU>[@model.layers.26.self_attn.o_proj.weight])[symbol:model.layers.26.self_attn.o_proj.weight]
        addr:0x000000000000ba prog.value_symbol() -> (%301:tensor<[7813120], UInt8, CPU>[@model.layers.26.mlp.gate_proj.weight])[symbol:model.layers.26.mlp.gate_proj.weight]
        addr:0x000000000000bb prog.value_symbol() -> (%377:tensor<[7813120], UInt8, CPU>[@model.layers.26.mlp.up_proj.weight])[symbol:model.layers.26.mlp.up_proj.weight]
        addr:0x000000000000bc prog.value_symbol() -> (%190:tensor<[7753728], UInt8, CPU>[@model.layers.26.mlp.down_proj.weight])[symbol:model.layers.26.mlp.down_proj.weight]
        addr:0x000000000000bd prog.value_symbol() -> (%367:tensor<[1339392], UInt8, CPU>[@model.layers.27.self_attn.q_proj.weight])[symbol:model.layers.27.self_attn.q_proj.weight]
        addr:0x000000000000be prog.value_symbol() -> (%358:tensor<[223232], UInt8, CPU>[@model.layers.27.self_attn.k_proj.weight])[symbol:model.layers.27.self_attn.k_proj.weight]
        addr:0x000000000000bf prog.value_symbol() -> (%526:tensor<[223232], UInt8, CPU>[@model.layers.27.self_attn.v_proj.weight])[symbol:model.layers.27.self_attn.v_proj.weight]
        addr:0x000000000000c0 prog.value_symbol() -> (%151:tensor<[1339392], UInt8, CPU>[@model.layers.27.self_attn.o_proj.weight])[symbol:model.layers.27.self_attn.o_proj.weight]
        addr:0x000000000000c1 prog.value_symbol() -> (%177:tensor<[7813120], UInt8, CPU>[@model.layers.27.mlp.gate_proj.weight])[symbol:model.layers.27.mlp.gate_proj.weight]
        addr:0x000000000000c2 prog.value_symbol() -> (%172:tensor<[7813120], UInt8, CPU>[@model.layers.27.mlp.up_proj.weight])[symbol:model.layers.27.mlp.up_proj.weight]
        addr:0x000000000000c3 prog.value_symbol() -> (%195:tensor<[7753728], UInt8, CPU>[@model.layers.27.mlp.down_proj.weight])[symbol:model.layers.27.mlp.down_proj.weight]
        addr:0x000000000000c4 prog.value_symbol() -> (%464:tensor<[132488192], UInt8, CPU>[@model.lm_head.weight])[symbol:model.lm_head.weight]
        addr:0x000000000000c5 prog.kernel_symbol[symbol:model.norm, op_type:RMSNorm, op_options:{"add_unit_offset":false,"epsilon":9.999999974752427e-07}]
        addr:0x000000000000c6 prog.kernel_symbol[symbol:model.lm_head, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":151936}]
        addr:0x000000000000c7 prog.kernel_symbol[symbol:model.layers.0.input_layernorm, op_type:RMSNorm, op_options:{"add_unit_offset":false,"epsilon":9.999999974752427e-07}]
        addr:0x000000000000c8 prog.kernel_symbol[symbol:model.layers.0.post_attention_layernorm, op_type:RMSNorm, op_options:{"add_unit_offset":false,"epsilon":9.999999974752427e-07}]
        addr:0x000000000000c9 prog.kernel_symbol[symbol:model.layers.0.self_attn.q_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x000000000000ca prog.kernel_symbol[symbol:model.layers.0.self_attn.k_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x000000000000cb prog.kernel_symbol[symbol:model.layers.0.self_attn.v_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x000000000000cc prog.kernel_symbol[symbol:model.layers.0.self_attn.q_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x000000000000cd prog.kernel_symbol[symbol:model.layers.0.self_attn.k_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x000000000000ce prog.kernel_symbol[symbol:model.layers.0.self_attn.kv_cache, op_type:KVCache, op_options:{"head_dim":128,"kv_head":2,"layer_idx":0,"q_head":12,"use_fa2":false}]
        addr:0x000000000000cf prog.kernel_symbol[symbol:model.layers.0.self_attn.mask, op_type:CausalMask, op_options:{"sliding_window":false,"window_size":0}]
        addr:0x000000000000d0 prog.kernel_symbol[symbol:model.layers.0.self_attn.softmax, op_type:Softmax, op_options:{"axis":-1}]
        addr:0x000000000000d1 prog.kernel_symbol[symbol:model.layers.0.self_attn.o_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x000000000000d2 prog.kernel_symbol[symbol:model.layers.0.mlp.gate_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x000000000000d3 prog.kernel_symbol[symbol:model.layers.0.mlp.act, op_type:SiLU, op_options:null]
        addr:0x000000000000d4 prog.kernel_symbol[symbol:model.layers.0.mlp.up_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x000000000000d5 prog.kernel_symbol[symbol:model.layers.0.mlp.down_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":8960,"out_channels":1536}]
        addr:0x000000000000d6 prog.kernel_symbol[symbol:model.layers.1.input_layernorm, op_type:RMSNorm, op_options:{"add_unit_offset":false,"epsilon":9.999999974752427e-07}]
        addr:0x000000000000d7 prog.kernel_symbol[symbol:model.layers.1.post_attention_layernorm, op_type:RMSNorm, op_options:{"add_unit_offset":false,"epsilon":9.999999974752427e-07}]
        addr:0x000000000000d8 prog.kernel_symbol[symbol:model.layers.1.self_attn.q_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x000000000000d9 prog.kernel_symbol[symbol:model.layers.1.self_attn.k_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x000000000000da prog.kernel_symbol[symbol:model.layers.1.self_attn.v_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x000000000000db prog.kernel_symbol[symbol:model.layers.1.self_attn.q_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x000000000000dc prog.kernel_symbol[symbol:model.layers.1.self_attn.k_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x000000000000dd prog.kernel_symbol[symbol:model.layers.1.self_attn.kv_cache, op_type:KVCache, op_options:{"head_dim":128,"kv_head":2,"layer_idx":0,"q_head":12,"use_fa2":false}]
        addr:0x000000000000de prog.kernel_symbol[symbol:model.layers.1.self_attn.mask, op_type:CausalMask, op_options:{"sliding_window":false,"window_size":0}]
        addr:0x000000000000df prog.kernel_symbol[symbol:model.layers.1.self_attn.softmax, op_type:Softmax, op_options:{"axis":-1}]
        addr:0x000000000000e0 prog.kernel_symbol[symbol:model.layers.1.self_attn.o_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x000000000000e1 prog.kernel_symbol[symbol:model.layers.1.mlp.gate_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x000000000000e2 prog.kernel_symbol[symbol:model.layers.1.mlp.act, op_type:SiLU, op_options:null]
        addr:0x000000000000e3 prog.kernel_symbol[symbol:model.layers.1.mlp.up_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x000000000000e4 prog.kernel_symbol[symbol:model.layers.1.mlp.down_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":8960,"out_channels":1536}]
        addr:0x000000000000e5 prog.kernel_symbol[symbol:model.layers.2.input_layernorm, op_type:RMSNorm, op_options:{"add_unit_offset":false,"epsilon":9.999999974752427e-07}]
        addr:0x000000000000e6 prog.kernel_symbol[symbol:model.layers.2.post_attention_layernorm, op_type:RMSNorm, op_options:{"add_unit_offset":false,"epsilon":9.999999974752427e-07}]
        addr:0x000000000000e7 prog.kernel_symbol[symbol:model.layers.2.self_attn.q_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x000000000000e8 prog.kernel_symbol[symbol:model.layers.2.self_attn.k_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x000000000000e9 prog.kernel_symbol[symbol:model.layers.2.self_attn.v_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x000000000000ea prog.kernel_symbol[symbol:model.layers.2.self_attn.q_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x000000000000eb prog.kernel_symbol[symbol:model.layers.2.self_attn.k_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x000000000000ec prog.kernel_symbol[symbol:model.layers.2.self_attn.kv_cache, op_type:KVCache, op_options:{"head_dim":128,"kv_head":2,"layer_idx":0,"q_head":12,"use_fa2":false}]
        addr:0x000000000000ed prog.kernel_symbol[symbol:model.layers.2.self_attn.mask, op_type:CausalMask, op_options:{"sliding_window":false,"window_size":0}]
        addr:0x000000000000ee prog.kernel_symbol[symbol:model.layers.2.self_attn.softmax, op_type:Softmax, op_options:{"axis":-1}]
        addr:0x000000000000ef prog.kernel_symbol[symbol:model.layers.2.self_attn.o_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x000000000000f0 prog.kernel_symbol[symbol:model.layers.2.mlp.gate_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x000000000000f1 prog.kernel_symbol[symbol:model.layers.2.mlp.act, op_type:SiLU, op_options:null]
        addr:0x000000000000f2 prog.kernel_symbol[symbol:model.layers.2.mlp.up_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x000000000000f3 prog.kernel_symbol[symbol:model.layers.2.mlp.down_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":8960,"out_channels":1536}]
        addr:0x000000000000f4 prog.kernel_symbol[symbol:model.layers.3.input_layernorm, op_type:RMSNorm, op_options:{"add_unit_offset":false,"epsilon":9.999999974752427e-07}]
        addr:0x000000000000f5 prog.kernel_symbol[symbol:model.layers.3.post_attention_layernorm, op_type:RMSNorm, op_options:{"add_unit_offset":false,"epsilon":9.999999974752427e-07}]
        addr:0x000000000000f6 prog.kernel_symbol[symbol:model.layers.3.self_attn.q_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x000000000000f7 prog.kernel_symbol[symbol:model.layers.3.self_attn.k_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x000000000000f8 prog.kernel_symbol[symbol:model.layers.3.self_attn.v_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x000000000000f9 prog.kernel_symbol[symbol:model.layers.3.self_attn.q_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x000000000000fa prog.kernel_symbol[symbol:model.layers.3.self_attn.k_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x000000000000fb prog.kernel_symbol[symbol:model.layers.3.self_attn.kv_cache, op_type:KVCache, op_options:{"head_dim":128,"kv_head":2,"layer_idx":0,"q_head":12,"use_fa2":false}]
        addr:0x000000000000fc prog.kernel_symbol[symbol:model.layers.3.self_attn.mask, op_type:CausalMask, op_options:{"sliding_window":false,"window_size":0}]
        addr:0x000000000000fd prog.kernel_symbol[symbol:model.layers.3.self_attn.softmax, op_type:Softmax, op_options:{"axis":-1}]
        addr:0x000000000000fe prog.kernel_symbol[symbol:model.layers.3.self_attn.o_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x000000000000ff prog.kernel_symbol[symbol:model.layers.3.mlp.gate_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x00000000000100 prog.kernel_symbol[symbol:model.layers.3.mlp.act, op_type:SiLU, op_options:null]
        addr:0x00000000000101 prog.kernel_symbol[symbol:model.layers.3.mlp.up_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x00000000000102 prog.kernel_symbol[symbol:model.layers.3.mlp.down_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":8960,"out_channels":1536}]
        addr:0x00000000000103 prog.kernel_symbol[symbol:model.layers.4.input_layernorm, op_type:RMSNorm, op_options:{"add_unit_offset":false,"epsilon":9.999999974752427e-07}]
        addr:0x00000000000104 prog.kernel_symbol[symbol:model.layers.4.post_attention_layernorm, op_type:RMSNorm, op_options:{"add_unit_offset":false,"epsilon":9.999999974752427e-07}]
        addr:0x00000000000105 prog.kernel_symbol[symbol:model.layers.4.self_attn.q_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x00000000000106 prog.kernel_symbol[symbol:model.layers.4.self_attn.k_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x00000000000107 prog.kernel_symbol[symbol:model.layers.4.self_attn.v_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x00000000000108 prog.kernel_symbol[symbol:model.layers.4.self_attn.q_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x00000000000109 prog.kernel_symbol[symbol:model.layers.4.self_attn.k_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x0000000000010a prog.kernel_symbol[symbol:model.layers.4.self_attn.kv_cache, op_type:KVCache, op_options:{"head_dim":128,"kv_head":2,"layer_idx":0,"q_head":12,"use_fa2":false}]
        addr:0x0000000000010b prog.kernel_symbol[symbol:model.layers.4.self_attn.mask, op_type:CausalMask, op_options:{"sliding_window":false,"window_size":0}]
        addr:0x0000000000010c prog.kernel_symbol[symbol:model.layers.4.self_attn.softmax, op_type:Softmax, op_options:{"axis":-1}]
        addr:0x0000000000010d prog.kernel_symbol[symbol:model.layers.4.self_attn.o_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x0000000000010e prog.kernel_symbol[symbol:model.layers.4.mlp.gate_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x0000000000010f prog.kernel_symbol[symbol:model.layers.4.mlp.act, op_type:SiLU, op_options:null]
        addr:0x00000000000110 prog.kernel_symbol[symbol:model.layers.4.mlp.up_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x00000000000111 prog.kernel_symbol[symbol:model.layers.4.mlp.down_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":8960,"out_channels":1536}]
        addr:0x00000000000112 prog.kernel_symbol[symbol:model.layers.5.input_layernorm, op_type:RMSNorm, op_options:{"add_unit_offset":false,"epsilon":9.999999974752427e-07}]
        addr:0x00000000000113 prog.kernel_symbol[symbol:model.layers.5.post_attention_layernorm, op_type:RMSNorm, op_options:{"add_unit_offset":false,"epsilon":9.999999974752427e-07}]
        addr:0x00000000000114 prog.kernel_symbol[symbol:model.layers.5.self_attn.q_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x00000000000115 prog.kernel_symbol[symbol:model.layers.5.self_attn.k_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x00000000000116 prog.kernel_symbol[symbol:model.layers.5.self_attn.v_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x00000000000117 prog.kernel_symbol[symbol:model.layers.5.self_attn.q_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x00000000000118 prog.kernel_symbol[symbol:model.layers.5.self_attn.k_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x00000000000119 prog.kernel_symbol[symbol:model.layers.5.self_attn.kv_cache, op_type:KVCache, op_options:{"head_dim":128,"kv_head":2,"layer_idx":0,"q_head":12,"use_fa2":false}]
        addr:0x0000000000011a prog.kernel_symbol[symbol:model.layers.5.self_attn.mask, op_type:CausalMask, op_options:{"sliding_window":false,"window_size":0}]
        addr:0x0000000000011b prog.kernel_symbol[symbol:model.layers.5.self_attn.softmax, op_type:Softmax, op_options:{"axis":-1}]
        addr:0x0000000000011c prog.kernel_symbol[symbol:model.layers.5.self_attn.o_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x0000000000011d prog.kernel_symbol[symbol:model.layers.5.mlp.gate_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x0000000000011e prog.kernel_symbol[symbol:model.layers.5.mlp.act, op_type:SiLU, op_options:null]
        addr:0x0000000000011f prog.kernel_symbol[symbol:model.layers.5.mlp.up_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x00000000000120 prog.kernel_symbol[symbol:model.layers.5.mlp.down_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":8960,"out_channels":1536}]
        addr:0x00000000000121 prog.kernel_symbol[symbol:model.layers.6.input_layernorm, op_type:RMSNorm, op_options:{"add_unit_offset":false,"epsilon":9.999999974752427e-07}]
        addr:0x00000000000122 prog.kernel_symbol[symbol:model.layers.6.post_attention_layernorm, op_type:RMSNorm, op_options:{"add_unit_offset":false,"epsilon":9.999999974752427e-07}]
        addr:0x00000000000123 prog.kernel_symbol[symbol:model.layers.6.self_attn.q_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x00000000000124 prog.kernel_symbol[symbol:model.layers.6.self_attn.k_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x00000000000125 prog.kernel_symbol[symbol:model.layers.6.self_attn.v_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x00000000000126 prog.kernel_symbol[symbol:model.layers.6.self_attn.q_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x00000000000127 prog.kernel_symbol[symbol:model.layers.6.self_attn.k_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x00000000000128 prog.kernel_symbol[symbol:model.layers.6.self_attn.kv_cache, op_type:KVCache, op_options:{"head_dim":128,"kv_head":2,"layer_idx":0,"q_head":12,"use_fa2":false}]
        addr:0x00000000000129 prog.kernel_symbol[symbol:model.layers.6.self_attn.mask, op_type:CausalMask, op_options:{"sliding_window":false,"window_size":0}]
        addr:0x0000000000012a prog.kernel_symbol[symbol:model.layers.6.self_attn.softmax, op_type:Softmax, op_options:{"axis":-1}]
        addr:0x0000000000012b prog.kernel_symbol[symbol:model.layers.6.self_attn.o_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x0000000000012c prog.kernel_symbol[symbol:model.layers.6.mlp.gate_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x0000000000012d prog.kernel_symbol[symbol:model.layers.6.mlp.act, op_type:SiLU, op_options:null]
        addr:0x0000000000012e prog.kernel_symbol[symbol:model.layers.6.mlp.up_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x0000000000012f prog.kernel_symbol[symbol:model.layers.6.mlp.down_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":8960,"out_channels":1536}]
        addr:0x00000000000130 prog.kernel_symbol[symbol:model.layers.7.input_layernorm, op_type:RMSNorm, op_options:{"add_unit_offset":false,"epsilon":9.999999974752427e-07}]
        addr:0x00000000000131 prog.kernel_symbol[symbol:model.layers.7.post_attention_layernorm, op_type:RMSNorm, op_options:{"add_unit_offset":false,"epsilon":9.999999974752427e-07}]
        addr:0x00000000000132 prog.kernel_symbol[symbol:model.layers.7.self_attn.q_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x00000000000133 prog.kernel_symbol[symbol:model.layers.7.self_attn.k_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x00000000000134 prog.kernel_symbol[symbol:model.layers.7.self_attn.v_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x00000000000135 prog.kernel_symbol[symbol:model.layers.7.self_attn.q_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x00000000000136 prog.kernel_symbol[symbol:model.layers.7.self_attn.k_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x00000000000137 prog.kernel_symbol[symbol:model.layers.7.self_attn.kv_cache, op_type:KVCache, op_options:{"head_dim":128,"kv_head":2,"layer_idx":0,"q_head":12,"use_fa2":false}]
        addr:0x00000000000138 prog.kernel_symbol[symbol:model.layers.7.self_attn.mask, op_type:CausalMask, op_options:{"sliding_window":false,"window_size":0}]
        addr:0x00000000000139 prog.kernel_symbol[symbol:model.layers.7.self_attn.softmax, op_type:Softmax, op_options:{"axis":-1}]
        addr:0x0000000000013a prog.kernel_symbol[symbol:model.layers.7.self_attn.o_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x0000000000013b prog.kernel_symbol[symbol:model.layers.7.mlp.gate_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x0000000000013c prog.kernel_symbol[symbol:model.layers.7.mlp.act, op_type:SiLU, op_options:null]
        addr:0x0000000000013d prog.kernel_symbol[symbol:model.layers.7.mlp.up_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x0000000000013e prog.kernel_symbol[symbol:model.layers.7.mlp.down_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":8960,"out_channels":1536}]
        addr:0x0000000000013f prog.kernel_symbol[symbol:model.layers.8.input_layernorm, op_type:RMSNorm, op_options:{"add_unit_offset":false,"epsilon":9.999999974752427e-07}]
        addr:0x00000000000140 prog.kernel_symbol[symbol:model.layers.8.post_attention_layernorm, op_type:RMSNorm, op_options:{"add_unit_offset":false,"epsilon":9.999999974752427e-07}]
        addr:0x00000000000141 prog.kernel_symbol[symbol:model.layers.8.self_attn.q_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x00000000000142 prog.kernel_symbol[symbol:model.layers.8.self_attn.k_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x00000000000143 prog.kernel_symbol[symbol:model.layers.8.self_attn.v_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x00000000000144 prog.kernel_symbol[symbol:model.layers.8.self_attn.q_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x00000000000145 prog.kernel_symbol[symbol:model.layers.8.self_attn.k_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x00000000000146 prog.kernel_symbol[symbol:model.layers.8.self_attn.kv_cache, op_type:KVCache, op_options:{"head_dim":128,"kv_head":2,"layer_idx":0,"q_head":12,"use_fa2":false}]
        addr:0x00000000000147 prog.kernel_symbol[symbol:model.layers.8.self_attn.mask, op_type:CausalMask, op_options:{"sliding_window":false,"window_size":0}]
        addr:0x00000000000148 prog.kernel_symbol[symbol:model.layers.8.self_attn.softmax, op_type:Softmax, op_options:{"axis":-1}]
        addr:0x00000000000149 prog.kernel_symbol[symbol:model.layers.8.self_attn.o_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x0000000000014a prog.kernel_symbol[symbol:model.layers.8.mlp.gate_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x0000000000014b prog.kernel_symbol[symbol:model.layers.8.mlp.act, op_type:SiLU, op_options:null]
        addr:0x0000000000014c prog.kernel_symbol[symbol:model.layers.8.mlp.up_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x0000000000014d prog.kernel_symbol[symbol:model.layers.8.mlp.down_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":8960,"out_channels":1536}]
        addr:0x0000000000014e prog.kernel_symbol[symbol:model.layers.9.input_layernorm, op_type:RMSNorm, op_options:{"add_unit_offset":false,"epsilon":9.999999974752427e-07}]
        addr:0x0000000000014f prog.kernel_symbol[symbol:model.layers.9.post_attention_layernorm, op_type:RMSNorm, op_options:{"add_unit_offset":false,"epsilon":9.999999974752427e-07}]
        addr:0x00000000000150 prog.kernel_symbol[symbol:model.layers.9.self_attn.q_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x00000000000151 prog.kernel_symbol[symbol:model.layers.9.self_attn.k_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x00000000000152 prog.kernel_symbol[symbol:model.layers.9.self_attn.v_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x00000000000153 prog.kernel_symbol[symbol:model.layers.9.self_attn.q_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x00000000000154 prog.kernel_symbol[symbol:model.layers.9.self_attn.k_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x00000000000155 prog.kernel_symbol[symbol:model.layers.9.self_attn.kv_cache, op_type:KVCache, op_options:{"head_dim":128,"kv_head":2,"layer_idx":0,"q_head":12,"use_fa2":false}]
        addr:0x00000000000156 prog.kernel_symbol[symbol:model.layers.9.self_attn.mask, op_type:CausalMask, op_options:{"sliding_window":false,"window_size":0}]
        addr:0x00000000000157 prog.kernel_symbol[symbol:model.layers.9.self_attn.softmax, op_type:Softmax, op_options:{"axis":-1}]
        addr:0x00000000000158 prog.kernel_symbol[symbol:model.layers.9.self_attn.o_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x00000000000159 prog.kernel_symbol[symbol:model.layers.9.mlp.gate_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x0000000000015a prog.kernel_symbol[symbol:model.layers.9.mlp.act, op_type:SiLU, op_options:null]
        addr:0x0000000000015b prog.kernel_symbol[symbol:model.layers.9.mlp.up_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x0000000000015c prog.kernel_symbol[symbol:model.layers.9.mlp.down_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":8960,"out_channels":1536}]
        addr:0x0000000000015d prog.kernel_symbol[symbol:model.layers.10.input_layernorm, op_type:RMSNorm, op_options:{"add_unit_offset":false,"epsilon":9.999999974752427e-07}]
        addr:0x0000000000015e prog.kernel_symbol[symbol:model.layers.10.post_attention_layernorm, op_type:RMSNorm, op_options:{"add_unit_offset":false,"epsilon":9.999999974752427e-07}]
        addr:0x0000000000015f prog.kernel_symbol[symbol:model.layers.10.self_attn.q_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x00000000000160 prog.kernel_symbol[symbol:model.layers.10.self_attn.k_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x00000000000161 prog.kernel_symbol[symbol:model.layers.10.self_attn.v_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x00000000000162 prog.kernel_symbol[symbol:model.layers.10.self_attn.q_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x00000000000163 prog.kernel_symbol[symbol:model.layers.10.self_attn.k_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x00000000000164 prog.kernel_symbol[symbol:model.layers.10.self_attn.kv_cache, op_type:KVCache, op_options:{"head_dim":128,"kv_head":2,"layer_idx":0,"q_head":12,"use_fa2":false}]
        addr:0x00000000000165 prog.kernel_symbol[symbol:model.layers.10.self_attn.mask, op_type:CausalMask, op_options:{"sliding_window":false,"window_size":0}]
        addr:0x00000000000166 prog.kernel_symbol[symbol:model.layers.10.self_attn.softmax, op_type:Softmax, op_options:{"axis":-1}]
        addr:0x00000000000167 prog.kernel_symbol[symbol:model.layers.10.self_attn.o_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x00000000000168 prog.kernel_symbol[symbol:model.layers.10.mlp.gate_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x00000000000169 prog.kernel_symbol[symbol:model.layers.10.mlp.act, op_type:SiLU, op_options:null]
        addr:0x0000000000016a prog.kernel_symbol[symbol:model.layers.10.mlp.up_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x0000000000016b prog.kernel_symbol[symbol:model.layers.10.mlp.down_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":8960,"out_channels":1536}]
        addr:0x0000000000016c prog.kernel_symbol[symbol:model.layers.11.input_layernorm, op_type:RMSNorm, op_options:{"add_unit_offset":false,"epsilon":9.999999974752427e-07}]
        addr:0x0000000000016d prog.kernel_symbol[symbol:model.layers.11.post_attention_layernorm, op_type:RMSNorm, op_options:{"add_unit_offset":false,"epsilon":9.999999974752427e-07}]
        addr:0x0000000000016e prog.kernel_symbol[symbol:model.layers.11.self_attn.q_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x0000000000016f prog.kernel_symbol[symbol:model.layers.11.self_attn.k_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x00000000000170 prog.kernel_symbol[symbol:model.layers.11.self_attn.v_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x00000000000171 prog.kernel_symbol[symbol:model.layers.11.self_attn.q_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x00000000000172 prog.kernel_symbol[symbol:model.layers.11.self_attn.k_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x00000000000173 prog.kernel_symbol[symbol:model.layers.11.self_attn.kv_cache, op_type:KVCache, op_options:{"head_dim":128,"kv_head":2,"layer_idx":0,"q_head":12,"use_fa2":false}]
        addr:0x00000000000174 prog.kernel_symbol[symbol:model.layers.11.self_attn.mask, op_type:CausalMask, op_options:{"sliding_window":false,"window_size":0}]
        addr:0x00000000000175 prog.kernel_symbol[symbol:model.layers.11.self_attn.softmax, op_type:Softmax, op_options:{"axis":-1}]
        addr:0x00000000000176 prog.kernel_symbol[symbol:model.layers.11.self_attn.o_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x00000000000177 prog.kernel_symbol[symbol:model.layers.11.mlp.gate_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x00000000000178 prog.kernel_symbol[symbol:model.layers.11.mlp.act, op_type:SiLU, op_options:null]
        addr:0x00000000000179 prog.kernel_symbol[symbol:model.layers.11.mlp.up_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x0000000000017a prog.kernel_symbol[symbol:model.layers.11.mlp.down_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":8960,"out_channels":1536}]
        addr:0x0000000000017b prog.kernel_symbol[symbol:model.layers.12.input_layernorm, op_type:RMSNorm, op_options:{"add_unit_offset":false,"epsilon":9.999999974752427e-07}]
        addr:0x0000000000017c prog.kernel_symbol[symbol:model.layers.12.post_attention_layernorm, op_type:RMSNorm, op_options:{"add_unit_offset":false,"epsilon":9.999999974752427e-07}]
        addr:0x0000000000017d prog.kernel_symbol[symbol:model.layers.12.self_attn.q_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x0000000000017e prog.kernel_symbol[symbol:model.layers.12.self_attn.k_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x0000000000017f prog.kernel_symbol[symbol:model.layers.12.self_attn.v_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x00000000000180 prog.kernel_symbol[symbol:model.layers.12.self_attn.q_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x00000000000181 prog.kernel_symbol[symbol:model.layers.12.self_attn.k_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x00000000000182 prog.kernel_symbol[symbol:model.layers.12.self_attn.kv_cache, op_type:KVCache, op_options:{"head_dim":128,"kv_head":2,"layer_idx":0,"q_head":12,"use_fa2":false}]
        addr:0x00000000000183 prog.kernel_symbol[symbol:model.layers.12.self_attn.mask, op_type:CausalMask, op_options:{"sliding_window":false,"window_size":0}]
        addr:0x00000000000184 prog.kernel_symbol[symbol:model.layers.12.self_attn.softmax, op_type:Softmax, op_options:{"axis":-1}]
        addr:0x00000000000185 prog.kernel_symbol[symbol:model.layers.12.self_attn.o_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x00000000000186 prog.kernel_symbol[symbol:model.layers.12.mlp.gate_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x00000000000187 prog.kernel_symbol[symbol:model.layers.12.mlp.act, op_type:SiLU, op_options:null]
        addr:0x00000000000188 prog.kernel_symbol[symbol:model.layers.12.mlp.up_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x00000000000189 prog.kernel_symbol[symbol:model.layers.12.mlp.down_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":8960,"out_channels":1536}]
        addr:0x0000000000018a prog.kernel_symbol[symbol:model.layers.13.input_layernorm, op_type:RMSNorm, op_options:{"add_unit_offset":false,"epsilon":9.999999974752427e-07}]
        addr:0x0000000000018b prog.kernel_symbol[symbol:model.layers.13.post_attention_layernorm, op_type:RMSNorm, op_options:{"add_unit_offset":false,"epsilon":9.999999974752427e-07}]
        addr:0x0000000000018c prog.kernel_symbol[symbol:model.layers.13.self_attn.q_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x0000000000018d prog.kernel_symbol[symbol:model.layers.13.self_attn.k_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x0000000000018e prog.kernel_symbol[symbol:model.layers.13.self_attn.v_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x0000000000018f prog.kernel_symbol[symbol:model.layers.13.self_attn.q_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x00000000000190 prog.kernel_symbol[symbol:model.layers.13.self_attn.k_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x00000000000191 prog.kernel_symbol[symbol:model.layers.13.self_attn.kv_cache, op_type:KVCache, op_options:{"head_dim":128,"kv_head":2,"layer_idx":0,"q_head":12,"use_fa2":false}]
        addr:0x00000000000192 prog.kernel_symbol[symbol:model.layers.13.self_attn.mask, op_type:CausalMask, op_options:{"sliding_window":false,"window_size":0}]
        addr:0x00000000000193 prog.kernel_symbol[symbol:model.layers.13.self_attn.softmax, op_type:Softmax, op_options:{"axis":-1}]
        addr:0x00000000000194 prog.kernel_symbol[symbol:model.layers.13.self_attn.o_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x00000000000195 prog.kernel_symbol[symbol:model.layers.13.mlp.gate_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x00000000000196 prog.kernel_symbol[symbol:model.layers.13.mlp.act, op_type:SiLU, op_options:null]
        addr:0x00000000000197 prog.kernel_symbol[symbol:model.layers.13.mlp.up_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x00000000000198 prog.kernel_symbol[symbol:model.layers.13.mlp.down_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":8960,"out_channels":1536}]
        addr:0x00000000000199 prog.kernel_symbol[symbol:model.layers.14.input_layernorm, op_type:RMSNorm, op_options:{"add_unit_offset":false,"epsilon":9.999999974752427e-07}]
        addr:0x0000000000019a prog.kernel_symbol[symbol:model.layers.14.post_attention_layernorm, op_type:RMSNorm, op_options:{"add_unit_offset":false,"epsilon":9.999999974752427e-07}]
        addr:0x0000000000019b prog.kernel_symbol[symbol:model.layers.14.self_attn.q_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x0000000000019c prog.kernel_symbol[symbol:model.layers.14.self_attn.k_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x0000000000019d prog.kernel_symbol[symbol:model.layers.14.self_attn.v_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x0000000000019e prog.kernel_symbol[symbol:model.layers.14.self_attn.q_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x0000000000019f prog.kernel_symbol[symbol:model.layers.14.self_attn.k_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x000000000001a0 prog.kernel_symbol[symbol:model.layers.14.self_attn.kv_cache, op_type:KVCache, op_options:{"head_dim":128,"kv_head":2,"layer_idx":0,"q_head":12,"use_fa2":false}]
        addr:0x000000000001a1 prog.kernel_symbol[symbol:model.layers.14.self_attn.mask, op_type:CausalMask, op_options:{"sliding_window":false,"window_size":0}]
        addr:0x000000000001a2 prog.kernel_symbol[symbol:model.layers.14.self_attn.softmax, op_type:Softmax, op_options:{"axis":-1}]
        addr:0x000000000001a3 prog.kernel_symbol[symbol:model.layers.14.self_attn.o_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x000000000001a4 prog.kernel_symbol[symbol:model.layers.14.mlp.gate_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x000000000001a5 prog.kernel_symbol[symbol:model.layers.14.mlp.act, op_type:SiLU, op_options:null]
        addr:0x000000000001a6 prog.kernel_symbol[symbol:model.layers.14.mlp.up_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x000000000001a7 prog.kernel_symbol[symbol:model.layers.14.mlp.down_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":8960,"out_channels":1536}]
        addr:0x000000000001a8 prog.kernel_symbol[symbol:model.layers.15.input_layernorm, op_type:RMSNorm, op_options:{"add_unit_offset":false,"epsilon":9.999999974752427e-07}]
        addr:0x000000000001a9 prog.kernel_symbol[symbol:model.layers.15.post_attention_layernorm, op_type:RMSNorm, op_options:{"add_unit_offset":false,"epsilon":9.999999974752427e-07}]
        addr:0x000000000001aa prog.kernel_symbol[symbol:model.layers.15.self_attn.q_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x000000000001ab prog.kernel_symbol[symbol:model.layers.15.self_attn.k_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x000000000001ac prog.kernel_symbol[symbol:model.layers.15.self_attn.v_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x000000000001ad prog.kernel_symbol[symbol:model.layers.15.self_attn.q_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x000000000001ae prog.kernel_symbol[symbol:model.layers.15.self_attn.k_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x000000000001af prog.kernel_symbol[symbol:model.layers.15.self_attn.kv_cache, op_type:KVCache, op_options:{"head_dim":128,"kv_head":2,"layer_idx":0,"q_head":12,"use_fa2":false}]
        addr:0x000000000001b0 prog.kernel_symbol[symbol:model.layers.15.self_attn.mask, op_type:CausalMask, op_options:{"sliding_window":false,"window_size":0}]
        addr:0x000000000001b1 prog.kernel_symbol[symbol:model.layers.15.self_attn.softmax, op_type:Softmax, op_options:{"axis":-1}]
        addr:0x000000000001b2 prog.kernel_symbol[symbol:model.layers.15.self_attn.o_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x000000000001b3 prog.kernel_symbol[symbol:model.layers.15.mlp.gate_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x000000000001b4 prog.kernel_symbol[symbol:model.layers.15.mlp.act, op_type:SiLU, op_options:null]
        addr:0x000000000001b5 prog.kernel_symbol[symbol:model.layers.15.mlp.up_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x000000000001b6 prog.kernel_symbol[symbol:model.layers.15.mlp.down_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":8960,"out_channels":1536}]
        addr:0x000000000001b7 prog.kernel_symbol[symbol:model.layers.16.input_layernorm, op_type:RMSNorm, op_options:{"add_unit_offset":false,"epsilon":9.999999974752427e-07}]
        addr:0x000000000001b8 prog.kernel_symbol[symbol:model.layers.16.post_attention_layernorm, op_type:RMSNorm, op_options:{"add_unit_offset":false,"epsilon":9.999999974752427e-07}]
        addr:0x000000000001b9 prog.kernel_symbol[symbol:model.layers.16.self_attn.q_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x000000000001ba prog.kernel_symbol[symbol:model.layers.16.self_attn.k_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x000000000001bb prog.kernel_symbol[symbol:model.layers.16.self_attn.v_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x000000000001bc prog.kernel_symbol[symbol:model.layers.16.self_attn.q_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x000000000001bd prog.kernel_symbol[symbol:model.layers.16.self_attn.k_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x000000000001be prog.kernel_symbol[symbol:model.layers.16.self_attn.kv_cache, op_type:KVCache, op_options:{"head_dim":128,"kv_head":2,"layer_idx":0,"q_head":12,"use_fa2":false}]
        addr:0x000000000001bf prog.kernel_symbol[symbol:model.layers.16.self_attn.mask, op_type:CausalMask, op_options:{"sliding_window":false,"window_size":0}]
        addr:0x000000000001c0 prog.kernel_symbol[symbol:model.layers.16.self_attn.softmax, op_type:Softmax, op_options:{"axis":-1}]
        addr:0x000000000001c1 prog.kernel_symbol[symbol:model.layers.16.self_attn.o_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x000000000001c2 prog.kernel_symbol[symbol:model.layers.16.mlp.gate_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x000000000001c3 prog.kernel_symbol[symbol:model.layers.16.mlp.act, op_type:SiLU, op_options:null]
        addr:0x000000000001c4 prog.kernel_symbol[symbol:model.layers.16.mlp.up_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x000000000001c5 prog.kernel_symbol[symbol:model.layers.16.mlp.down_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":8960,"out_channels":1536}]
        addr:0x000000000001c6 prog.kernel_symbol[symbol:model.layers.17.input_layernorm, op_type:RMSNorm, op_options:{"add_unit_offset":false,"epsilon":9.999999974752427e-07}]
        addr:0x000000000001c7 prog.kernel_symbol[symbol:model.layers.17.post_attention_layernorm, op_type:RMSNorm, op_options:{"add_unit_offset":false,"epsilon":9.999999974752427e-07}]
        addr:0x000000000001c8 prog.kernel_symbol[symbol:model.layers.17.self_attn.q_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x000000000001c9 prog.kernel_symbol[symbol:model.layers.17.self_attn.k_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x000000000001ca prog.kernel_symbol[symbol:model.layers.17.self_attn.v_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x000000000001cb prog.kernel_symbol[symbol:model.layers.17.self_attn.q_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x000000000001cc prog.kernel_symbol[symbol:model.layers.17.self_attn.k_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x000000000001cd prog.kernel_symbol[symbol:model.layers.17.self_attn.kv_cache, op_type:KVCache, op_options:{"head_dim":128,"kv_head":2,"layer_idx":0,"q_head":12,"use_fa2":false}]
        addr:0x000000000001ce prog.kernel_symbol[symbol:model.layers.17.self_attn.mask, op_type:CausalMask, op_options:{"sliding_window":false,"window_size":0}]
        addr:0x000000000001cf prog.kernel_symbol[symbol:model.layers.17.self_attn.softmax, op_type:Softmax, op_options:{"axis":-1}]
        addr:0x000000000001d0 prog.kernel_symbol[symbol:model.layers.17.self_attn.o_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x000000000001d1 prog.kernel_symbol[symbol:model.layers.17.mlp.gate_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x000000000001d2 prog.kernel_symbol[symbol:model.layers.17.mlp.act, op_type:SiLU, op_options:null]
        addr:0x000000000001d3 prog.kernel_symbol[symbol:model.layers.17.mlp.up_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x000000000001d4 prog.kernel_symbol[symbol:model.layers.17.mlp.down_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":8960,"out_channels":1536}]
        addr:0x000000000001d5 prog.kernel_symbol[symbol:model.layers.18.input_layernorm, op_type:RMSNorm, op_options:{"add_unit_offset":false,"epsilon":9.999999974752427e-07}]
        addr:0x000000000001d6 prog.kernel_symbol[symbol:model.layers.18.post_attention_layernorm, op_type:RMSNorm, op_options:{"add_unit_offset":false,"epsilon":9.999999974752427e-07}]
        addr:0x000000000001d7 prog.kernel_symbol[symbol:model.layers.18.self_attn.q_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x000000000001d8 prog.kernel_symbol[symbol:model.layers.18.self_attn.k_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x000000000001d9 prog.kernel_symbol[symbol:model.layers.18.self_attn.v_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x000000000001da prog.kernel_symbol[symbol:model.layers.18.self_attn.q_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x000000000001db prog.kernel_symbol[symbol:model.layers.18.self_attn.k_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x000000000001dc prog.kernel_symbol[symbol:model.layers.18.self_attn.kv_cache, op_type:KVCache, op_options:{"head_dim":128,"kv_head":2,"layer_idx":0,"q_head":12,"use_fa2":false}]
        addr:0x000000000001dd prog.kernel_symbol[symbol:model.layers.18.self_attn.mask, op_type:CausalMask, op_options:{"sliding_window":false,"window_size":0}]
        addr:0x000000000001de prog.kernel_symbol[symbol:model.layers.18.self_attn.softmax, op_type:Softmax, op_options:{"axis":-1}]
        addr:0x000000000001df prog.kernel_symbol[symbol:model.layers.18.self_attn.o_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x000000000001e0 prog.kernel_symbol[symbol:model.layers.18.mlp.gate_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x000000000001e1 prog.kernel_symbol[symbol:model.layers.18.mlp.act, op_type:SiLU, op_options:null]
        addr:0x000000000001e2 prog.kernel_symbol[symbol:model.layers.18.mlp.up_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x000000000001e3 prog.kernel_symbol[symbol:model.layers.18.mlp.down_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":8960,"out_channels":1536}]
        addr:0x000000000001e4 prog.kernel_symbol[symbol:model.layers.19.input_layernorm, op_type:RMSNorm, op_options:{"add_unit_offset":false,"epsilon":9.999999974752427e-07}]
        addr:0x000000000001e5 prog.kernel_symbol[symbol:model.layers.19.post_attention_layernorm, op_type:RMSNorm, op_options:{"add_unit_offset":false,"epsilon":9.999999974752427e-07}]
        addr:0x000000000001e6 prog.kernel_symbol[symbol:model.layers.19.self_attn.q_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x000000000001e7 prog.kernel_symbol[symbol:model.layers.19.self_attn.k_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x000000000001e8 prog.kernel_symbol[symbol:model.layers.19.self_attn.v_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x000000000001e9 prog.kernel_symbol[symbol:model.layers.19.self_attn.q_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x000000000001ea prog.kernel_symbol[symbol:model.layers.19.self_attn.k_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x000000000001eb prog.kernel_symbol[symbol:model.layers.19.self_attn.kv_cache, op_type:KVCache, op_options:{"head_dim":128,"kv_head":2,"layer_idx":0,"q_head":12,"use_fa2":false}]
        addr:0x000000000001ec prog.kernel_symbol[symbol:model.layers.19.self_attn.mask, op_type:CausalMask, op_options:{"sliding_window":false,"window_size":0}]
        addr:0x000000000001ed prog.kernel_symbol[symbol:model.layers.19.self_attn.softmax, op_type:Softmax, op_options:{"axis":-1}]
        addr:0x000000000001ee prog.kernel_symbol[symbol:model.layers.19.self_attn.o_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x000000000001ef prog.kernel_symbol[symbol:model.layers.19.mlp.gate_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x000000000001f0 prog.kernel_symbol[symbol:model.layers.19.mlp.act, op_type:SiLU, op_options:null]
        addr:0x000000000001f1 prog.kernel_symbol[symbol:model.layers.19.mlp.up_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x000000000001f2 prog.kernel_symbol[symbol:model.layers.19.mlp.down_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":8960,"out_channels":1536}]
        addr:0x000000000001f3 prog.kernel_symbol[symbol:model.layers.20.input_layernorm, op_type:RMSNorm, op_options:{"add_unit_offset":false,"epsilon":9.999999974752427e-07}]
        addr:0x000000000001f4 prog.kernel_symbol[symbol:model.layers.20.post_attention_layernorm, op_type:RMSNorm, op_options:{"add_unit_offset":false,"epsilon":9.999999974752427e-07}]
        addr:0x000000000001f5 prog.kernel_symbol[symbol:model.layers.20.self_attn.q_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x000000000001f6 prog.kernel_symbol[symbol:model.layers.20.self_attn.k_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x000000000001f7 prog.kernel_symbol[symbol:model.layers.20.self_attn.v_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x000000000001f8 prog.kernel_symbol[symbol:model.layers.20.self_attn.q_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x000000000001f9 prog.kernel_symbol[symbol:model.layers.20.self_attn.k_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x000000000001fa prog.kernel_symbol[symbol:model.layers.20.self_attn.kv_cache, op_type:KVCache, op_options:{"head_dim":128,"kv_head":2,"layer_idx":0,"q_head":12,"use_fa2":false}]
        addr:0x000000000001fb prog.kernel_symbol[symbol:model.layers.20.self_attn.mask, op_type:CausalMask, op_options:{"sliding_window":false,"window_size":0}]
        addr:0x000000000001fc prog.kernel_symbol[symbol:model.layers.20.self_attn.softmax, op_type:Softmax, op_options:{"axis":-1}]
        addr:0x000000000001fd prog.kernel_symbol[symbol:model.layers.20.self_attn.o_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x000000000001fe prog.kernel_symbol[symbol:model.layers.20.mlp.gate_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x000000000001ff prog.kernel_symbol[symbol:model.layers.20.mlp.act, op_type:SiLU, op_options:null]
        addr:0x00000000000200 prog.kernel_symbol[symbol:model.layers.20.mlp.up_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x00000000000201 prog.kernel_symbol[symbol:model.layers.20.mlp.down_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":8960,"out_channels":1536}]
        addr:0x00000000000202 prog.kernel_symbol[symbol:model.layers.21.input_layernorm, op_type:RMSNorm, op_options:{"add_unit_offset":false,"epsilon":9.999999974752427e-07}]
        addr:0x00000000000203 prog.kernel_symbol[symbol:model.layers.21.post_attention_layernorm, op_type:RMSNorm, op_options:{"add_unit_offset":false,"epsilon":9.999999974752427e-07}]
        addr:0x00000000000204 prog.kernel_symbol[symbol:model.layers.21.self_attn.q_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x00000000000205 prog.kernel_symbol[symbol:model.layers.21.self_attn.k_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x00000000000206 prog.kernel_symbol[symbol:model.layers.21.self_attn.v_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x00000000000207 prog.kernel_symbol[symbol:model.layers.21.self_attn.q_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x00000000000208 prog.kernel_symbol[symbol:model.layers.21.self_attn.k_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x00000000000209 prog.kernel_symbol[symbol:model.layers.21.self_attn.kv_cache, op_type:KVCache, op_options:{"head_dim":128,"kv_head":2,"layer_idx":0,"q_head":12,"use_fa2":false}]
        addr:0x0000000000020a prog.kernel_symbol[symbol:model.layers.21.self_attn.mask, op_type:CausalMask, op_options:{"sliding_window":false,"window_size":0}]
        addr:0x0000000000020b prog.kernel_symbol[symbol:model.layers.21.self_attn.softmax, op_type:Softmax, op_options:{"axis":-1}]
        addr:0x0000000000020c prog.kernel_symbol[symbol:model.layers.21.self_attn.o_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x0000000000020d prog.kernel_symbol[symbol:model.layers.21.mlp.gate_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x0000000000020e prog.kernel_symbol[symbol:model.layers.21.mlp.act, op_type:SiLU, op_options:null]
        addr:0x0000000000020f prog.kernel_symbol[symbol:model.layers.21.mlp.up_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x00000000000210 prog.kernel_symbol[symbol:model.layers.21.mlp.down_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":8960,"out_channels":1536}]
        addr:0x00000000000211 prog.kernel_symbol[symbol:model.layers.22.input_layernorm, op_type:RMSNorm, op_options:{"add_unit_offset":false,"epsilon":9.999999974752427e-07}]
        addr:0x00000000000212 prog.kernel_symbol[symbol:model.layers.22.post_attention_layernorm, op_type:RMSNorm, op_options:{"add_unit_offset":false,"epsilon":9.999999974752427e-07}]
        addr:0x00000000000213 prog.kernel_symbol[symbol:model.layers.22.self_attn.q_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x00000000000214 prog.kernel_symbol[symbol:model.layers.22.self_attn.k_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x00000000000215 prog.kernel_symbol[symbol:model.layers.22.self_attn.v_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x00000000000216 prog.kernel_symbol[symbol:model.layers.22.self_attn.q_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x00000000000217 prog.kernel_symbol[symbol:model.layers.22.self_attn.k_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x00000000000218 prog.kernel_symbol[symbol:model.layers.22.self_attn.kv_cache, op_type:KVCache, op_options:{"head_dim":128,"kv_head":2,"layer_idx":0,"q_head":12,"use_fa2":false}]
        addr:0x00000000000219 prog.kernel_symbol[symbol:model.layers.22.self_attn.mask, op_type:CausalMask, op_options:{"sliding_window":false,"window_size":0}]
        addr:0x0000000000021a prog.kernel_symbol[symbol:model.layers.22.self_attn.softmax, op_type:Softmax, op_options:{"axis":-1}]
        addr:0x0000000000021b prog.kernel_symbol[symbol:model.layers.22.self_attn.o_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x0000000000021c prog.kernel_symbol[symbol:model.layers.22.mlp.gate_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x0000000000021d prog.kernel_symbol[symbol:model.layers.22.mlp.act, op_type:SiLU, op_options:null]
        addr:0x0000000000021e prog.kernel_symbol[symbol:model.layers.22.mlp.up_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x0000000000021f prog.kernel_symbol[symbol:model.layers.22.mlp.down_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":8960,"out_channels":1536}]
        addr:0x00000000000220 prog.kernel_symbol[symbol:model.layers.23.input_layernorm, op_type:RMSNorm, op_options:{"add_unit_offset":false,"epsilon":9.999999974752427e-07}]
        addr:0x00000000000221 prog.kernel_symbol[symbol:model.layers.23.post_attention_layernorm, op_type:RMSNorm, op_options:{"add_unit_offset":false,"epsilon":9.999999974752427e-07}]
        addr:0x00000000000222 prog.kernel_symbol[symbol:model.layers.23.self_attn.q_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x00000000000223 prog.kernel_symbol[symbol:model.layers.23.self_attn.k_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x00000000000224 prog.kernel_symbol[symbol:model.layers.23.self_attn.v_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x00000000000225 prog.kernel_symbol[symbol:model.layers.23.self_attn.q_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x00000000000226 prog.kernel_symbol[symbol:model.layers.23.self_attn.k_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x00000000000227 prog.kernel_symbol[symbol:model.layers.23.self_attn.kv_cache, op_type:KVCache, op_options:{"head_dim":128,"kv_head":2,"layer_idx":0,"q_head":12,"use_fa2":false}]
        addr:0x00000000000228 prog.kernel_symbol[symbol:model.layers.23.self_attn.mask, op_type:CausalMask, op_options:{"sliding_window":false,"window_size":0}]
        addr:0x00000000000229 prog.kernel_symbol[symbol:model.layers.23.self_attn.softmax, op_type:Softmax, op_options:{"axis":-1}]
        addr:0x0000000000022a prog.kernel_symbol[symbol:model.layers.23.self_attn.o_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x0000000000022b prog.kernel_symbol[symbol:model.layers.23.mlp.gate_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x0000000000022c prog.kernel_symbol[symbol:model.layers.23.mlp.act, op_type:SiLU, op_options:null]
        addr:0x0000000000022d prog.kernel_symbol[symbol:model.layers.23.mlp.up_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x0000000000022e prog.kernel_symbol[symbol:model.layers.23.mlp.down_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":8960,"out_channels":1536}]
        addr:0x0000000000022f prog.kernel_symbol[symbol:model.layers.24.input_layernorm, op_type:RMSNorm, op_options:{"add_unit_offset":false,"epsilon":9.999999974752427e-07}]
        addr:0x00000000000230 prog.kernel_symbol[symbol:model.layers.24.post_attention_layernorm, op_type:RMSNorm, op_options:{"add_unit_offset":false,"epsilon":9.999999974752427e-07}]
        addr:0x00000000000231 prog.kernel_symbol[symbol:model.layers.24.self_attn.q_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x00000000000232 prog.kernel_symbol[symbol:model.layers.24.self_attn.k_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x00000000000233 prog.kernel_symbol[symbol:model.layers.24.self_attn.v_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x00000000000234 prog.kernel_symbol[symbol:model.layers.24.self_attn.q_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x00000000000235 prog.kernel_symbol[symbol:model.layers.24.self_attn.k_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x00000000000236 prog.kernel_symbol[symbol:model.layers.24.self_attn.kv_cache, op_type:KVCache, op_options:{"head_dim":128,"kv_head":2,"layer_idx":0,"q_head":12,"use_fa2":false}]
        addr:0x00000000000237 prog.kernel_symbol[symbol:model.layers.24.self_attn.mask, op_type:CausalMask, op_options:{"sliding_window":false,"window_size":0}]
        addr:0x00000000000238 prog.kernel_symbol[symbol:model.layers.24.self_attn.softmax, op_type:Softmax, op_options:{"axis":-1}]
        addr:0x00000000000239 prog.kernel_symbol[symbol:model.layers.24.self_attn.o_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x0000000000023a prog.kernel_symbol[symbol:model.layers.24.mlp.gate_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x0000000000023b prog.kernel_symbol[symbol:model.layers.24.mlp.act, op_type:SiLU, op_options:null]
        addr:0x0000000000023c prog.kernel_symbol[symbol:model.layers.24.mlp.up_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x0000000000023d prog.kernel_symbol[symbol:model.layers.24.mlp.down_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":8960,"out_channels":1536}]
        addr:0x0000000000023e prog.kernel_symbol[symbol:model.layers.25.input_layernorm, op_type:RMSNorm, op_options:{"add_unit_offset":false,"epsilon":9.999999974752427e-07}]
        addr:0x0000000000023f prog.kernel_symbol[symbol:model.layers.25.post_attention_layernorm, op_type:RMSNorm, op_options:{"add_unit_offset":false,"epsilon":9.999999974752427e-07}]
        addr:0x00000000000240 prog.kernel_symbol[symbol:model.layers.25.self_attn.q_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x00000000000241 prog.kernel_symbol[symbol:model.layers.25.self_attn.k_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x00000000000242 prog.kernel_symbol[symbol:model.layers.25.self_attn.v_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x00000000000243 prog.kernel_symbol[symbol:model.layers.25.self_attn.q_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x00000000000244 prog.kernel_symbol[symbol:model.layers.25.self_attn.k_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x00000000000245 prog.kernel_symbol[symbol:model.layers.25.self_attn.kv_cache, op_type:KVCache, op_options:{"head_dim":128,"kv_head":2,"layer_idx":0,"q_head":12,"use_fa2":false}]
        addr:0x00000000000246 prog.kernel_symbol[symbol:model.layers.25.self_attn.mask, op_type:CausalMask, op_options:{"sliding_window":false,"window_size":0}]
        addr:0x00000000000247 prog.kernel_symbol[symbol:model.layers.25.self_attn.softmax, op_type:Softmax, op_options:{"axis":-1}]
        addr:0x00000000000248 prog.kernel_symbol[symbol:model.layers.25.self_attn.o_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x00000000000249 prog.kernel_symbol[symbol:model.layers.25.mlp.gate_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x0000000000024a prog.kernel_symbol[symbol:model.layers.25.mlp.act, op_type:SiLU, op_options:null]
        addr:0x0000000000024b prog.kernel_symbol[symbol:model.layers.25.mlp.up_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x0000000000024c prog.kernel_symbol[symbol:model.layers.25.mlp.down_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":8960,"out_channels":1536}]
        addr:0x0000000000024d prog.kernel_symbol[symbol:model.layers.26.input_layernorm, op_type:RMSNorm, op_options:{"add_unit_offset":false,"epsilon":9.999999974752427e-07}]
        addr:0x0000000000024e prog.kernel_symbol[symbol:model.layers.26.post_attention_layernorm, op_type:RMSNorm, op_options:{"add_unit_offset":false,"epsilon":9.999999974752427e-07}]
        addr:0x0000000000024f prog.kernel_symbol[symbol:model.layers.26.self_attn.q_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x00000000000250 prog.kernel_symbol[symbol:model.layers.26.self_attn.k_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x00000000000251 prog.kernel_symbol[symbol:model.layers.26.self_attn.v_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x00000000000252 prog.kernel_symbol[symbol:model.layers.26.self_attn.q_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x00000000000253 prog.kernel_symbol[symbol:model.layers.26.self_attn.k_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x00000000000254 prog.kernel_symbol[symbol:model.layers.26.self_attn.kv_cache, op_type:KVCache, op_options:{"head_dim":128,"kv_head":2,"layer_idx":0,"q_head":12,"use_fa2":false}]
        addr:0x00000000000255 prog.kernel_symbol[symbol:model.layers.26.self_attn.mask, op_type:CausalMask, op_options:{"sliding_window":false,"window_size":0}]
        addr:0x00000000000256 prog.kernel_symbol[symbol:model.layers.26.self_attn.softmax, op_type:Softmax, op_options:{"axis":-1}]
        addr:0x00000000000257 prog.kernel_symbol[symbol:model.layers.26.self_attn.o_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x00000000000258 prog.kernel_symbol[symbol:model.layers.26.mlp.gate_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x00000000000259 prog.kernel_symbol[symbol:model.layers.26.mlp.act, op_type:SiLU, op_options:null]
        addr:0x0000000000025a prog.kernel_symbol[symbol:model.layers.26.mlp.up_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x0000000000025b prog.kernel_symbol[symbol:model.layers.26.mlp.down_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":8960,"out_channels":1536}]
        addr:0x0000000000025c prog.kernel_symbol[symbol:model.layers.27.input_layernorm, op_type:RMSNorm, op_options:{"add_unit_offset":false,"epsilon":9.999999974752427e-07}]
        addr:0x0000000000025d prog.kernel_symbol[symbol:model.layers.27.post_attention_layernorm, op_type:RMSNorm, op_options:{"add_unit_offset":false,"epsilon":9.999999974752427e-07}]
        addr:0x0000000000025e prog.kernel_symbol[symbol:model.layers.27.self_attn.q_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x0000000000025f prog.kernel_symbol[symbol:model.layers.27.self_attn.k_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x00000000000260 prog.kernel_symbol[symbol:model.layers.27.self_attn.v_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x00000000000261 prog.kernel_symbol[symbol:model.layers.27.self_attn.q_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x00000000000262 prog.kernel_symbol[symbol:model.layers.27.self_attn.k_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x00000000000263 prog.kernel_symbol[symbol:model.layers.27.self_attn.kv_cache, op_type:KVCache, op_options:{"head_dim":128,"kv_head":2,"layer_idx":0,"q_head":12,"use_fa2":false}]
        addr:0x00000000000264 prog.kernel_symbol[symbol:model.layers.27.self_attn.mask, op_type:CausalMask, op_options:{"sliding_window":false,"window_size":0}]
        addr:0x00000000000265 prog.kernel_symbol[symbol:model.layers.27.self_attn.softmax, op_type:Softmax, op_options:{"axis":-1}]
        addr:0x00000000000266 prog.kernel_symbol[symbol:model.layers.27.self_attn.o_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x00000000000267 prog.kernel_symbol[symbol:model.layers.27.mlp.gate_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x00000000000268 prog.kernel_symbol[symbol:model.layers.27.mlp.act, op_type:SiLU, op_options:null]
        addr:0x00000000000269 prog.kernel_symbol[symbol:model.layers.27.mlp.up_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x0000000000026a prog.kernel_symbol[symbol:model.layers.27.mlp.down_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":8960,"out_channels":1536}]
    }
}
 
